Title,Assigned Topic,Topic Keywords,Upvote Ratio,Poster
(Meta research) MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.95,llamaShill
MediaTek Leverages Meta’s Llama 2 to Enhance On-Device Generative AI,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,noiseinvacuum
My open-source & cross-platform on-device LLMs app is now available on TestFlight & GitHub. Feedback & testers welcome.,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.55,BrutalCoding
I've created Distributed Llama project. Increase the inference speed of LLM by using multiple devices. It allows to run Llama 2 70B on 8 x Raspberry Pi 4B 4.8sec/token,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.98,b4rtaz
Run models on a real Android device with Qualcomm AI Hub,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.91,ephemeralshot
Native LORA finetuning on Apple Devices (New MLX Framework) 😲!!,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.85,phoneixAdi
llm on mobile devices - reading pdf usecase,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,Helloworld1907
"Opentensor and Cerebras announce BTLM-3B-8K, a 3 billion parameter state-of-the-art open-source language model that can fit on mobile devices",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,CS-fan-101
FP4 quantization state not initialized. Please call .cuda() or .to(device) on the LinearFP4 layer first.,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,AstronomerChance5093
Has someone tried LLMFarm for native inference on iOS devices?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,frapastique
How do I run Stable Diffusion and LLMs from my PC on my mobile device? Offline and private ways?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.62,ThrowawayProgress99
"I'm about to open source my Flutter / Dart plugin to run local inference on all major platforms. See how it runs on my personal Apple devices: macOS (Intel & M1), iOS, iPadOS. Next up: Android, Linux & Windows. AMA.",Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.92,BrutalCoding
What am I doing wrong?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.96,slykethephoxenix
I wonder theres way to run LLM without loading on ram,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.64,wjohhan
A local LLM is the ultimate doomsday device,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.9,Ninjinka
"Fine tuned coqui XTTS voice, how to use the model.pth?",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.8,hwknd
Maybe we will be able to run far larger models on Apple hardware than previously thought,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,Ward_0
Apple CoreML,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,krazzmann
Any way to optimally use GPU for faster llama calls?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.89,todaysgamer
Mbp m3,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.8,tshawkins
Simple demo app of TinyStories-1m that runs locally on iOS,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.93,seattleeng
GPU-Accelerated LLM on a $100 Orange Pi,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.96,EmotionalFeed0
Why is the idea of more than one participant so foreign to LLMs other than GPT4?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.42,TradingDreams
Two RTX 3060 for running llms locally,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.88,arc_pi
Speculation. Could Apple pull off an end run around everyone else and offer low cost large model inference?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.68,fallingdowndizzyvr
Merging LoRA with Mistral models?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,ququrydza
[Project] MLC LLM for Android,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,crowwork
TinyLlama-1.1B: Compact Language Model Pretrained for Super Long,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,Either_Ad_1649
"Is anyone inferencing on something like an Intel nuc, barebone or similar formfactor?",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.7,Frequent_Valuable_47
Deploying LM/LMM to edge device,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,rower22
Looking for a lightweight Model ideally fine-tuned for generating compliments or similar,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.9,PSRD
oobabooga Update broke loading u/The-Bloke huggingface models?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.78,shzam123
Optimum Intel OpenVino Performance,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,fakezeta
Is Llama 2 7B or 7B Chat better for a talking Teddy Bear?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.93,danl999
Guide: Installing ROCm/hip for LLaMa.cpp on Linux for the 7900xtx,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,Combinatorilliance
Why are MLX and Ollama way faster than PyTorch on M1 Mac?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.78,juanluisback
Skill Issue while running GPU accelerated llama,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.83,MDCurrent
Silent Release: Llama2 7B on Snapdragon Gen2 with 8 tok/s,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.94,YYY_333
What are your thoughts on the future of LLMs running mobile?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,Tree-Sheep
A fine tuned Llama2-chat model can’t answer questions from the dataset,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.67,celsowm
Should I start this project?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.86,DatOneGuy73
MLC-LLM Chat vicuna-Wizard-7B-Uncensored-q3f16_0,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.92,jetro30087
Is it possible to process layers one by one on a low vram gpu?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.57,moozoo64
M3 Max/Ultra vs RTX3090 vs RTX4090 with large context windows?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.9,Some_Endian_FP17
Home setup for future home model use,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.85,Sockosophist
Llama 2 as a local copilot!!,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.95,tarun-at-pieces
Llama finetuning question,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Alert_Record5063
Apple releases MLX for Apple Silicon,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.9,Tommy-kun
Awful quantisation outputs with V100,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,gpu_go_brrr
CLblast is nice on crap systems!,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.89,thebadslime
Having a Local LLM interact with an API,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.57,Iseenoghosts
Has Anyone Successfully Utilized the Neural Networks API on Android for LLMS with EdgeTPU?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.86,dewijones92
Best model to convert voice commands to JSON?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.72,slykethephoxenix
"When LLM doesn’t fit into memory, how to make it work?",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Robert-treboR
No CUDA GPUs are available error on text generator webui,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,OvercookedSatellite
Let's create a 65B benchmark in this thread,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.96,Big_Communication353
Apple has an excellent hardware base for local generative AI,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,Balance-
Best backends for running models on Android?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.86,GamerWael
Llama 2 7b-Instruct on 2 RTX 2080 Ti GPUs,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,T3h_Laughing_Man
Is Llama.cpp Using GPU's on my M2 Max?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,Mbando
Single RTX 4090 FE at 40 tokens/s but with penalty if running 2 get only 10 tokens/s. Confirmed with Xwin-MLewd-13B-V0.2-GGUF.,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.81,easyllaama
2x Teslas in an OEM system,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.9,ConcaveTriangle5761
What 7b llm to use,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.82,bot-333
Most advanced LLM for a Jetson Orin Nano?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.84,fission4433
Accelerate not working with merged model?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,rmt77
Koboldcpp linux with gpu guide,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,amdgptq
"Finally got a model running on my XTX, using llama.cpp",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.96,TeakTop
Success with a local voice chat agent,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.99,dkjroot
Any interesting LLM project ideas?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.72,theoretical_entity
"Vicuna 13b on RK3588 with Mail G610, OpenCL enabled. prefill: 2.3 tok/s, decode: 1.6 tok/s",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.92,EmotionalFeed0
ONNX to run LLM,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.57,hungrydit
Introducing Vaartaalaap: A Chatbot UI for Local LLM Servers,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.82,Reasonable_Ad9033
"Could I serialize models in current ""state""?",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Woitee
"MiniCPM: An end-side LLM achieves equivalent performance to Mistral-7B, and outperforms Llama2-13B",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.76,x_swordfaith_l
A new type of transistor is more efficient at (some) machine learning tasks,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.95,neph1010
How to run base models w. finetuned adapters in LlamaIndex or Langchain?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,salah_ahdin
Slow LLM speeds on RTX 4090,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,Reign2294
LLM.swift library lets you interact LLMs on iOS easily,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.95,eastriver0720
[Project] Making AMD GPUs Competitive for LLM inference,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.96,yzgysjr
"Need Help Optimizing Language Model Performance on Nvidia Jetson AGX Xavier
",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.75,iamnotdeadnuts
I made a language-agnostic function calling (open source),Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.93,mcharytoniuk
flan T5-Large just gives the context as the response,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.81,IamFuckinTomato
Running Llama2 on Android,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.89,Deep-View-2411
Utilizing two different size GPUs for fine-tuning,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,ali0100u
Estimated Time for SFT Fine-Tuning of Mistral-7B Model,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.87,Aron-One
Exploding loss when trying to train OpenOrca-Platypus2-13B,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,Crafty_Charge_4079
Anyway to use Runpod on android?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.5,theshoelesschap
"I love running locally, but",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.75,__Maximum__
I love hallucinations,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.96,Fusseldieb
Which is the smallest Llama model out there?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.95,AnonymousD3vil
Speed difference not matching file size between quants?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.83,Tree-Sheep
Help needed on building doc translation LLM,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,dodo13333
Why do prompts work so differently depending on the model used?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.6,DoktorMerlin
How to compile models for MlC-LLM,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,jetro30087
Weird Dual GPU setup in Ooba,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.81,disarmyouwitha
Guide for oogaboooga on amd using rocm gpu on linux ubuntu and fedora,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.81,thesawyer7102
I built my own android chatting frontend for LLMs.,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.96,----Val----
An Alternative Approach to Building Generative AI Models,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.9,buildinstuff5432
"No-code, uncertainty-aware LLM classification and fine-tuned semantic search for Mac",Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.69,Reexpressionist
Unable to train LLaMA2-7B-HF in an RTX 3050,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,CollectionFar336
Lightweight LLama variants for Mobile applications,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.8,thesithlord27
Half Precision (e.g. no quantization) Phind V2 CodeLlama 34B running on Mac M1 at 8.6 tokens per second,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.77,Thalesian
Mark Zuckerberg on upcoming LLaMA v2,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.98,llamaShill
Model parallelism with LoRA,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,jeremyhoward
(New Model) Rift Coder 7B. Python & TypeScript Fine-Tuned Code Llama for IDE Use,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.95,pranavmital
Llama2 Qualcom partnership,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.86,AstrionX
A Free AI Scribe Project I am Working on! Please Provide Feedback!,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.85,ThrowAway12461246124
LLaMA for poor,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.88,pratiknarola
Using LLMs to build custom Operating Systems?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.67,Wroisu
"Used GPT4ALL for the first time and wondering if I could somehow feed it (technical) PDFs and turn it into ""sidekick"" for embedded programming?",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.85,SaarN
Guide to running llama.cpp on Windows+Powershell+AMD GPUs,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.9,fatboy93
Commercial model + API question,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.44,akuhl101
"Since the launch of the AI Pin, have there been any alternatives announced that are open platform?",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.7,spar_x
Dual 3090 ti GPU's on Ubuntu Desktop x64 help,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,snowmobeetle
Fine-tuning for custom domain knowledge,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.98,rinse_repeat_wash
Exllama on windows using CPU,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,HeavyDiamond8069
"Documentation Paradigm for Large Language Models (LLMs): Log Today, Train Tomorrow",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.83,phoneixAdi
LLaMA-4bit inference speed for various context limits on dual RTX 4090 (triton optimized),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.95,MasterH0rnet
num_beams > 1 breaking my model (Open-LLaMA7b - Alpaca-finetuned),Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,BuzzLightr
Code Llama - The Hugging Face Edition,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.97,hackerllama
Can't use multi-gpu with 8x A100 80GB,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.81,nhanha_castanha
Some advice on running Guidance?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.67,iChinguChing
Best prompt and model for fact-checking a text (disinformation/fake-news detection),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.6,Fit_Check_919
[HELP] It's there a way to make Llama 2 model generate text token by token or word by word like what ChatGPT does?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.82,MrForExample
Most performant option to run 13b LLM locally as a personal assistant for under $300 USD,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.55,Envoy0675
Is the Nvidia Jetson AGX Orin any good?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.96,zippyfan
Time to first token (TTFF) with llama.cpp vs. vllm,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.6,silvanoluciano
"Anyone working on a ""tiny"" version of Mixtral-8x7b",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,bonthebruh
How to fine-tune Llama 70B fp16 on 8x A100 80GB?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,kupo1
I am looking for information regarding Running llama on a zen4 or xeon 4th generation cpu? Or alternative no gpu suggestions (for 180b falcon),Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,jasonmbrown
Running Llama-65B with moderate context sizes,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.67,Xir0s
CogAgent: A Visual Language Model for GUI Agents,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.92,llamaShill
Anyone working on linking local Ai with Home Assistant?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.81,TheSilentFire
HF transformers vs llama 2 example script performance,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,FormerAlternative707
KoboldCpp - Combining all the various ggml.cpp CPU LLM inference projects with a WebUI and API (formerly llamacpp-for-kobold),Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.99,HadesThrowaway
Running [Crataco ]Pythia Deduped GGML on 4 GB Ram Laptop,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.71,Merchant_Lawrence
"I will do the fine-tuning for you, or here's my DIY guide",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.98,phoneixAdi
"Working on a QLORA hub for model personalities, help needed",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.86,Lang2lang
Error in load_in_8bit when running alpaca-lora using 3080,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,Full_Sentence_3678
Yet another quantization method: SpQR by Tim Dettmers et al.,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.99,rerri
Falcon 180B GPTQ Model on Multi-GPU Setup with RunPod,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,Wrong_User_Logged
A simple Huggingface Downloader,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.86,q5sys
llama2-7B/llama2-13B parameter model generates random text after few questions,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.86,Optimal_Original_815
Why does my fine tuning of Mistral 7B Instruct not stop?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.63,ProjectProgramAMark
Can we create a megathread for cataloging all the projects and installation guides of Llama?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.98,utkvishwas
Help to use pipeline conversational on mistral instruct v0.2,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.6,celsowm
Improving Falcon-180B Performance,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.93,Simusid
Building an IDE with native support for Open Source models,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,ragingWater_
Best setup and Settings for a Beginner?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,ChrisX930
"Trying to load togethercomputer_LLaMA-2-7B-32K with fully loaded context but it OOMs, but I should have enough VRAM?",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.94,tenmileswide
Building koboldcpp_CUDA on Linux,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,Current-Voice2755
Retrieval Augmented Generation optimised Llm's,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.98,ale10xtu
Guanaco fine-tuning for classification,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.99,aidalovegood
Help with objective tokens per second measurement,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,zDraco_Meteor
Finetuned Mistral outputting multiple Q and A responses.,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,No-Point1424
Codelamma 7b code completion giving multiple responses and i only want one?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,llamasaresavager
I am not special. Though I seek guidance from those that are.,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.38,ThemWhoNoseNothing
Hugging Face community blogpost: 🕳️ Attention Sinks in LLMs for endless fluency (related to StreamingLLM),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,CubieDev
I wish there was a market for buying access to proprietary LLMs to run locally,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.86,SomeOddCodeGuy
Training LLaMA-2 for Keyword Extraction,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,TaleOfTwoDres
Limiting GPU memory allocation by LLM during inference/generate step.,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,catzilla_06790
Keep running out of memory when pre-training (without LoRA) a model 7B on 2 A100 80GB GPU?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,scienceotaku68
Was Joi from Blade Runner 2049 a local LLM?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.72,Herr_Drosselmeyer
How do I load a gptq LLaMA model (Vicuna) in .safetensors format?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,KillerX629
"New Oobabooga Standard, 8bit, and 4bit plus LLaMA conversion instructions, Windows 10 no WSL needed",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,Inevitable-Start-653
Multi GPU splitting performance,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.9,BreakIt-Boris
Trouble creating document Q&A chat bot,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,RidesFlysAndVibes
LOL why did this work on InternLM 20b?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.84,LetMeGuessYourAlts
How to perform multi-GPU parallel inference for llama2?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.9,cringelord000222
SillyTavern 1.10.0 Release,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,RossAscends
Engaging topics for conversations in a small local workshop,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.75,besabestin
New badass model OpenAssistant/llama2-13b-orca-8k released 🎉,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.98,FHSenpai
"Can we discuss MLOps, Deployment, Optimizations, and Speed?",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.93,BayesMind
Increasing speed for webui/Wizard-Vicuna-13B with my Mac Pro M1 16gb setup?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.91,spoilingba
LLMs that can run on CPUs and less RAM,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.75,IamFuckinTomato
Thunderbolt and multiple eGPUs,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,throwaway075489
PrivateGPT - Asking itself questions and answering?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.75,masterblaster269
Text Generator webui - how do I select which GPU to use?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,OvercookedSatellite
Help for the n00b? Optimal loader parameters...,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.84,AirwolfPL
TIL Sharding a model into smaller chunks may make it possible to load in Free Colab instances without running out of memory,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.94,ragnarkar
Seeking Orientation on Developing an LLM-Based QA Chatbot,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.75,emersounds
Train model from scratch (llama.cpp) - any experiences?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.94,dual_ears
Splitting models and device map.,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,a_beautiful_rhind
"Different results with float16. [Actually, gemma-7b-it does not work with float16]",Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.57,Asleep-Agency3023
Desktop 3xP40 Rig,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,Mass2018
"Can't load new Landmark models in ooba, complains that trust_remote_code not enabled when it clearly is",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,tenmileswide
Appreciation and Inspiration!,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.99,SeymourBits
Performance report - Inference with two RTX 4060 Ti 16Gb,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.98,pmelendezu
Why Mistral-7b is repeating the promp everytime I ask him something?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.83,datapim
Phi-2 & Tiny LLama on Raspberry Pi 5,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.93,flopik
"Training Large Language Models: Fluctuating Training Loss But Smooth Eval Loss, What's Happening?",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,Traditional-Gain-593
"Newbie , installed dalai with llama locally, trying to make sense of responses",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,homtanksreddit
How to install LLaMA: 8-bit and 4-bit,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,Technical_Leather949
Vicuna on AMD APU via Vulkan & MLC,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.95,AnomalyNexus
New trained storytelling LoRA returns really interesting results.,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,DaniyarQQQ
Just sharing some quick tips to painlessly install PrivateGPT on your windows machine with Ubuntu as WSL (Windows Subsystem for Linux),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.93,108er
Project: Using Mixtral 8x7b Instruct v0.1 Q8 to Generate a Synthetic Dataset for LLM Finetuning,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,Mbando
Llama.cpp with 13B is hallucinating in my domain?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.82,fhirflyer
How to Get Around Context Limits with Data Files,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,prettyobviousthrow
"Looking for for folks to share llama.cpp settings/strategies (and models) which will help write creative (interesting), verbose (long), true-to-prompt stories (plus a short discussion of --multiline-input flag)",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.97,spanielrassler
Perplexity Testing Mac vs Windows Pt 2- Mac still 3x lower,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.73,LearningSomeCode
"Quick hardware comparison Ryzen 5700X, RTX 3090, Mac Studio M1",Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.77,Late_as_ever
LLM for chatting and command recognition,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,3ngaged
Perplexity Testing Mac vs Windows Pt 4: CPU test time. Results continue to point to a fundamental difference of Metal inference,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.92,LearningSomeCode
Perplexity Testing Mac vs Windows Pt 3: Adding context for context; something is definitely different,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,LearningSomeCode
Error while finetuning,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,1azytux
Comparing LLaMA and Alpaca models deterministically,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.99,WolframRavenwolf
"Comparing Image/Chart/Illustration Descriptions generated by GPT-4V, LLaVa, Owen-VL for RAG Pipelines",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.96,ramprasad27
"🐺🐦‍⬛ LLM Comparison/Test: 6 new models from 1.6B to 120B (StableLM, DiscoLM German 7B, Mixtral 2x7B, Beyonder, Laserxtral, MegaDolphin)",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.98,WolframRavenwolf
airoboros-65B-gpt4-1.2.ggmlv3.q8_0.bin - harry potter erotica,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.38,dewijones92
Difference between merged model (with adapters) and load adapters on top,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.77,No_Organization_2634
"Is Shared memory on a laptop better than regular desktop DDR5. The 4090 has 64GB of ""total"" memory?",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.79,Future-Freedom-4631
Best Local LLM For low end device coding,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.75,Low-Plastic-2399
Inconsistent Token Speed on Llama 2 Chat 70b with Exllama,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Used_Carpenter_6674
"Advice for model to run on laptop gtx 1060 (6gb), i7 8750H, ram 16gb",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.82,blacktie_redstripes
"Low token/s count on 7b models, is this normal",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.75,nono577
NVidia vGPU on esx,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.72,BreakIt-Boris
CodeLlama 70B Local Deployment with JIT Compilation,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.9,SnooMachines3070
Speculative Decoding in Exllama v2 and llama.cpp comparison,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.97,lone_striker
Your Mixtral Experiences So Far,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.87,psi-love
Complete guide for KoboldAI and Oobabooga 4 bit gptq on linux AMD GPU,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,amdgptq
Understanding embeddings,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.9,EnPaceRequiescat
Can I use embedding models from the MTEB board with generate embedding api of ollama to generate embedding?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,coderinlaw
Llama2 Embeddings FastAPI Service,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.96,dicklesworth
Embeddings or Instructor Embeddings?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.91,jnk_str
Fine tuning embeddings,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.87,learning_agent
nomic-embed-text-v1.5: Resizable Production Embeddings with Matryoshka Representation Learning - Scalable Vector Embeddings from 64 to 768 dimensions,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.95,davidmezzetti
One Embedding To Rule Them All?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.88,Simusid
How does resize_token_embeddings() refactor embeddings for newly added tokens?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,Sheamus-Firehead
First OS embedding model with 8k context,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.98,Amgadoz
What Embedding Models Are You Using For RAG?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.99,Simusid
Chunking Text & Normalizing embeddings (C#),Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.67,1EvilSexyGenius
I'm confused about embeddings,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.92,julio_oa
Introduction to Matryoshka Embedding Models with Sentence Transformers,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.96,davidmezzetti
Embeddings?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,phree_radical
Code LLaMA embeddings?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.75,oomydoomy
Finding better embedding models,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.95,Relative_Winner_4588
Why Aren't Custom Embeddings Helping More?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,Mbando
Testing Google Multimodal Embeddings,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.92,Electrical-Profile79
RAG Embeddings,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.89,Ill_Bodybuilder3499
What hardware is better for embedding,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.75,pangolinportent
New Model: Nomic Embed - A Truly Open Embedding Model,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,shouryannikam
Are there any boilerplate RAG/embedding programs? (python),Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.86,pr1vacyn0eb
Embeddings vs Context,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,Longjumping_Time_639
txtai 6.0 - the all-in-one embeddings database,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,davidmezzetti
RAG in a couple lines of code with txtai-wikipedia embeddings database + Mistral,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.96,davidmezzetti
What are the benefits of using open source embeddings model?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.94,99OG121314
Local Rag/embedding clarifications,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,Appropriate-Tax-9585
TF-IDF + Embedding for RAGs?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.67,hag_o_hi
How important is choosing an embedding model?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,malicious510
Lora vs Embeddings (Vector DB?) Knowledge Training,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.97,cringelord000222
Hi I'm seeking for any embedding model for vietnamese,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.78,nah_nan
Looking for open-source contributors for text-embedding server for inference,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.98,OrganicMesh
PEG (Progressively Learned Textual Embedding),Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.93,Thistleknot
Minimal local embedding?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,User1539
What embedding model do you guys use?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.79,Distinct-Target7503
Interpret Llama.cpp embeddings,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Simusid
What makes a good embedding model?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Robot_Graffiti
Does including timestamps in a transcript degrade embedding performance?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.75,Beautiful_Surround
Chunking and embedding XML,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,dickfreelancer
Question about embeddings and vectordatabase,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,8Optimism
NEFTune: Noisy Embeddings Improve Instruction Finetuning,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.96,Unstable_Llama
LLM for RAG - embedding and chat not compatible?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.84,_donau_
"Infinity, a project for supporting RAG and Vector Embeddings.",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.92,OrganicMesh
Hosting your own embeddings API,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,java_dev_throwaway
What's the SOTA for open-source search embeddings?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.72,AM_DS
Keeping the input embeddings entirely on CPU in PyTorch,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.88,mildresponse
OpenAI Embeddings API alternative?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.95,pahulrathak
"Help me choose: Need local RAG, options for embedding, GPU, with GUI. PrivateGPT, localGPT, MemGPT, AutoGen, Taskweaver, GPT4All, or ChatDocs?",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.93,TheWebbster
Which 30b model should I use for embeddings?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.91,HaOrbanMaradEnMegyek
Creating your own embedding model to boost retrieval performance,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.86,drivenkey
What is the best embedding search?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,aiworshipper
Would you use a free and anonymous (no login) website for embeddings?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.89,Simusid
Embeddings for Q&A over docs,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,wsebos
Is it possilbe to create embeddings using LLama2 model?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,kitkatmafia
Kalosm v0.2.0: A simple open source framework for embedded AI,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.82,ControlNational
Need guidance to achieve semantic search using embeddings for non English text,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,Clasyc
UAE: New Sentence Embeddings for RAG | SOTA on MTEB Leaderboard,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,PrudentCherry322
PubMedBERT Embeddings with Matryoshka Representation Learning enabling dynamic vector sizes,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,davidmezzetti
Can we do similarity search with different embedding models?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,Shoddy_Vegetable_115
VectorDB with Llama Embeddings - Few Questions and Doubts,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.79,--lael--
Why does positional encodings add to token embedding instead of having a separate embedding dimension just for the position?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,gi_beelzebub
h2ogpt on CPU: any experience on how long embeddings creation will take?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.81,tarasoraptor
"[Text embedding] Why do I need to run a model to generate embeddings for an entire book, if I can run it for every possible token and perform mean pooling?",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.9,k110111
Best Practices for Semantic Search on 200k vectors (30GB) Worth of Embeddings?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.75,stoicbats_
'Missing tok_embeddings.weight' when using GGML models,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,TizocWarrior
Searching for basic chunking - embedding example,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,Natural_Speaker7954
New embedding models from Jina AI,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.67,Acrobatic-Site2065
"Fast Vector Similarity Library, Useful for Working With Llama2 Embedding Vectors",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.84,dicklesworth
"Embeddings for Search, alternatives?",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.86,Fun_Tangerine_1086
Question about multiple sources with vector embeddings & local LLM.,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.91,pickandmix222
"As of 2024, what is the state of non-english models (embedding/LLM) for RAG?",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.63,graphitout
"Is local LLM models (Zephyr, Mistral, etc..) good enough to generate embeddings? (using Ollama)",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.95,Uncensored4488
Why is there not as much cutting edge research into embeddings?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.92,srvhfvakc
embedding from RedPajama INCITE chat 3B,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.75,hungrydit
Is Embeddings are giving better answer than finetuned model?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.57,darshil-3099
Can I use a LocalLLaMA to create embeddings from my text?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Phptower
Is words represented as embedding fed to the LLM's best way to train and do inference?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.77,Reddeasa
Build Enterprise Retrieval-Augmented Generation Apps with NVIDIA Retrieval QA Embedding Model,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.91,Scary-Knowledgable
"Retrieval, Extensible search, Embeddings, or Teaching",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,DamienHavok
How do you determine which embedding models will fit into memory available?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.75,nuusain
How can I use embeddings from llama.cpp using OpenAI in Python?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.8,mmmanel00
Is there any research on using embedding as tokens to dramatically increase transformers context limits?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.75,Another__one
PubMedBERT Embeddings - Semantic search and retrieval augmented generation for medical literature,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,davidmezzetti
What are the text chunking/splitting and embedding best practices for RAG applications?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,malicious510
German language embedding model for fine tuned Mistral 7B model ( Leo LM &EM_German) for RAG based implementation.,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.92,TheAmendingMonk
Passing embeddings to llama with ctransformers for long term memory,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,GOD_HIMSELVES
Best OS/Paid Embedding Model for Longer Token Length + Retrieval,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Expert-Supermarket-4
Reality check on good embedding model (and this idea in general),Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,cap811
"bge-m3 - a multilingual embedding model, from the authors of bge-large-en-v1.5 that topped for a long time the MTEB leaderboard",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,vasileer
Langchain Vicuna Server - Added Support for GPTQ-4bit and Experimental Vicuna Embeddings (Hugging Face only),Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.96,rustedbits
"Any similar no code, easy to use tools/methods for knowledge embedding / RAG that work with local LLMs",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,gyaani_guy
Seeking Opinions on e5-large-v2 and instructor-xl Embedding Models for Multilingual Applications,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Reality-Sufficient
"Used GPT4ALL for the first time and wondering if I could somehow feed it (technical) PDFs and turn it into ""sidekick"" for embedded programming?",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.72,SaarN
fastText: Embeddings for 157 languages for identification and similarity search tasks,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,kryptkpr
"HuggingChat, the open-source alternative to ChatGPT from HuggingFace just released a new websearch feature. It uses RAG and local embeddings to provide better results and show sources.",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.98,SensitiveCranberry
Sojee: My own little dual-stage prompt embedding chatbot that can be quickly customized to any particular corpus.,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,alittleteap0t
Yet another RAG system - implementation details and lessons learned,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,snexus_d
Any ideas? Pdf -> Q&A form -> embedding -> vector db -> similarity search -> chat gpt 3.5 answer,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.84,sexychipss
Today is the first day I’m getting results comparable to GPT4 on OpenSource LLM workflows.,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.99,LocoMod
Introducing LocalGPT: Offline ChatBOT for your FILES with GPU - Vicuna,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.96,satmarz
Huge issue with TruthfulQA contamination and license issues on HF 7B leaderboards,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.86,ouxjshsz
Apple silicon local inference - can M3 pro max do it all?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.71,dimsumham
What's a **fair** way of computing similarity across sequence length?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.67,Its_All_Chain_Rules
RAG beginner questions,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,PersonSuitLevitating
LLongMA 2: A Llama-2 8k model,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,EnricoShippole
Whats in your RAG setup?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,EnvironmentalDepth62
R2R – Open-source framework for production-grade RAG,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,docsoc1
Hermes LLongMA-2 8k,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.97,EnricoShippole
This can make a huge difference. Extending context from 4k to 400k. Llama-2-chat-7b.,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.97,AIMatrixRedPill
Open/Local LLM support for MineDojo/Voyager,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,spyderwebab
MoE-LLaVA: Mixture of Experts for Large Vision-Language Models - Peking University 2024 - MoE-LLaVA-3B demonstrates performance comparable to the LLaVA-1.5-7B !,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.99,Singularian2501
LLongMA-2 16k: A Llama 2 16k model,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,EnricoShippole
"I'm building an Open Source (and optionally local) Google for LLM Agents, any requests?",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.93,docsoc1
Anyone who has worked with using LLMs for creating recommendation engines,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.67,mayiSLYTHERINyourbed
Is my Data safe when using trust_remote_code?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.84,Purity1212
My experience on starting with fine tuning LLMs with custom data,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,Ion_GPT
Create/Query vector database with LLM,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,that_one_guy63
Survey about Retrieval Augmented Generation (RAG) in Real Production,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.89,PrimaryHeat5864
"Gemma finetuning 243% faster, uses 58% less VRAM",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.95,danielhanchen
Extending LLM's Context with Activation Beacon [Model/code release],Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.98,mpasila
Chat with RTX on a large folder of PDFs - how long will this take?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.87,GrimmigerDienstag
Launching AgentSearch - A local search engine for your LLM agent,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,docsoc1
Swiss Army Llama: Do tons of useful stuff with local LLMs with a REST API,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.98,dicklesworth
Some questions of implementing LLM to generate Q/A pairs based on local documents,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,william_luckybob
"txtai 6.3 released: Adds new LLM inference methods, API Authorization and RAG improvements",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,davidmezzetti
Tokenizer of GGUF with LlamaCPP,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,Sheamus-Firehead
RAG + real TXT book + Yi34b-chat = creative writing beast,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.97,Shir_man
Explain Re-Ranking,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.93,Simusid
LLongMA-2 13b 8k,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,EnricoShippole
Fantasy writing assistant model?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.88,TheCouncilNovel
The last LLM (or Over-hype will cause another Winter),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.53,True_Giraffe_7712
Cross pollination of ideas from Stable Diffusion: Textual Inversions,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.96,revolved
Expand the Context Length with RoPE from a β-based Encoding perspective,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.99,Alternative_World936
How to process queries that require real-time information with RAG?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.86,unknow_from_vietnam
Problems using quantised llama models with cpu,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.85,Spiritual-Ask-9766
Training an LLM on multiple documents: first steps.,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.92,ArsePotatoes_
Semantic text similarity,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.44,adeel_hasan81
Mistral Vision/Audio LoRAs & a Lossy 260K+ Token Context Window Prototype,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,sshh12
AI and RAG in the context of a public library,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.94,CedricLimousin
Scalable distributed computing open source RAG framework using Ray,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.95,ameriadyte
"Google is training with 8 bit ints, next will move to 4 bit ints.",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.89,danielcar
GPU-Accelerated LLM on a $100 Orange Pi,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.96,EmotionalFeed0
Rag vs Vector db,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.89,troposfer
Reor: an AI personal knowledge management app powered by local models,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.97,undamp
Is it feasible to use an open source model & vector database to manage a growing library of ebooks & papers?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.95,-mickomoo-
Chatting with internal company documents,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.82,Humble-Helicopter-43
Simple ollama rag in python,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.86,capivaraMaster
FAQ Retrieval design,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,Similar-Ingenuity-36
Can't handle efficiently RAG with large PDF,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.75,Temporary-Size7310
Fundamental limitations of *current* LMM approaches,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.61,BalorNG
LaVague: Open-source Text2Action AI pipeline to turn natural language into Selenium code,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.89,Separate-Still3770
Impact of regulations on open source LLM,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.97,Kujamara
New to LLMs: Seeking Feedback on Feasibility of Hierarchical Multi-Label Text Classification for Large Web Archives,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.76,ReversingEntropy
How to handle dependencies between text changes in RAG,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.86,vile_proxima
My meta-llama/Llama-2-7b-hf fine-tuned model does not learn to use the additional special tokens,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,rares13
How to build a multimodal RAG FAQ app with pptx documents,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.91,violet_bloom_87
LLM with Built In Vector Search and Database,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.35,sachingkk
Llama-2-13b and document QA,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.75,Kukaracax
how to embed code?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.83,LyPreto
Trying to understand what tokens are focused in to start inference with attention weights,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.75,Eastern_Macaroon7672
How do I automate the back-and-forth process of running and eliminating bugs from code generated by LLMs?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.55,ResearcherNo4728
Favorite RAG tools/frameworks [Early Feb 2024],Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.84,Sebba8
Guide on building training datasets from unstructured text sources?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.81,HunterAmacker
Can I use LLM to compare 2 pdf documents and find changes ?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.72,Plane_Ad9568
How would you do it? Handling multi-turn QA conversation with matching of questions to vector database.,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,kotschi1997
"A python package I created for llms application including my own implementation of long short term memory and a web search tool for llm, it supports both Openai-like API or loading local models directly from different formats.",Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.98,llordnt
What are interesting open source resources/ projects for building LLMs for India/ Indic languages?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.89,ashutrv
Looking for sentence-transformer libraries that I can use locally with JavaScript,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,Bright_Mission_8279
"MultiToken: Embed arbitrary modalities (images, audio, documents, etc) into large language models",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,sshh12
any ideas on how sites like Grok and Perplexity return the most recent relevant result?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,shafinlearns2jam
Building a super-simple memory service for language models,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.93,andyndino
Is there any demand for a Shared Public Contextual Database for RAG?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,niksteel123
Using Generated Q/A Pairs for Fine Tuning? Is It Worth It?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.87,Simusid
"ChatGPT is a Lazy Piece of Shit, CodeBooga Rules",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.83,xadiant
"Ok, I’m just curious of the security risks?",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.65,lordlysparrow
End-to-End Encrypted Local LLMs,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.89,MoneroBee
Llama 8k context length on V100,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.83,HopeElephant
"What are the differences/similarities between Llama 7B, 13B and 70B?",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.82,Equal_Newspaper_1000
Any other fun local AI tools other than ooba and automatic1111?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,skocznymroczny
"Indexing 900Tb as fast as possible, any advices ?",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.88,ToothOne6699
Dataset Distillation,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.91,Thistleknot
RAG Dataset - 3.7M LA Times headlines and links (1914-2024),Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.98,metaprotium
Does labeling datasets stored in a Vector DB make sense?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.84,Moist_Influence1022
Current best codebase for pretraining a model from scratch?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,ZealousidealBlock330
Live Llava on Jetson Orin,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,nanobot_1000
Thinking about a comprehensive RAG,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.93,onlyalad
"Finetune, RAG or live search",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,e-nigmaNL
What is the best model to talk about ai with?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.72,revolved
Best DB structure for my use-case...,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.67,trojans10
Are any Semantic Search Experts Willing to Opine on the RAG Search Building Code?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.83,AggravatingSyrup8146
Train LLM to think like Elon Musk with RAG?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.25,gptzerozero
Project Launch: GatoGPT - Local LLM inference & management server with built-in OpenAI API,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.94,ElGatoPanzon
"A .gguf chatbot gradio interface experiment, to sequentially chain prompts, scripted in csv file : gpt-sequencer",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.9,dbddv01
Which database to use for semantic search?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.95,CompetitiveSal
Managing Follow up question for retriever based chatbot,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,Optimal_Original_815
Best approach for large custom knowledge base LLMs?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.75,RandomTrollface
RAG,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.67,hungnm009
Text-gen-webui + RAG,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,e-nigmaNL
"Retrieving a list of movies from a natural language query, given their plots",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.75,jlteja
Eploring Methods to Improve Text Chunking in RAG Models (and other things...),Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,BXresearch
Long context Fine tune and AutoGPTQ quantization with rope?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,fappleacts
What's the best use case for phi-1 (~1bn param GPT3.5)?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,docsoc1
Running LLaMa on Google Colab/cloud differences w.r.t local system,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Thanos_nap
MoE expert logging?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.86,LoSboccacc
Confining LLaMA 2's context for RAG QA,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,asakura_matsunoki
Which Linux distribution is best for LLM development?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.7,pan_and_scan
Is it feasible to use a NAS to store a vector database and access it in realtime using a local LLM?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.9,ApprehensiveJob171
"Looking for something better than TinyLlama, but still fits into 12GB",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.88,evranch
RAG oriented fine-tune... Searching for coherence,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,Distinct-Target7503
16k context for OpenAI GPT-3.5 API,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.93,noco-ai
Tech Stack recommendations for running RAG locally on a Macbook with M2,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.92,Frequent-Let231
Do you think there will ever be a time where we reach GPT 3.5 quality LLMs in under 1 billion parameters?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.87,Piper8x7b
How to reduce Hallunications of the outputs generated by the LLMs.,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.75,vm123313223
RAG with KG,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.88,Silver_Equivalent_58
JSON data for RAG based approach,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,Optimal_Original_815
"NF4 inference quantization is awesome: Comparison of answer quality of the same model quantized to INT8, NF4, q2_k, q3_km, q3_kl, q4_0, q8_0",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.84,epicfilemcnulty
Can llama 2 continue pretraining using qlora?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.9,Thistleknot
Similarity RAG Search?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,phillyguy2008
Document search and retrieval apps (preferably full-stack),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.86,curlmytail
Need help - RAG on large unstructured PDFs,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.91,bunny__0
Llama on Azure endpoint online and privacy,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Stunning_Art4243
Tips on optimization when loading GGUF models?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.85,weedcommander
Trouble understanding the implementation of how Multi-modality is “solved” through alignment of CLIP and LLM,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.93,Its_All_Chain_Rules
Feed LLM with local knowledge,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,front-equal
How to deploy/host custom LLM app for production?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,Kukaracax
RAG over Knowledge Graphs,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.95,rikiiyer
How to Get Around Context Limits with Data Files,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,prettyobviousthrow
Llama2 Qualcom partnership,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.87,AstrionX
"The best model for ""Talk to your data"" scenarios?",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.94,Raise_Fickle
How to improve results when using Dolphin mixtral with BakLLaVa multimodal projector?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,stduhpf
Best way to interact with local LLMs from distributed app?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.75,-json-
"txtai 7.0 released: Adds support for graph search, advanced graph traversal and graph RAG",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.98,davidmezzetti
With LLMs we can create a fully open-source Library of Alexandria.,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.87,docsoc1
Replacing LLM of Suno Ai's Bark TTS model with Mistral or TinyLlama?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.9,Independent_Key1940
Can we talk about back and front end settings?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.95,3m84rk
Fine Tuning Step Wise Instruction,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.75,One-Calligrapher1792
Url scraping for llama,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.89,ilt1
Small llm model within 100M to 1B parameter,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,AwayConsideration855
Use Llama2 to Improve the Accuracy of Tesseract OCR,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.98,dicklesworth
Llama 2 chatbot performance for multiple users,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.84,godspeedrebel
What GPU factors boost local performance the most?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,DanInVirtualReality
Can we extend falcon context length like llama?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.94,Spare_Side_5907
What's the best description you've seen for what fine tuning is and isn't?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.83,real_bro
need your input on this,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,karthiceaswar
I finetuned Phi-1.5 and Phi-2 on MathInstruct using MAmmoTH but...,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.87,codys12
How to run base models w. finetuned adapters in LlamaIndex or Langchain?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,salah_ahdin
LLMS4Rec through RAG?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.84,LyPreto
CLEX: Continuous Length Extrapolation for Large Language Models,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,starstruckmon
Shit hardware and shoestring budgets,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.95,AndrewVeee
About to buy Hardware for 7k,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.89,Moist_Influence1022
Ai accelerator hardware is slowly becoming available,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,Combinatorilliance
"Talk me off the ledge, I want to buy more hardware for local LLMs...",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.86,silenceimpaired
Advice on cheap hardware,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.6,Red_Redditor_Reddit
200 000 dollars what in hardware can i buy and build with my students,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.87,dupido
Options for running Falcon 180B on (kind of) sane hardware?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,Chance-Device-9033
Hardware guide,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.75,kerneleus
Hardware Design for LLM Inference: Von Neumann Bottleneck,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.94,saucysassy
Hardware,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.67,theguyabroad
Recommended hardware (Windows or Linux)?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.92,jacek2023
Hardware Recommendations,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.88,AcceptableMacaron497
Hardware needed to run Nous-Copybara 34b quickly?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.87,PurplishDev
Multi GPU hardware selection,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.75,rugg0064
Hardware problem,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.75,2047Escaper
LLM's in production hardware requirements,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.94,purton_i
Apple has an excellent hardware base for local generative AI,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,Balance-
"P40 is slow they say, Old hardware is slow they say, maybe, maybe not, here are my numbers",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.93,segmond
Hardware leaderboard?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.75,lxe
Microsoft and AMD collaborating to improve LLM performance on AMD hardware?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.98,daedelus82
Will this hardware upgrade make sense?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.8,mhaustria
[Hardware] M2 ultra 192gb mac studio inference speeds,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.87,limpoko
recommend hardware for running big LLMs locally?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.71,slashdottir
How do you guys cope with ai addiction? Hardware advice for someone who realizes is too obsessed.,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.71,fluffywuffie90210
This is the new king of LLM hardware with 576 GB of RAM.,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.95,fallingdowndizzyvr
25k reports to analyse - Whats the best model/hardware.,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,driftypixel
What are the capabilities of consumer grade hardware to work with LLMs?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.98,TalketyTalketyTalk
Hardware for local LLaMA for 1.000 - 1.500€?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.88,BonoboAffe
Hardware for LLM,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.92,xynyxyn
Technical question about hardware limits,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,Mobile-Bandicoot-553
What hardware is better for embedding,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.75,pangolinportent
Requesting some performance data for pure CPU inference on DDR5-based consumer hardware,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,involviert
Maybe we will be able to run far larger models on Apple hardware than previously thought,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,Ward_0
Hardware for Startup,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.67,ConceptGT
Mistral how to run it via API on my hardware?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.78,olddoglearnsnewtrick
Best performance for 40-60K€ hardware: lost in the possibilities,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.97,superap101
What is the current best 3B model for low end hardware,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.85,CodeAnguish
Scaling LLM server hardware question,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,Creative-Scene-6743
Hardware question: combining a 3090 and a p40,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,Noxusequal
what kind of hardware would it take to run falcon 180b,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.88,Avocado_Express
Looking for CPU Inference Hardware (8 Channel Ram Server Motherboards),Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,jasonmbrown
🖥️🔮 Future Hardware Options for LLMs: Nvidia vs. Apple?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.83,Prince-of-Privacy
Hardware Q's: Best model performance with 75+ 30 series GPU's?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,divijulius
Building a AI and Data Science Rig - Using Server Hardware + Ampere GPU's?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.94,JustinPooDough
Benchmarks for LLMs on Consumer Hardware,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,catid
program that does sensible model recommendations based on the current machine hardware?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.78,staticfull
A small test I did with falcon-180b-chat.Q2_K.gguf (at home on consumer grade hardware),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.99,frapastique
Cheap options hardware for running LLM ?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.9,makakiel
Is there a way to benchmark hardware performance?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.8,BoeJonDaker
"Quick hardware comparison Ryzen 5700X, RTX 3090, Mac Studio M1",Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.77,Late_as_ever
Hardware explanation,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,That0neSummoner
Prospects for future hardware releases,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.78,WarmCartoonist
On what hardware/setup are you running your local LLM?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,snarfi
Utilize my current hardware or upgrade?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.62,reiniken
What's the biggest size Llama-3 could go to whilst running on consumer hardware?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.72,OldAd9530
Hardware needed for LLaMa 2 13b for 100 daily users or a campus of 800 students.,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,hawaiian0n
Is there any way currently to try out and utilize Local LLM's without Enterprise Grade hardware?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.71,Articulity
A Review: Using Llama 2 to Chat with Notes on Consumer Hardware,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.92,hoperyto
Great info resource for AI on AMD hardware (Linux),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.97,mhogag
Best hardware for <= $7k that allows for speed > GPT3.5,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.73,caikenboeing727
Question about fine-tuning ~1B LLM on low-end hardware.,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.95,SuccessIsHardWork
Best hardware for inference requiring 64GB of memory?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.71,spbike
Seeking Recommendation - Cooling Hardware for NVIDIA Tesla Cards,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,nbuster
Is it possible to get 100% deterministic results across different hardware?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,involviert
Could someone summarise the hardware requirements for local models?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.9,clv101
Hardware for scaling LLM services,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,grantory
Looking for hardware and model recommendations -- data center install,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.75,GWBrooks
Is it worth it to build/buy a machine now or wait for more AI hardware releases? ,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.9,okmine-
Help me understanding needed hardware specs/build for running 120B models locally.,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.89,ShenBear
Hardware requirements for GGML quantization,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.92,terhisseur
How to calculate hardware required for training a model?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.75,aLuViAn87
Effects of long term use on hardware,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.9,cddelgado
What are some models you have been running? On what hardware? and why?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.79,Mafyuh
Is it possible to train BabyLM on consumer-grade hardware?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,fabkosta
What kind of hardware do you need to run LLaMA locally?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.82,Darkhog
Hardware advice - would this be a good starting PC for home LLM / machine learning?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.6,ZachCope
What is the tiniest GPT model one can fine tune on home hardware?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.82,herozorro
Do you run your LLM on your hardware or in a VM?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.63,DuckFormer
Will Llama 3 be equivalent in terms of hardware requirements to Llama 2?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.45,Secret_Joke_2262
Advice on running models on my hardware (4090 R9-3950x 128GB RAM),Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,ReAnimatedCell
Hoping for some advice based on my hardware,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.75,LuckyIngenuity
What are the hardware requirements for the 70B model in actual production?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,rxdaozhang
Does anyone have a price comparison breakdown of running llms on local hardware vs cloud gpu vs gpt-3 api?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.94,trv893
Seeking advice on hardware and LLM choices for an internal document query application,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.89,zasp2300
Hardware requirements to build a personalized assistant using LLaMa,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.96,Ibrahim2714
Question: Hardware development for large transformer models.... ??,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.86,BackwardGoose
What hardware do I need for fine tuning/training?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.83,EnPaceRequiescat
Are there benchmarks out there for comparing hardware?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.72,soleblaze
Need help planning a project with hardware restrictions.,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,Serenityprayer69
How to squeeze more speed with my hardware?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,SebSenseGreen
What's the current best model if you have no concern about the hardware?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,hattapliktir
Hardware resources needed for training vs running local LLMs?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,Cunninghams_right
Hardware recommendations for running stable diffusion and 65b ggml at the same time.,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,Erdeem
"With limited hardware (laptop), what kind of local AI would be most viable?",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.8,neilyogacrypto
What's the best koboldcpp command line/settings for this hardware?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.75,Innomen
What are the variables to take into account to calculate the best model that fit in my hardware?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.97,joaco84
I am very impressed by Claude.AI. What kind of hardware and models do i need to replicate it?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.47,herozorro
"GGML hardware questions, regarding a mid 2010s quad CPU Xeon server based build, with intent of 70b unquantized",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,CanineAssBandit
"Seeking Advice: Optimal Hardware Specs for 24/7 LLM Inference (RAG) with Scaling Requests - CPU, GPU, RAM, MOBO Considerations",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.75,Distinct_Maximum_760
"If I can't afford to buy the necessary hardware to run a high performance model, is there a service that I can use on a monthly basis to host it for me?",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.92,OKArchon
"If I don't care about inference time at all, can I run larger models on weaker hardware? I'm fine with like 5-6 tokens a minute.",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.98,Mescallan
Will local hardware ever be able to compete with companies Google or Bing in the search engine space?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.81,unraveleverything
Tom's Hardware wrote a guide to running LLaMa locally with benchmarks of GPUs,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.96,Gudeldar
Question: Option to run LLaMa and LLaMa2 on external hardware (GPU / Hard Drive)?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.87,SiltoruzExarz
How can I determine my hardware requirements (especially VRAM) for fine-tuning a LLM with a PEFT method? Is there a formula?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,a_fish1
Is it possible to run multiple models simultaneously yet? If so what kind of hardware would I need to be able to pull it off?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.94,sephy009
A fun experiment running concurrent inference in multiple MacBook Pro's to evaluate model and hardware performance. M2 Max 64GB (34B model) vs M3 Max 128GB (67B model),Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.94,LocoMod
"What is the major difference between different frameworks with regards to performance, hardware requirements vs model support? Llama.cpp vs koboldcpp vs local ai vs gpt4all vs Oobabooga",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,gpt872323
There's a 20% off Ebay coupon right now. It works on a lot of stuff like GPUs. Worth a try to save money on GPUs and server hardware if you are trying to pick up a EPYC machine.,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.89,fallingdowndizzyvr
"Asking for hardware recommendations for a personal machine capable of running +70B models. With cloud options I have to re-download the model every time. Should I bite the bullet and get Mac Studio M2 Ultra ($7000 after tax), or build a PC? What specs do you recommend?",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.86,nderstand2grow
Will we see consumer grade AI accelerator cards 2024?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.94,pure_x01
Building the best LLM rig for $10000,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.93,peace-of-me
Testing LLamA 2-7B's character impersonation abilities before I buy hardware to run it locally. The idea is to include this character (a perpetually grumpy individual prompted to be so) in an RPG game to generate dialogue through event prompts.,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.91,swagonflyyyy
One-bit quantization is a thing now,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.98,BalorNG
Anything going on with minimal (virtualized?) standardized hardware? (Kind of like demoscene stuff). Probably has benchmarking and fast-iteration applications? I'm talking setups that would run on any random laptop (without a gpu). Maybe throw in Nix for reproducibility?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.8,blueeyedlion
"Helpful VRAM requirement table for qlora, lora, and full finetuning.",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,Aaaaaaaaaeeeee
Why run LLMs locally?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.82,jsfour
"What do you think about the LLM market in the next 12 months? Comments, reasoning, data points are highly welcome! I think LLM's with equivalent to today's GPT-4 performance will get 10x cheaper. Do you think ASIC hardware may appear for crunching transformer models (so no Nividia needed)?",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.77,goproai
review of 10 ways to run LLMs locally,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.9,md1630
Why are you running local models? What are you doing with them?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.94,ExtremelyQualified
Is anyone else super eager to upgrade their computer but they're also trying to patiently wait to see what might come out? What's your game plan?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.98,ThePseudoMcCoy
What is stopping us from creating an open source GPT-4 & Gemini Ultra? (Or better),Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.89,askchris
"I have a dataset of around 100 stories, each around 1000-2000 words each. Can I train a model to generate new stories in similar style and content?",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.96,thatavidreadertrue
is 4 rtx 4090s better than single a6000 ada?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.94,fx76
Fastest 7GB Model?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.58,MarsCityVR
Tim Cook speaks about AI at the Apple shareholder meeting. More on Generative AI later this year. Also that there is no better computer than the Mac for AI.,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.8,fallingdowndizzyvr
Budget of 5-10k to get a good performance,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.94,Available_College_79
"Given the rapid pace of progress, how far are we until local models approach gpt4? Can local models ever be equivalent?",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.91,TopRecognition9302
"🐺🐦‍⬛ LLM Comparison/Test: 2x 34B Yi (Dolphin, Nous Capybara) vs. 12x 70B, 120B, ChatGPT/GPT-4",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.99,WolframRavenwolf
If CPU to GPU memory transfer is a bottleneck why is there no unified silicon from NVIDIA?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.94,discretemathematics
How long would it take for Local LLMs to catch up with gpt-4? Few or several years?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.97,jl303
"Let's discuss how technology and society would change with high-end, low-power LLMs",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.91,platistocrates
Build my own version of HackerGPT,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.93,SampleTextzzzzzzz
Benchmark post,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.91,PrzemChuck
How much more can the current model sizes improve?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.95,Admirable-Star7088
Suggestions for a > 24GB VRAM build?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,sinsro
Can I run Mixtral 8x7b?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.85,Deep-Yoghurt878
Open Source RAG - 500 users,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.9,pathfinder6709
5x 3090 LLM rig opportunity – stupid decision?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.89,inquisitive-spaniard
Opinions on Mixtral 0.1 8x7b and Mistral 0.2 7b,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.94,Admirable-Star7088
Models Megathread #2 - What models are you currently using?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.99,Technical_Leather949
"How do the 7B, 13B, 30B, and 65B parameter models compare?",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.9,Akimbo333
256 GB RAM + Mid GPU vs High End GPU? ,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.69,An_Original_ID
I have some questions,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.9,EvokerTCG
Can you help me understand where the advantages in having an opensource model with the power of GPT-4 would lie?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.73,hugo-the-second
Fine-tuning on AMD? (7900XT),Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.84,Exotic-Investment110
Meta - Other than RP what are you folks doing?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.84,cleuseau
70B build at $4000,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.93,flemhans
"If you owned a nvidia tesla a100, what would you do with it?",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.87,mehrdotcom
What's the best way to beat GPT-5 with Open Source & Distributed Compute?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.63,askchris
Running Llama 70B - Costs and Approaches,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.88,JustinPooDough
AMD compability?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.86,No-Dot-6573
"Would it be possible to ""live train"" an llm on the current conversation, so it's basically ""self learning""?",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.88,Frequent_Valuable_47
Can the RX7900XT (Or any other gpu with 20gb of vram) now run 30b models?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,ethanol_addicted
Replacing ChatGPT 4 - $20 Subscription,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.72,chkpwd
FlashDecoding++: Faster Large Language Model Inference on GPUs,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.97,ninjasaid13
If I want to train a local model on par with chatGPT how difficult would it be and how much would it cost?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.43,Old-Calligrapher1950
Is fine tuning QLoRA still state of the art?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.88,HotRepresentative325
Chat with RTX is VERY fast (it's the only local LLM platform that uses Nvidia's Tensor cores),Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.77,TechExpert2910
What is the best current Local LLM to run?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.97,KaihogyoMeditations
PC configuration to run a llama2 70B,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,SpatolaNellaRoccia
Anyone running dual 3090?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.89,Remarkable_Ad4470
I've gotten allocation on an enterprise server. Which model type has fastest inference on pure CPU/RAM ?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.97,gentlecucumber
Any hope for the future of open source?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.72,open_23
Especialized Chips to run AI only?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.67,Sword2410
Is there any organization working on an open source 175B model? If not what is the extra compute required compared to current 70B models.,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.92,timedacorn369
[Project] MLC LLM for Android,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.99,crowwork
Refusals and excuse-making is the poison pill for open source LLMs.,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.95,Lumiphoton
2024: Small Models Will Be Insane,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.9,FuckShitFuck223
What would be the smallest open source llm models that are still of reasonable function? ,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.64,Maelstrom100
What services do you guys use for hosting models?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.83,Sixhaunt
Squeezing performance out of models with no video card.,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,NBehrends
Question about data privacy,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.66,kucukkanat
What should I monitor while running LLMs?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.82,Fr4y3R
Local LLM Specifications,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.57,mobileappz
The Dilemma of AI Accelerators: Bridging the Gap for Affordable Solutions,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.88,greysourcecode
Collaborative renting server for LLM,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.91,docloulou
"Is Shared memory on a laptop better than regular desktop DDR5. The 4090 has 64GB of ""total"" memory?",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.79,Future-Freedom-4631
"If I have $200 to burn, should I buy 128gb worth of ram or a used gtx 1080?",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.79,oldusernumber1
Finetuned llama 2-7b on my WhatsApp chats,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,KingGongzilla
2-bit Mixtral via HQQ ( requires only 13B of runtime RAM ( runs on RTX 3090 ),Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.93,sightio
Help wanted with a project,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.5,Rear-gunner
Mixtral_7Bx2 MoE GGUF models Q2-Q5,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,ZHName
Euryale 70B vs Nous Capybara 34B for chatting?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.78,ll_Teto_ll
Experiences with Caching in llama.cpp,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.96,Frequent_Valuable_47
Most cost effective GPU for local LLMs?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.9,Mefi282
[Project] MLC LLM: Universal LLM Deployment with GPU Acceleration,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.96,crowwork
What is the optimal model to run on 64GB +16 VRAM?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,kitten888
Silicon-Maid-7B is surprisingly good for its weight.,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.92,ai_waifu_enjoyer
LocalGPT on NVIDIA A30 24GB.,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.78,buckybeeee
Goliath-120B - quants and future plans,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.98,AlpinDale
RTX 3060 6GB GPU: Am I doomed or is there some some version of LLaMA I could run?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.89,phlame64
AL/ML Model - ADVICE REQUESTED,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,StreamConst
Let's discuss Tinygrad,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.76,Exotic-Investment110
Don't Buy an AMD 7000 Series for LLaMA Yet,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.98,friedrichvonschiller
"HW Resources regarding Inference vs. Training, Fine-Tuning, etc.",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Smeetilus
Is it possible to host two different models at the same time on the same machine?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.67,ishtarcrab
Quip# quantization of Tess-M,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,MLTyrunt
[Question] - Chat UI for Mistral API,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,ghac101
Some rumors are claiming this Mistral-Medium got leaked,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.79,Shir_man
Three things I think should get more attention,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,ExaminationNo8522
Question about Training Llama 13B GGML Models on Local Documents,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.94,GiantFlyingPikachu
Discussion about Hadware Requirements for local LlaMa,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.83,Plane_Discussion_924
Which Macbook Pro to buy for running an LLM locally? I created a buyer's guide to help you decide.,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.73,fabkosta
24GB GPU OOM while qlora peft finetuning Llama2 7B,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.8,DirectionOdd9824
Local LLM + Image Gen = Like GPT 4 & Dalle 3 ?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.88,Yuri1103
Fine tuning for dummies,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.96,Jenniher
Cheapest and best way to run LLM online on an on-demand basis?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.9,broodysupertramp
Free ChatGPT (not the paid version) locally?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.47,crav88
Fine-tuning Xwin-LM 70B?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,ll_Teto_ll
Looking to build a new system for local AI model/experimentation and possibly training,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.67,Dry-Vermicelli-682
"Upcoming APU Discussions (AMD, Intel, Qualcomm)",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.92,zippyfan
Falcon 180B minimum specs?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,Beautiful-Answer-327
"Any <13B model able to answer the simple ""Apples today"" vs ""Apples yesterday"" trick question?",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.83,phr00t_
GPU bloat stifles AI,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.46,chipstrat
would 500 parallel GPU workers be useful to you?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.63,Ok_Post_149
"Local LLM for ""Hot Dog or Not Hot dog"" kind of fact checking",Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,Capital-Alps5626
Does a token calculator exist?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,and69
How to add new languages to existing models?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.83,CodeAnguish
llama.cpp generation with (older) GPU is *slower* than pure CPU?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.92,dual_ears
Qualcomm Ai accelerators: ANYONE know ANYTING other than this marketing material? sounds quite intriguing...,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,leschnoid
bGPT - Byte-Level Transformer,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,Glat0s
Thoughts after building a text-adventure game using local models,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.96,antimateusz
100k context windows. How soon before you can run them locally?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.97,Reluctant_Pumpkin
Need help selecting platform for fine tuning 7B model.,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,dark_surfer
LLM GPU buyer beware?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.66,coaststl
"Deepseek 67b is amazing, and in at least 1 usecase it seems better than ChatGPT 4",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.93,SomeOddCodeGuy
Blockwise Parallel Transformer for Long Context Large Models,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.99,IxinDow
Fine tuning on Apple Silicon,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,Acrobatic-Site2065
Thinking about purchasing a 4090 for KoboldCPP... Got some questions.,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,wh33t
I'm convinced now that “personal LLMs” are going to be a huge thing,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.96,docsoc1
Serious inquiry: I've been tinkering a lot with finetuning and was wondering if it would be worth to buy a V100 of my own,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.96,holistic-engine
Benefits of self-hosting that carry over to professional work?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.94,LostGoatOnHill
Shoutout to a great RP model,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.96,Meryiel
"Help me choose: Need local RAG, options for embedding, GPU, with GUI. PrivateGPT, localGPT, MemGPT, AutoGen, Taskweaver, GPT4All, or ChatDocs?",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.93,TheWebbster
Nous-Hermes-2-Mixtral-8x7B DPO & SFT+DPO out! Matches perf of Mixtral instruct + supports ChatML (and thus System prompt!),Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,ablasionet
"Xbox series X, GDDR6 LLM beast?",Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.67,randomqhacker
What would be the best way to go about making a C# model?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.63,maxwell321
Inference with an iGPU,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.7,Sloppyjoeman
Is renting GPUs only possible because we still don't have a killer open source model?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.88,Agusx1211
I have 200+ 30 series GPUS and I'd like to use them to generate income using LLaMA where do I go to get started?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.26,Inevitable-Syrup8232
What's the latest on 1B models and Mac Mini M2 efficiency?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.74,IEatGnomes
Can time compensate for lack of power?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Sandy-Eyes
GPU Recommendations,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.71,Second26
End-to-End Encrypted Local LLMs,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.89,MoneroBee
Autogen + mistral small/moe/mixtral,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.91,Unusual_Pride_6480
Best model-setup for CPU-only?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,andromedians
Is there a high quality desktop UI for MLC Chat?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.78,ihexx
A local LLM is the ultimate doomsday device,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.9,Ninjinka
Deploying LM/LMM to edge device,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,rower22
Best Local LLM For low end device coding,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.75,Low-Plastic-2399
I've created Distributed Llama project. Increase the inference speed of LLM by using multiple devices. It allows to run Llama 2 70B on 8 x Raspberry Pi 4B 4.8sec/token,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.98,b4rtaz
"Hugging Face, the GitHub of AI, hosted code that backdoored user devices",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.43,WonaBee
Run models on a real Android device with Qualcomm AI Hub,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.91,ephemeralshot
(Meta research) MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.95,llamaShill
Native LORA finetuning on Apple Devices (New MLX Framework) 😲!!,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.83,phoneixAdi
What is Apple thinking? Why are they so radio silent about LLMs despite having the edge over Microsoft/Google to bring local LLMs to iDevices?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.68,nderstand2grow
LLM Models for Edge Devices,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.81,thesithlord27
"A completely open-source AI Wearable device like Avi’s Tab, Rewind’s pendant, and Humane’s Pin",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.93,No-Camel-3819
Someone know any projects about an alexa like device build with llama,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.83,kroryan
llm on mobile devices - reading pdf usecase,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,Helloworld1907
Splitting models and device map.,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,a_beautiful_rhind
MediaTek Leverages Meta’s Llama 2 to Enhance On-Device Generative AI,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,noiseinvacuum
"Introducing LlamaEdge — lightweight & portable LLM tools for your local, edge & server devices.",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.88,Melinda_McCartney
"Opentensor and Cerebras announce BTLM-3B-8K, a 3 billion parameter state-of-the-art open-source language model that can fit on mobile devices",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,CS-fan-101
FP4 quantization state not initialized. Please call .cuda() or .to(device) on the LinearFP4 layer first.,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,AstronomerChance5093
Has someone tried LLMFarm for native inference on iOS devices?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.9,frapastique
My open-source & cross-platform on-device LLMs app is now available on TestFlight & GitHub. Feedback & testers welcome.,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.55,BrutalCoding
How do I run Stable Diffusion and LLMs from my PC on my mobile device? Offline and private ways?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.56,ThrowawayProgress99
How to implement gpu/cpu offloading for text-generation-webui? [custom device_map],Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,SomeGuyInDeutschland
"humane ai pin dropping next week, how come I haven't heard more news abt this? seems like very capable tech packed into such a small device if it indeed works as they are marketing it--- i'm curious to see what type of latency it has for voice commands.",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.71,LyPreto
What am I doing wrong?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.96,slykethephoxenix
"I'm about to open source my Flutter / Dart plugin to run local inference on all major platforms. See how it runs on my personal Apple devices: macOS (Intel & M1), iOS, iPadOS. Next up: Android, Linux & Windows. AMA.",Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.91,BrutalCoding
"Fellow nerds of reddit, I somehow fried my rig using TextGen Webui.",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.72,ThrowawayEmail000
2x Teslas in an OEM system,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.78,ConcaveTriangle5761
Mbp m3,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.78,tshawkins
Difference between merged model (with adapters) and load adapters on top,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.83,No_Organization_2634
Two RTX 3060 for running llms locally,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.88,arc_pi
Gemini Nano is a 4bit 3.25B LLM,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.98,Amgadoz
Why is the idea of more than one participant so foreign to LLMs other than GPT4?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.42,TradingDreams
Any way to optimally use GPU for faster llama calls?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.9,todaysgamer
Merging LoRA with Mistral models?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,ququrydza
"Fine tuned coqui XTTS voice, how to use the model.pth?",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.8,hwknd
I wonder theres way to run LLM without loading on ram,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.69,wjohhan
GPU-Accelerated LLM on a $100 Orange Pi,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.96,EmotionalFeed0
"You can mix different brand GPUs for multi-GPU setups with llama.cpp. Here are some numbers for a Nvidia/Intel mix. Also, the A770 works really well now.",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.95,fallingdowndizzyvr
oobabooga Update broke loading u/The-Bloke huggingface models?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.8,shzam123
Maybe we will be able to run far larger models on Apple hardware than previously thought,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,Ward_0
[Project] MLC LLM for Android,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,crowwork
Apple CoreML,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,krazzmann
Speculation. Could Apple pull off an end run around everyone else and offer low cost large model inference?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.69,fallingdowndizzyvr
"Is anyone inferencing on something like an Intel nuc, barebone or similar formfactor?",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.8,Frequent_Valuable_47
Guide: Installing ROCm/hip for LLaMa.cpp on Linux for the 7900xtx,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,Combinatorilliance
A fine tuned Llama2-chat model can’t answer questions from the dataset,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.67,celsowm
TinyLlama-1.1B: Compact Language Model Pretrained for Super Long,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,Either_Ad_1649
Is Llama 2 7B or 7B Chat better for a talking Teddy Bear?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.87,danl999
Awful quantisation outputs with V100,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,gpu_go_brrr
Skill Issue while running GPU accelerated llama,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.8,MDCurrent
Home setup for future home model use,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.72,Sockosophist
Llama finetuning question,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Alert_Record5063
What's your best monitor tool?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.67,Mundane_Definition_8
Optimum Intel OpenVino Performance,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.91,fakezeta
ExLlamaV2: The Fastest Library to Run LLMs,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.98,alchemist1e9
Looking for a lightweight Model ideally fine-tuned for generating compliments or similar,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.9,PSRD
Out of memory using multiple GPUs,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,EffectiveFood4933
Best model to convert voice commands to JSON?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.83,slykethephoxenix
Is it possible to process layers one by one on a low vram gpu?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.57,moozoo64
Why are MLX and Ollama way faster than PyTorch on M1 Mac?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.78,juanluisback
Having a Local LLM interact with an API,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.5,Iseenoghosts
Simple demo app of TinyStories-1m that runs locally on iOS,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,seattleeng
Is Llama.cpp Using GPU's on my M2 Max?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,Mbando
No CUDA GPUs are available error on text generator webui,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,OvercookedSatellite
MLC-LLM Chat vicuna-Wizard-7B-Uncensored-q3f16_0,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.9,jetro30087
Waiting times for A100 hw?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,BreakIt-Boris
Should I start this project?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.82,DatOneGuy73
A new type of transistor is more efficient at (some) machine learning tasks,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.94,neph1010
Apple releases MLX for Apple Silicon,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.91,Tommy-kun
7B models cannot fit in RTX 4090 VRAM (24GB),Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.71,DrJokeTech
Let's create a 65B benchmark in this thread,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.98,Big_Communication353
Has Anyone Successfully Utilized the Neural Networks API on Android for LLMS with EdgeTPU?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.84,dewijones92
What are your thoughts on the future of LLMs running mobile?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,Tree-Sheep
"Mistral finetuning , Loss increases crazy",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.81,Secure_Track8616
"Guys, why are we sleeping on MLC LLM - Running on Vulkan?",Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.91,APUsilicon
Best way to make models understand certain language without fine tuning,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,laveriaroha
Koboldcpp linux with gpu guide,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,amdgptq
M3 Max/Ultra vs RTX3090 vs RTX4090 with large context windows?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.9,Some_Endian_FP17
"Is Shared memory on a laptop better than regular desktop DDR5. The 4090 has 64GB of ""total"" memory?",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.8,Future-Freedom-4631
How to run base models w. finetuned adapters in LlamaIndex or Langchain?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,salah_ahdin
What 7b llm to use,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.82,bot-333
CLblast is nice on crap systems!,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.92,thebadslime
Why do prompts work so differently depending on the model used?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.6,DoktorMerlin
Estimated Time for SFT Fine-Tuning of Mistral-7B Model,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.87,Aron-One
"""Guidance"" a prompting language by Microsoft.",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,QFTornotQFT
Llama 2 7b-Instruct on 2 RTX 2080 Ti GPUs,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,T3h_Laughing_Man
Accelerate not working with merged model?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,rmt77
flan T5-Large just gives the context as the response,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,IamFuckinTomato
How long is it taking you guys to run a 7b llama 2 model?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.43,TheHunter920
Any decent open source voice assistants (ChatGPT or local llm) that allow interrupting?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.85,PurplishDev
Help needed on building doc translation LLM,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,dodo13333
ONNX to run LLM,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.5,hungrydit
NVIDIA RTX 5880 48GB GDDR6 Ada Generation Graphics Card,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.44,sapporonight
Best backends for running models on Android?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,GamerWael
Guide to running llama.cpp on Windows+Powershell+AMD GPUs,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.94,fatboy93
"Intel AI built into CPU's, is it at all useful for text or image generation? What is it?",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.92,SGAShepp
Weird Dual GPU setup in Ooba,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,disarmyouwitha
I made a language-agnostic function calling (open source),Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.96,mcharytoniuk
"Finally got a model running on my XTX, using llama.cpp",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.97,TeakTop
Utilizing two different size GPUs for fine-tuning,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,ali0100u
Exploding loss when trying to train OpenOrca-Platypus2-13B,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,Crafty_Charge_4079
Slow LLM speeds on RTX 4090,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,Reign2294
Loading LLaMA2-70B model,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.9,1azytux
Anyway to use Runpod on android?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.5,theshoelesschap
Grammar is key to next generation AI game dev?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.98,EVLG2112
Half Precision (e.g. no quantization) Phind V2 CodeLlama 34B running on Mac M1 at 8.6 tokens per second,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.77,Thalesian
"Vicuna 13b on RK3588 with Mail G610, OpenCL enabled. prefill: 2.3 tok/s, decode: 1.6 tok/s",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,EmotionalFeed0
"Nearing Q4 23, what's the best web UI frontend?",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.93,ctrl-brk
What are the limits of LLM?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.82,throwaway275912
Any interesting LLM project ideas?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.69,theoretical_entity
What is the current best 3B model for low end hardware,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.81,CodeAnguish
"Blind Chat - OS privacy-first ChatGPT alternative, running fully in-browser",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.88,hackerllama
Which is the smallest Llama model out there?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.95,AnonymousD3vil
Unable to train LLaMA2-7B-HF in an RTX 3050,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,CollectionFar336
"MiniCPM: An end-side LLM achieves equivalent performance to Mistral-7B, and outperforms Llama2-13B",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.75,x_swordfaith_l
open llama failed to predict eos token?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,130L
Running Llama2 on Android,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.78,Deep-View-2411
"If you train QLora in Transformers (PEFT) and 4-bit, you need to do this --->>>",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.91,FPham
An Alternative Approach to Building Generative AI Models,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.9,buildinstuff5432
"Advice for model to run on laptop gtx 1060 (6gb), i7 8750H, ram 16gb",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.83,blacktie_redstripes
Inconsistent Token Speed on Llama 2 Chat 70b with Exllama,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Used_Carpenter_6674
Introducing Vaartaalaap: A Chatbot UI for Local LLM Servers,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.81,Reasonable_Ad9033
The Acer Intel A770 16GB GPU is now $250. You won't find a better new 16GB GPU for less.,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.91,fallingdowndizzyvr
[Project] Making AMD GPUs Competitive for LLM inference,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.96,yzgysjr
LLM.swift library lets you interact LLMs on iOS easily,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.95,eastriver0720
"Could I serialize models in current ""state""?",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Woitee
3080 16gb laptop + desktop gpus?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.6,Aaaaaaaaaeeeee
The S24 will have gemini nano onboard!,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.91,Amgadoz
Speed difference not matching file size between quants?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.72,Tree-Sheep
D: What prompts do you use to evaluate new LLM capabilities?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.94,head_robotics
Silent Release: Llama2 7B on Snapdragon Gen2 with 8 tok/s,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.93,YYY_333
Success with a local voice chat agent,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.99,dkjroot
How to compile models for MlC-LLM,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,jetro30087
LLaMA for poor,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.89,pratiknarola
"Using a NUC, SBC, or SFF for LLMs?",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.67,Inous
Lightweight LLama variants for Mobile applications,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.83,thesithlord27
"Need Help Optimizing Language Model Performance on Nvidia Jetson AGX Xavier
",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.75,iamnotdeadnuts
"Noob: I try to fine-tune a LoRA with a very small dataset (10 samples) on Oobabooga, the model never learns.",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.82,tgredditfc
"Open Inference Engine Comparison | Features and Functionality of TGI, vLLM, llama.cpp, and TensorRT-LLM",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.94,andrewlapp
I built my own android chatting frontend for LLMs.,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.98,----Val----
I love hallucinations,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.96,Fusseldieb
What's the latest on 1B models and Mac Mini M2 efficiency?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.74,IEatGnomes
Fine-tuning for custom domain knowledge,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.98,rinse_repeat_wash
NVidia vGPU on esx,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.83,BreakIt-Boris
Dual 3090 ti GPU's on Ubuntu Desktop x64 help,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,snowmobeetle
Qnap TS-264,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.38,id278437
Model parallelism with LoRA,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,jeremyhoward
HF transformers vs llama 2 example script performance,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,FormerAlternative707
Long load time using from_pretrained?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.75,ThineGame
num_beams > 1 breaking my model (Open-LLaMA7b - Alpaca-finetuned),Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,BuzzLightr
How to speed up tokenizer loading speed for lmsys/vicuna-13b-v1.3? (Takes 3 min),Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,ToeAdministrative493
How to find good llama.cpp command line parameters,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,TypeDeep4564
Exllama on windows using CPU,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,HeavyDiamond8069
"Offline voice assistant using Ollama API, Mistral 7B, and Whisper",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.99,PleasantYoung513
Llama2 Qualcom partnership,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.86,AstrionX
1x rtx 6000 ada or 2x 4090 ?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.67,EasternBeyond
"I love running locally, but",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.73,__Maximum__
Guide for oogaboooga on amd using rocm gpu on linux ubuntu and fedora,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.89,thesawyer7102
[HELP] It's there a way to make Llama 2 model generate text token by token or word by word like what ChatGPT does?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.9,MrForExample
Best prompt and model for fact-checking a text (disinformation/fake-news detection),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.6,Fit_Check_919
Commercial model + API question,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.5,akuhl101
Is the Nvidia Jetson AGX Orin any good?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,zippyfan
Can't use multi-gpu with 8x A100 80GB,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.81,nhanha_castanha
Using LLMs to build custom Operating Systems?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.67,Wroisu
A Free AI Scribe Project I am Working on! Please Provide Feedback!,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.85,ThrowAway12461246124
"Is there an equivalent of ChatGPT ""Plugins"" for local LLMs Web UIs? Like Code Interpreter, Plot Generator (using matplotlib), etc. I know Langchain and others claim to use ""Tools"", but those are not as capable as ChatGPT's Plugins.",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.78,nderstand2grow
Resume training from a checkpoint with different hyperparameters when training with PEFT,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.67,01jonathanf
"Since the launch of the AI Pin, have there been any alternatives announced that are open platform?",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.7,spar_x
"Low token/s count on 7b models, is this normal",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.75,nono577
How to fine-tune Llama 70B fp16 on 8x A100 80GB?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.87,kupo1
Apple has an excellent hardware base for local generative AI,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,Balance-
"Documentation Paradigm for Large Language Models (LLMs): Log Today, Train Tomorrow",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.83,phoneixAdi
Experience of setting up LLAMA 2 70B Chat locally,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.97,UncertainLangur
Ryzen Direct Memory Access in OpenCL mode,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,Apprehensive_Sock_71
"Anyone working on a ""tiny"" version of Mixtral-8x7b",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.88,bonthebruh
Seeking Recommendation - Cooling Hardware for NVIDIA Tesla Cards,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.92,nbuster
llama-cpp-python not using GPU,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,Artistic_Okra7288
"New llama-cpp-python out, with performance patches",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.99,CodeGriot
Small local model for git/shell,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,Tyson1405
Anyone working on linking local Ai with Home Assistant?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.81,TheSilentFire
Single RTX 4090 FE at 40 tokens/s but with penalty if running 2 get only 10 tokens/s. Confirmed with Xwin-MLewd-13B-V0.2-GGUF.,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.81,easyllaama
"When LLM doesn’t fit into memory, how to make it work?",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Robert-treboR
How to optimize fine-tuning of Llama-2 13B?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,mokamokatin
Some advice on running Guidance?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.67,iChinguChing
"Running RedPajama and other open LLMs on phones, browsers and AMD/NV/Intel GPUs",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.97,yzgysjr
How to use Rocm with windows?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.75,SimRacer101
Distributed Inference and Fine-tuning of Large Language Models Over The Internet,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,ninjasaid13
"Can I run an LLM that takes up no more than 1-4GB of RAM / VRAM and have it answer questions using my notes, or is that unrealistic?",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.82,TheTwelveYearOld
Most advanced LLM for a Jetson Orin Nano?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.84,fission4433
Llama 2 as a local copilot!!,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.95,tarun-at-pieces
"Used GPT4ALL for the first time and wondering if I could somehow feed it (technical) PDFs and turn it into ""sidekick"" for embedded programming?",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.83,SaarN
llama2-7B/llama2-13B parameter model generates random text after few questions,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,Optimal_Original_815
Running Llamacpp with an RX5700,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.66,MrUserbox
What is the best API right now for self-hosted LLM usage?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.57,InkognetoInkogneto
Running Llama-65B with moderate context sizes,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.67,Xir0s
Yet another quantization method: SpQR by Tim Dettmers et al.,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.99,rerri
Why does my fine tuning of Mistral 7B Instruct not stop?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.67,ProjectProgramAMark
Help to use pipeline conversational on mistral instruct v0.2,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.6,celsowm
Multi-Model Multi-GPU Architectures: LLM + ASR + TTS,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.92,grim-432
Anyone know the nuances of running a model metal performance shader?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.8,HatLover91
Keep running out of memory when pre-training (without LoRA) a model 7B on 2 A100 80GB GPU?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,scienceotaku68
Code-LLaMa: Stuck after loading,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,Kaushik2002
Text Generator webui - how do I select which GPU to use?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.81,OvercookedSatellite
Improving Falcon-180B Performance,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.92,Simusid
Building an IDE with native support for Open Source models,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,ragingWater_
Error in load_in_8bit when running alpaca-lora using 3080,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,Full_Sentence_3678
I am looking for information regarding Running llama on a zen4 or xeon 4th generation cpu? Or alternative no gpu suggestions (for 180b falcon),Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,jasonmbrown
Most performant option to run 13b LLM locally as a personal assistant for under $300 USD,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.55,Envoy0675
Time to first token (TTFF) with llama.cpp vs. vllm,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.6,silvanoluciano
Running [Crataco ]Pythia Deduped GGML on 4 GB Ram Laptop,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.71,Merchant_Lawrence
"Working on a QLORA hub for model personalities, help needed",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,Lang2lang
Best setup and Settings for a Beginner?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,ChrisX930
Retrieval Augmented Generation optimised Llm's,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.98,ale10xtu
Guanaco fine-tuning for classification,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.99,aidalovegood
Engaging topics for conversations in a small local workshop,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.75,besabestin
LLaMA-4bit inference speed for various context limits on dual RTX 4090 (triton optimized),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.95,MasterH0rnet
Issue Loading 13B Model in Ooba Booga on RTX 4070 with 12GB VRAM,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,alexthai7
Finetuned Mistral outputting multiple Q and A responses.,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,No-Point1424
What's the reasonable tks/s running 30B q5 with llama.cpp (13900K + 4090) ?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,dostorm
Forget OpenAi function calling (openhermes + outlines),Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.91,dulldata
Help with objective tokens per second measurement,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,zDraco_Meteor
CogAgent: A Visual Language Model for GUI Agents,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.97,llamaShill
I am not special. Though I seek guidance from those that are.,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.47,ThemWhoNoseNothing
KoboldCpp - Combining all the various ggml.cpp CPU LLM inference projects with a WebUI and API (formerly llamacpp-for-kobold),Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.99,HadesThrowaway
Can we create a megathread for cataloging all the projects and installation guides of Llama?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.98,utkvishwas
QLoRA with GPTQ problems,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,Lewba
Mark Zuckerberg on upcoming LLaMA v2,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.98,llamaShill
"Would an E-GPU work as good on Linux than an internal GPU, same model?",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,SirLordTheThird
Integrating Diverse Language Models in a single interface?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,Sibra_0000
"Using system RAM as ""swap"" for GPU?",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,AlpsAficionado
Issues with the starter code for codellama,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,FinePlant17
Falcon 180B GPTQ Model on Multi-GPU Setup with RunPod,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,Wrong_User_Logged
Hugging Face community blogpost: 🕳️ Attention Sinks in LLMs for endless fluency (related to StreamingLLM),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.99,CubieDev
Training LLaMA-2 for Keyword Extraction,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.88,TaleOfTwoDres
"Trying to load togethercomputer_LLaMA-2-7B-32K with fully loaded context but it OOMs, but I should have enough VRAM?",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.94,tenmileswide
How do I load a gptq LLaMA model (Vicuna) in .safetensors format?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.84,KillerX629
Codelamma 7b code completion giving multiple responses and i only want one?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,llamasaresavager
A simple Huggingface Downloader,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.9,q5sys
New badass model OpenAssistant/llama2-13b-orca-8k released 🎉,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.98,FHSenpai
"New Oobabooga Standard, 8bit, and 4bit plus LLaMA conversion instructions, Windows 10 no WSL needed",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.97,Inevitable-Start-653
Building koboldcpp_CUDA on Linux,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,Current-Voice2755
Thunderbolt and multiple eGPUs,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,throwaway075489
Trouble creating document Q&A chat bot,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,RidesFlysAndVibes
How to perform multi-GPU parallel inference for llama2?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.92,cringelord000222
Speculative Decoding in Exllama v2 and llama.cpp comparison,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,lone_striker
Model not responding and just says (is typing...),Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,FriendDimension
STOP using small models! just buy 8xH100 and inference your own GPT-4 instance,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.94,Wrong_User_Logged
2024: Small Models Will Be Insane,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.9,FuckShitFuck223
Phi-2: The surprising power of small language models,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,super-helper
Built a small quantization tool,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.97,Potential-Net-9375
Favorite small models?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.91,replikatumbleweed
Experimenting with small language models,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.98,IffyNibba01
Orca 2: Teaching Small Language Models How to Reason,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.99,Memories-Of-Theseus
"Small Benchmark: GPT4 vs OpenCodeInterpreter 6.7b for small isolated tasks with AutoNL. GPT4 wins w/ 10/12 complete, but OpenCodeInterpreter has strong showing w/ 7/12.",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.96,ciaguyforeal
The LLM Creativity benchmark (initial results: small models + miqu),Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.94,ex-arman68
Devs: What small building blocks would help you build better AI apps?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.98,AndrewVeee
llm's on small/portable laptop?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.76,fietsrad
"Small model for ""Open Interpreter"" use recommendation?",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,AJ47
Specific small models and parallel use,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,Full_Operation_9865
"CodeGen2.5: Small, but mighty",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.99,Acrobatic-Site2065
What's the best/practical use you've found for (Llama 2) 7B small models?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.98,nuketro0p3r
What are your favorite use cases for small models around 3b and less.,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.85,Noxusequal
LLM Assistant with function calling - Just a small test project I made,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.99,Rivridis
QLoRA hyperparameters for small fine-tuning task,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.87,ilfron
What can we achieve with small models ?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.92,Sufficient_Run1518
Small dataset for primary LLM training,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,Tricky-Box6330
"What are your favorite ""small"" models for text comprehension?",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.86,Noxusequal
Small Model similar to Character AI,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.75,Substantial-Club-582
"Quick comparison of mistral-small, mistral-medium, GPT-3 and GPT-4",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.99,Kinniken
Small AI Dev Tools Pt 1: Context Manager,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.96,AndrewVeee
Looking for a workstation recommendation to run small/medium LLM models.,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.72,jiejenn
Why only small number of models fight on Chatbot Arena?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.8,jacek2023
Autogen + mistral small/moe/mixtral,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Unusual_Pride_6480
Small First Aid / survival LLM,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.78,Balance-
Is anyone exploring the idea of a proxy local small model?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.82,RandCoder2
A small test I did with falcon-180b-chat.Q2_K.gguf (at home on consumer grade hardware),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.99,frapastique
Looks like someone did the needful and it's a small download. 70b trained on proxy logs.,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.94,a_beautiful_rhind
Few questions on small language models,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.78,meet20hal
DiscoLM German 7B V1 - best small german model so far?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.94,Blizado
Finetune a small 7B model for my native language,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,Friendly-Gur-3289
Small Giants: 10 sub-13B “Open Source” LLMs,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.65,datascienceharp
"Moondream, a small vision language model based on Phi 1.5",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,radiiquark
Local model for code in small machine,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,MagoViejo
Small local model for git/shell,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,Tyson1405
Best small fine tuning dataset,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.86,Amgadoz
Small llm model within 100M to 1B parameter,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.97,AwayConsideration855
Any chance mistral open-sources their Mistral-small model?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,aue_sum
Small models for text classification/basic understanding?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.67,Tasty-Lobster-8915
Tired of small model? Did you miss : Tulu V2 DPO 70B,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.92,mantafloppy
Using small language models as data validator for topic modelling results,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.81,deaththekid00
Brainstorm. Small company use of AI for customer support.,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.75,slemklumpen
Training a small model from scratch,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.89,Tasty-Lobster-8915
Small explanation question! Types of LLAMA,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.81,Real_Experience_5676
How to train 7B models with small documents?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.96,oliveoilcheff
ive seen a remarkable increase in performance of small models like 13b or even 1b at what point can these learn to play games?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.87,Imaginary-Support332
CAPPr: guarantee small structured outputs given a list of choices,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.93,KD_A
Is there a way to fine-tune llama on extremely small dataset?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.96,nikitastaf1996
Running a small model on a phone?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.83,ScoobySnackzz12
SanjiWatsuki/Loyal-Macaroni-Maid-7B a strong new roleplay model in a small package!,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.89,Quiet_Joker
Whats the best way to run small llms locally on an ond machine?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,chooseanything1
Idea about restricting format of LLM output (with small POC),Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.94,AssistBorn4589
Small model fine tuning or QLora on Bigger One?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.81,_ragnet_7
A small llama.cpp server playground,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.92,hwpoison
Engaging topics for conversations in a small local workshop,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.75,besabestin
Any way to verify training method with very small dataset?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.75,BGFlyingToaster
Example of a small fine tuning,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.86,mehrdotcom
Small request - how would you recreate goblin.tools locally?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,ethereal_intellect
Are big models always better than small ones for Transfer Learning?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,hackeddevil
What are small models that work for you? What are the configs that you use for them?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.91,Powerful-Cupcake-407
Looking for a small model to host on my server,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.67,ccpsleepyjoe
"Unpopular Opinion: All these small open-source foundational models coming out are not moving us forward. To truly rival closed-source, we need models with 100+ billion parameters.",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.47,DangerousBenefit
Are there any good math Datasets for Training small models?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.75,vatsadev
"Noob: I try to fine-tune a LoRA with a very small dataset (10 samples) on Oobabooga, the model never learns.",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.89,tgredditfc
Idea: Alternating token generation from a big and small model?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.93,teachersecret
Can someone explain simply: How/why do small models (llama-2 7b) outperform larger ones (chinchilla 70b)?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.92,cold-depths
Plausible to Train Small Models on MacBook Pro M2 Max?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.75,Mbando
Question on setting up local inference server for small team,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,CableConfident9280
How to do correctly speculative decoding on the CPU using small models 1B and 7B?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.86,vasileer
Seeking Advice on Training a Custom Language Model (small language models for specific domain),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,PewDPAC
What model would you choose if you wanted to fine tune a customer service agent model for a small business?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.56,user_00000000000001
Small Uncensored LLM model to train cheaply for a specific task.,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.83,ImpressiveFault42069
Llm Super Coach - Small fun project to use a local LLM to analyze one's diaries in various ways.,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,curiousdude
small script that adds automatically V1/2/3 to your ggmls,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.93,Evening_Ad6637
How do you highlight a small list of key points from a document?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,nihnuhname
"Some small pieces of statistics. Mixtral-8x7B-Chat(Mixtral finetune by Fireworks.ai) on Poe.com gets the armageddon question right. Not even 70Bs can get this(Surprisingly, they can't even make a legal hallucination that makes sense.). I think everyone would find this interesting.",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.83,bot-333
Fine-tuning the 13B wizard model with a small amount of dataset and achieving GPT-4 level results?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.73,mzbacd
"humane ai pin dropping next week, how come I haven't heard more news abt this? seems like very capable tech packed into such a small device if it indeed works as they are marketing it--- i'm curious to see what type of latency it has for voice commands.",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.69,LyPreto
"MIT-IBM Watson AI Lab releases MoLM suite with three small sparse MoE models, the largest of which (8B params with 700M experts) performs on par with Pythia 2.8B while its throughput is comparable to Pythia 1.4B",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,ain92ru
Phi-2 becomes open source (MIT license 🎉),Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.99,steph_pop
How much more stupid is the 120B goliath Q3_K_M than the larger options?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.96,Secret_Joke_2262
"I recently tested the ""MPT 1b RedPajama + dolly"" model and was pleasantly surprised by its overall quality despite its small model size. Could someone help to convert it to llama.cpp CPU ggml.q4?",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Shir_man
"Zuckerberg says they are training LLaMa 3 on 600,000 H100s.. mind blown!",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.92,kocahmet1
What's the smallest but stil useful model you encountered,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.91,Fisent
What's your 2024 AI Predictions?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.96,dulldata
"Yes I am an expert at training, how could you tell?",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.98,xadiant
Best LLM under 3 billion parameters currently?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.89,Polstick1971
New Model: Nomic Embed - A Truly Open Embedding Model,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.98,shouryannikam
TinyLlama-1.1B: Compact Language Model Pretrained for Super Long,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,Either_Ad_1649
Any other smaller LLM (smaLLM? :D) users here?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.95,Solstice_Projekt
Orca (built on llama13b) looks like the new sheriff in town,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.98,ironborn123
2.7B model that performs like Mistral 7B!,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.92,koehr
"Train Smarter, Not Harder? - MiniSymposium 7b",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,kindacognizant
Beating Bigger Models is All You Need (Where is the LLM industry headed?),Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.91,nderstand2grow
Why is no-one fine-tuning something like t5?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,No_Baseball_7130
Why did gpu AIB partners stop making extra vram models ?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.88,Noxusequal
GPT-3.5-Turbo-0125 is so much worse I need a replacement,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.89,HeronAI_com
First OS embedding model with 8k context,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.99,Amgadoz
"Just got a 4090 24GB, what should I run?",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.84,tindalos
"We all hate LangChain, so what do we actually want?",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.97,AndrewVeee
Can someone explain what is mixtral 8x7B?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,No_Afternoon_4260
Models Megathread #3 - What models are you currently using? What are your thoughts on Mixtral?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.99,Technical_Leather949
What are the business use of LLM that will generate revenue?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.85,AMGraduate564
5 x A100 setup finally complete,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.98,BreakIt-Boris
Nucleus 1B - an SLM based on Mistral!,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.9,Haghiri75
How would a 4090 laptop fair in the current state of LocalLLaMA?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.89,rexyuan
Mistral has an even more powerfull model in the prototype-phase,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.96,QuieselWusul
What would be the smallest open source llm models that are still of reasonable function? ,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.67,Maelstrom100
Running full Falcon-180B under budget constraint,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.99,mrobo_5ht2a
Serving a large number of users with a custom 7b model,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.99,Scared-Tip7914
Fine tuning for coding,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.96,mudmin
Writing a novel generator. Looking for help/testers from authors/prompt masters.,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.87,Symphatisch8510
GPU Requirements for LLMs,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.97,pathfinder6709
Local LLM movement feels like early days of PCs vs Mainframes,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.97,docsoc1
What's the best 1-4B LLM for a phone?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.92,Whydoiexist2983
"Microsoft makes new 1.3B coding LLM that outperforms all models on MBPP except GPT-4, reaches third place on HumanEval above GPT-3.5, and shows emergent properties",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.98,llamaShill
Is it possible to run some kind of LLM on 1 GB of RAM?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.91,Creative_Grade_2146
OpenOrca-Preview1-13B released,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.95,Amgadoz
Deducing Mistral Medium size from pricing: Is it a 195b parameter - 8x30b MoE model?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.95,Time-Winter-4319
Mac vs Windows,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.5,yodnokzo_writer
Has anyone tried using a smaller model with an RAG to train a larger model?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.92,my_aggr
Looking for the best cost/price mini(?)-pc for a local llm,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.89,Mundane_Maximum5795
Optimal and Cost-Effective GPUs and Server Specs for Local AI Model Development,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,MuieewitdaMu
"MoE locally, is it possible?",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.96,JKaique2501
A Paradigm Shift in Machine Translation: how to outperform GPT-3.5 for translation using 7B and 13B models,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.98,llamaShill
How are people using open source LLMs in production apps?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.98,TheAnonymousTickler
Making LLAMA model return only what I ask (JSON).,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.95,br4infreze
Arthur Mensch Clarification about recent events,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.93,nanowell
RAG: Flexible Context Retrieval around a matching chunk,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.97,SatoshiNotMe
How do you determine which embedding models will fit into memory available?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,nuusain
"Is anyone inferencing on something like an Intel nuc, barebone or similar formfactor?",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.78,Frequent_Valuable_47
Best uncensored multimodal / vision model?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.97,yupignome
Microsoft's Phi 1.5 (1.7 B parameter) is now a multimodal model,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,Ok-Recognition-3177
New LLM from (almost?) scratch.,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.86,scapocchione
Is some tiny (1GB) model available through pip install?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.81,GermanK20
A Recipe for Textbooks Are All You Need,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.99,LyPreto
Mistral 7b v0.2 is nuts,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.96,ThinkExtension2328
New Paper: Proxy-Tuning: An Efficient Alternative to Finetuning Large Language Models,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Blade1413
"Google is training with 8 bit ints, next will move to 4 bit ints.",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.91,danielcar
Beginner friendly Guide to run local model AI on 4 Gb ram windows (GGML/GGUF Guide),Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.86,Merchant_Lawrence
JSON parsing as the benchmark for a LLM,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.63,Infinite100p
Single GPU (3080 10gb) or M1 Max Fine Tuning Help,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,ltcanuck
One-bit quantization is a thing now,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.98,BalorNG
Upgrading GTX 1060 6GB to RTX 3070 Ti 8GB is good enough for 13B models?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.67,user0user
Evaluating mistral-medium,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.95,Kinniken
Looking for a lightweight Model ideally fine-tuned for generating compliments or similar,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.86,PSRD
I still feel a large gap in math vs gpt4,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.83,davikrehalt
"As of 2024, what is the state of non-english models (embedding/LLM) for RAG?",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.67,graphitout
Is Gemma the old Bard?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.5,Iory1998
Trying LLM Locally with Tesla P40,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.88,Murky-Tumbleweed-486
Best Model to locally run in a low end GPU with 4 GB RAM right now,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,curious_cat_says_hi
How much does VRAM matter?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.7,NeaZerros
This is so Deep (Mistral),Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.95,Supersonic97
RTX 3090 vs RTX 3060: inference comparison,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.98,mrscript_lt
🚀We trained a new 1.6B parameters code model that reaches 32% HumanEval and is SOTA for the size,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.98,kateklink
What tasks are 7b and 13b models really good at?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.96,platistocrates
"Using a NUC, SBC, or SFF for LLMs?",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.67,Inous
Most straight-forward repo/library for full-fine-tuning,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,Koliham
Open-source LoRa training guide & Code?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.93,Danny_Davitoe
Quip# quantization of Tess-M,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.96,MLTyrunt
Mixtral-8X7B - Local vs API performance,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.92,RoseRedCinderella
Simple Questions Megathread,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.99,Technical_Leather949
Toxicity classification model,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.57,amang0112358
Can I train Llama on my own document st?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,BeYeCursed100Fold
Could you guys give me tips for my potential school project?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.75,HappyLove4691
"That ""No Moat"" Google memo from 7 months ago has aged very well",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.98,baldr83
"Excited to share my ambitious free and open-source library for connecting AI, human, and computing systems.",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.95,helloimop
Which model works best for role-playing?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.72,DoctorTriplex
Best developing country LLM?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.73,revolved
Mixtral can play Akinator. Kinda.,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.94,Cantflyneedhelp
Give me some advice about mixtral,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.88,Terrible-Mongoose-84
"Would it be good way to learn transformers and LLMs' basic structure by trying to build something by myself? Not any way coherent, just technically working and at least outputting some gibberish? Goal is understand the basics and try same things people do in llama.cpp and new approaches like Mamba",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.67,WorkingSuspect3007
Experiences with smaller models with RAG?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.82,brandonZappy
So is RAG no longer needed if you have a ton of RAM?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.74,unraveleverything
How bad is finetuning on p40s vs p100s ?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.9,Noxusequal
How to add new languages to existing models?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.8,CodeAnguish
QuIP: 2-Bit Quantization of Large Language Models With Guarantees,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,georgejrjrjr
TIL Some tokens can be multiple words,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.97,psi-love
Most capable function calling open source models?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.95,waywardspooky
Dealing with big datasets,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.75,ExaminationNo8522
I am just learning about LLMs and for educational purposes made a simple language model,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.96,Various-Operation550
"LLM Chat/RP Comparison/Test (Euryale, FashionGPT, MXLewd, Synthia, Xwin)",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.98,WolframRavenwolf
Looking for a Handy LLM for Survival & DIY – Any Suggestions?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,cajun_spice
any ideas on how sites like Grok and Perplexity return the most recent relevant result?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,shafinlearns2jam
Can we participate in the Subredit Blackout?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.67,jl303
What is the smallest ggml model available?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,Shir_man
Anyone is using llamacpp for real?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.27,russellsparadox101
Datasets for local model grammar fixing and writing improvement,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.93,dandanda99
Can you pair RTX 4090 with RTX 6000 Ada?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,bigdude404
What's the latest on 1B models and Mac Mini M2 efficiency?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.75,IEatGnomes
Google Dialogue Flow replacement,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,AutomaticDriver5882
This is the new king of LLM hardware with 576 GB of RAM.,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.95,fallingdowndizzyvr
What is the status of autonomous agents manipulating our browser?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.75,kecepa5669
What can I run on this Dell PowerEdge R710 Server 32G Ram/12 Core?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.88,Overall-Importance54
"In my opinion open-source projects should focus an a very narrow thing, instead of focusing on being a ""GPT"", that focuses on being able to do everything.",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.75,GodEmperor23
Tiny models for contextually coherent conversations?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.8,Amazing_Sentence5393
Petals vs vLLM vs ? for serving LLMs to many users,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.92,humanoid64
tlm - using Ollama to create a GitHub Copilot CLI alternative for command line interface intelligence.,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.93,yusufcanbayrak
Relationship between GPU memory and context size,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.83,plsendfast
Would a merge between Neural Chat 7B v3.1 and OpenHermes-2.5 work?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.9,StrangeImagination5
Use llm as a development team,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.52,jacek2023
GGUF vs AWQ vs GGML,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,infiniteContrast
Proof of Concept: Local LLM to execute terminal comands (Here GPT-2),Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.93,Evening_Ad6637
How do you discover tools/ideas that might help improve your LLM-based apps which are not RAG?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.91,Mean-Night6324
DPO training strategy,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.96,SirStagMcprotein
What are the experts in Mixtral?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.83,Andrey_best_2
"Quantizing 70b models to 4-bit, how much does performance degrade?",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.96,ae_dataviz
Apple's tiny 34M paramters transformer,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.98,Evening_Ad6637
Mistral API: Initial test results,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.94,Conutu
Current best options for local LLM hosting?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,PataFunction
New merges: Aurora-Nights-70B and Aurora-Nights-103B,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.91,sophosympatheia
Mistral 7B + MiniGPT-v2 + Go1 robot-dog + custom gripper = CognitiveDog,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.94,WaterdanceAC
OrangePi 32GB worth it?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.89,PSRD
NeuralHermes-2.5: Boosting SFT models' performance with DPO,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,mlabonne
Introducing LIVA: Your Local Intelligent Voice Assistant,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,Automatic-Net-757
Don't underestimate the importance of the system prompt,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.97,Combinatorilliance
Cheaper cloud alternatives to train LLM for educational purpose,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.89,besabestin
"MiniCPM: An end-side LLM achieves equivalent performance to Mistral-7B, and outperforms Llama2-13B",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.76,x_swordfaith_l
New Model RP Comparison/Test (7 models tested),Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.96,WolframRavenwolf
Budget machine for tinkering with LLMs,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,the-uncle
"Why is CodeLlama on huggingface so opinionated, biased and arrogant ?",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.8,mindful999
Best tools for automated generation of questions and answers from unstructured text,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.75,SirStagMcprotein
New quantization method AWQ outperforms GPTQ in 4-bit and 3-bit with 1.45x speedup and works with multimodal LLMs,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,Spiritual-Roll3062
Budget rig for LLM,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,AerotyneInternationa
Meta Releases Llama Guard - the Hugging Edition,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.94,hackerllama
How is Solar so good for it's size,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.91,openLLM4All
Deepseeker Code language/IDE knowledge Questiom,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.7,TigerNinjaChan
Deploying LM/LMM to edge device,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,rower22
What are the problems faced when an AI customer support chatbot is used by a SaaS business.,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,arxavsx
Any help on using knowledge distillation on LLMs like Llama2 or Qwen?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.67,s1lv3rj1nx
Local LLM projects,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.88,wedcw
"I only said ""Hello..."" :( (Finetune going off the rails)",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.94,FPham
Single board computers with 64 GB RAM,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.9,jsalsman
Anyone tested speculative sampling in llama.cpp?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.91,Aaaaaaaaaeeeee
New dataset for fine-tuning: spicyfiction,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.98,threevox
"Refresh of ""GPUs4AI"" (database of AI-capable GPUs)",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,digital_m0nk
What is the smallest possible model (file size)? Or how to create a basic chat model?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,herozorro
Can i fine tune any model with m1 16gb ram,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,itshardtopicka_name_
Llama 2 Scaling Laws,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,georgejrjrjr
"Just getting started, low cost machine",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,xerfd
Train LLM From Scratch,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.86,idesireawill
Inference Speed Benchmark,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.87,AdventurousSwim1312
Good local models for data cleaning and extraction?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.75,ExaminationNo8522
"EXLlama test on 2x4090, Windows 11 and Ryzen 7 7800X3D",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.96,panchovix
Google just shipped libggml from llama-cpp into its Android AICore,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,perone
Running llama.c on budget android ,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.99,esharp007
Running Google's Gemma 2b on Android,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.96,Electrical-Hat-6302
Local LLaMa on Android phone,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.99,AstrionX
Best backends for running models on Android?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.88,GamerWael
Running LLMs locally on Android,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.9,atezan
Best model to run locally on an Android phone?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,Ok-Recognition-3177
I built my own android chatting frontend for LLMs.,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.96,----Val----
[Project] MLC LLM for Android,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,crowwork
Ai on a android phone?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.88,Gaming-invisibleman
Tiny LLM for android ereader?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.72,kwerky
Keyboard with word prediction on android,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,drdada
Running Llama2 on Android,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.88,Deep-View-2411
Why isn't anyone building an Oogabooga-like app for Android and iPhone?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.54,Winter_Tension5432
Running Phi-2 locally in Android Chrome browser with WebGPU,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.97,SnooMachines3070
How to run LLMs on Android/IOS?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,LiquidGunay
How to run mistral-7b-openorca on Android phone?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.81,ZotD0t
Run models on a real Android device with Qualcomm AI Hub,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.91,ephemeralshot
Running Wizard 7b (Q2) on an 8gb Android Phone,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.9,Due-Ad-7308
Most User-friendly Method of Running Local Models on Android Phones?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,OldAd9530
Anyway to use Runpod on android?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.5,theshoelesschap
"Gemma locally on iOS, Android, web browsers, and GPUs with a single framework",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.77,SnooMachines3070
Android app for LLama inference on GPU using JNI + llama.cpp,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.75,IonizedRay
What is the best config to run Mistral 7b on an Android phone?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.81,gianpaj
Has Anyone Successfully Utilized the Neural Networks API on Android for LLMS with EdgeTPU?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.84,dewijones92
Android app to interact with text-generation-webui and stable-diffusion-webui [github],Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.96,frapastique
Sherpa(Llama.cpp for Android) New Pull request add latest pulls from llama.cpp and it's faster now with no more crash. (apk link in description),Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.93,FHSenpai
Yup. Works great!,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,FPham
"I'm about to open source my Flutter / Dart plugin to run local inference on all major platforms. See how it runs on my personal Apple devices: macOS (Intel & M1), iOS, iPadOS. Next up: Android, Linux & Windows. AMA.",Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.91,BrutalCoding
Gemini Nano is a 4bit 3.25B LLM,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.98,Amgadoz
Absolute cheapest local LLM,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.93,SporksInjected
MLC-LLM Chat vicuna-Wizard-7B-Uncensored-q3f16_0,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.93,jetro30087
The S24 will have gemini nano onboard!,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.9,Amgadoz
What mobile apps are folks using for ChatGPT like experiences for self-hosted LLM backends?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.91,janniks
How has this not been mentioned here before? Layla,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.8,Derpy_Ponie
"Google doesn't have a moat, openai does",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.91,Amgadoz
LLM to query a product list,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.84,davide445
Deploy LLMs at the edge,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.86,YYY_333
Running a small model on a phone?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.88,ScoobySnackzz12
Llama-2 via MLC LLM,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.93,yzgysjr
How do I run Stable Diffusion and LLMs from my PC on my mobile device? Offline and private ways?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.56,ThrowawayProgress99
TinyLlama-1.1B: Compact Language Model Pretrained for Super Long,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,Either_Ad_1649
Local Autonomous Coding?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.8,Sl33py_4est
Cannot load GGUF model - invalid magic number ?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.86,Fit_Check_919
Help installing vicuna 7b on Google pixel 6a,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.8,-2b2t-
Is local LLM cheaper than ChatGPT API?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.88,Financial_Stranger52
Replacing LLM of Suno Ai's Bark TTS model with Mistral or TinyLlama?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.9,Independent_Key1940
Any successful guides on scanning internal pages and build a virtual assistant using LLAMA?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.96,vlodia
[Project] MLC LLM: Universal LLM Deployment with GPU Acceleration,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.96,crowwork
"Orange Pi 5 Plus Koboldcpp Demo (MPT, Falcon, Mini-Orca, Openllama)",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,CheshireAI
Describe your local LLM Setup,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.96,im_datta0
"Opentensor and Cerebras announce BTLM-3B-8K, a 3 billion parameter state-of-the-art open-source language model that can fit on mobile devices",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,CS-fan-101
CogAgent: A Visual Language Model for GUI Agents,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.92,llamaShill
some tests on rock 5b ARM SBC(RK3588),Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.67,Dyonizius
Should A.I. dream?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.53,freedom2adventure
Most performant option to run 13b LLM locally as a personal assistant for under $300 USD,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.6,Envoy0675
Current state of the iOS App I’m working on. Interact with text-generation-webui and your your local llms from everywhere. What features would you like to see implemented?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.96,frapastique
Why not standardize 3bit & 2bit GPTQ?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.6,onil_gova
Justice LLM?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.54,Tridente
How to compile models for MlC-LLM,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,jetro30087
"What I've learned from orca-mini-3b.ggmlv3.q4_1 using LLamaCPP (_python), so far.",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,Solstice_Projekt
Ollama iOS mobile app (open source),Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.97,1amrocket
Internlm2 20B 3.04bpw at 1 t/s on Pixel 6 Pro!,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.86,Aaaaaaaaaeeeee
"What do these words mean? Hermes, OpenHermes, OpenChat, Vicuna, Alpaca, Orca, OpenOrca, Airoboros, Synthia, Guanaco, Dolphin, Samantha, Synthia, ...",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.78,nderstand2grow
CodeLlama 70B Local Deployment with JIT Compilation,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.93,SnooMachines3070
"Announcing The best 7b model out there ""orca-mini-v3-7b""",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.93,Remarkable-Spite-107
What’s the best local LLM for low to medium end machines?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.94,SimRacer101
oss tts engine?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,LyPreto
Local LLM with mobile app?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,Data_Driven_Guy
Llama2 on Replicate faster than ChatGPT?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.71,VideoTo
Llama2 Qualcom partnership,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.86,AstrionX
Help me discover new LLMs for school project,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.71,shadowofdeath06r
Hacked away an abysmally simple Code Interpreter over the weekend using locally hosted Llama-2 based models.,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.96,LyPreto
"I have many questions and I seek answers, please help me.",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.67,CrimsonRedstone
Evaluating the effectiveness of finetuning on the Berkeley open_llama preview,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,Zovsky_
LLM As A Brain: A collection of hypotheses,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.92,mark-lord
"I'm trying to get TheBloke_airoboros-33B-GPT4-2.0-GPTQ to create an accurate list of modern science fiction books, and just... I just...",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.89,CatastrophicallyEmma
Optimum Intel OpenVino Performance,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.91,fakezeta
Help with using a local model to edit 1000s of novel chapters,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.86,Benista
PrivateGPT - Asking itself questions and answering?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.75,masterblaster269
Oobabooga not recognizing GPU for adding layers,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,yungfishstick
Llama1-based model says it's best model out there,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.47,Amgadoz
"Seeking clarification about LLM's, Tools, etc.. for developers.",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.9,Clicker7
airoboros-65B-gpt4-1.2.ggmlv3.q8_0.bin - harry potter erotica,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.38,dewijones92
Ollama iOS mobile app (open source),Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.94,1amrocket
Enchanted - Ollama iOS app for self hosted models,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.82,1amrocket
Clinical medicine LLM for iOS app,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.6,No_Slip497
How to run LLMs on Android/IOS?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.84,LiquidGunay
LLM.swift library lets you interact LLMs on iOS easily,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.95,eastriver0720
Found an iOS app for TestFlight that allows you to run LLMs locally,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.94,HemisphereGuide
Has someone tried LLMFarm for native inference on iOS devices?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.9,frapastique
Simple demo app of TinyStories-1m that runs locally on iOS,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.92,seattleeng
I wrote a server for proxying requests to serverless LLMs on Runpod.io using an OpenAI-compatible api,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.92,dannysemi
Current state of the iOS App I’m working on. Interact with text-generation-webui and your your local llms from everywhere. What features would you like to see implemented?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.97,frapastique
"LLaVA 1.6 released, 34B model beating Gemini Pro",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.99,rerri
review of 10 ways to run LLMs locally,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.9,md1630
"TheBloke has released ""SuperHot"" versions of various models, meaning 8K context!",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.99,CasimirsBlake
[tutorial] Easiest way to get started locally,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.9,Amgadoz
"Recent updates on the LLM Explorer (15,000+ LLMs listed)",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.98,Greg_Z_
Upcoming Qwen2 70B beating Mistral Medium (Miqu) at MT-Bench,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.9,DreamGenAI
What are the benefits of using open source embeddings model?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.95,99OG121314
Argilla release a new OpenHermes model outperforming baselines with 54% fewer DPO pairs,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.97,dvanstrien
OpenCodeInterpreter: Integrating Code Generation with Execution and Refinement - 2024 - HumanEval of 92.7! GPT-4 CodeInterpreter has only 88.0!,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.98,Singularian2501
Preset Arena: final results,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.98,oobabooga4
How can time-to-first-token be so fast on SaaS APIs (eg. Mixtral 8x7b on huggingface arena),Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.83,chregu
Introducing Qwen,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.89,DreamGenAI
First OS embedding model with 8k context,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.99,Amgadoz
Where and how to run Goliath 120b GGUF with good performance?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.95,abandonedexplorer
Offline llama,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.99,Jl_btdipsbro
Got my hand on 3x3090,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.94,No_Cryptographer9806
Horde-Client v1.0.2 is out today!,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.92,AnonymousD3vil
Adding LLaMa2.c support for Web with GGML.JS,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,AnonymousD3vil
Pygmalion 2 (7B & 13B) and Mythalion 13B released!,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.99,whtne047htnb
Open LLaMA 7B uncensored + HuggingFace QLoRA fine-tuning guide,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,georgesung
Apple's tiny 34M paramters transformer,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.98,Evening_Ad6637
"Microsoft researchers present LLaVA-Interactive: a demo for image chat, segmentation, generation, and editing",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,llamaShill
Best open source coding models today (EvalPlus leaderboard),Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.75,TechnoTherapist
What is stopping us from creating an open source GPT-4 & Gemini Ultra? (Or better),Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.89,askchris
Deploy LLMs at the edge,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.86,YYY_333
Reliable docker image to run python / local LLM code with Jupyiter that's easily available on most cloud platforms?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.86,gofiend
"Language Agent Tree Search method bumped GPT-4's HumanEval Pass@1 score from 80.1 to 94.4, GPT-3.5's score from 72.5 to 86.9, can we use this approach with local coding models?",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,OnurCetinkaya
Why not test all models for training on the test data with Min-K% Prob?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.9,vatsadev
"I just got 10k AWS credit, and I'll use it to host LLM for your app.",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.9,m0dE
any llm model practically work on an iphone,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.88,RepulsiveDepartment8
New Model: NVIDIA's Parakeet STT models beat whisper-large-v3,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,shouryannikam
CAPPr: guarantee small structured outputs given a list of choices,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.87,KD_A
How has this not been mentioned here before? Layla,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.81,Derpy_Ponie
How do websites retrieve all LLM VRAM requirements?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,Kaolin2
32GB vs 64GB vs 96GB M2Max?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.74,Infinite100p
"LocalAI v2.9.0 is out: parallel function calls, and much more!",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.92,mudler_it
"Chess-GPT, a 50M parameter LLM, plays 1500 ELO chess. We can visualize its internal board state, and it accurately estimates the ELO rating of the players in a game.",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,seraine
I built my own android chatting frontend for LLMs.,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,----Val----
Persistent cloud computing setup to run different LLM models.,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.86,Xardas1987
A local LLM is the ultimate doomsday device,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.9,Ninjinka
Suggestions on Running Airoboros 70B on Cloud GPU like runpod?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,broodysupertramp
LLM.js v1.0.2 is out with support for GGUF/GGML format and Model Playground,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.94,AnonymousD3vil
"New benchmark by Stanford: HELM lite v1.0.0 including Narrative, Math, Legal, Medicine, Translation tasks",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.99,galambalazs
StyleTTS 2 - Closes gap further on TTS quality + Voice generation from samples,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,super-helper
Declarai - a game-changer for Python-based language model interactions!,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.85,matkley12
LangCheck v0.3.0: Factual consistency improvements and text augmentations,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,kennysong
Expand the Context Length with RoPE from a β-based Encoding perspective,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,Alternative_World936
Gaining Insights from My Fine-Tuned Llama2 model,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.57,Dazzling-Candle5118
What will decide the loading speed of a model?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,JohnSmith004
UltraLM-13B reaches top of AlpacaEval leaderboard,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.94,TNTOutburst
Explore and compare the parameters of top-performing LLMs,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.96,Greg_Z_
Guidance for selecting a function-calling library?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.86,handyman5
[R] Half-Quadratic Quantization of Large Machine Learning Models,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,sightio
Are we wasting our time with these LLMs?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.81,Warm_Shelter1866
Dynalang code released,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.95,ninjasaid13
Reliable ways to get structured output from llms,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.98,amit13k
Bionic GPT - A front end for Local LLama that supports RAG and Teams.,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.86,purton_i
"Creating a really strong screenplay model, by finetuning it „challenge / solution“ style, based on a selection of the best scenes ever written?",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,hugo-the-second
"LLM Explorer has got 17,000+ models in db",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.89,Greg_Z_
Fine-tuned llama2-7b-lora vs chatGPT in a noble game of chess?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.9,Acceptable_Bed7015
6000+ tokens context with ExLlama,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.99,oobabooga4
LLaVA-1.5 7B and 13B released: Improved Baselines with Visual Instruction Tuning,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.98,llamaShill
"Preset Arena: 17,205 comparisons between 241 different presets. Vote on the best ones!",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.99,oobabooga4
Gorilla 7B: Large Language Model Connected with Massive APIs,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,The-Bloke
table extraction from pdf,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.91,happy_dreamer10
Best prompt and model for fact-checking a text (disinformation/fake-news detection),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.6,Fit_Check_919
LLM for Coding – Minimum Viable Speed/Quality?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.96,ihavehermes
Which Macbook Pro to buy for running an LLM locally? I created a buyer's guide to help you decide.,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.7,fabkosta
Train LLaMA with custom loss function,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,matkley12
Launching AgentSearch - A local search engine for your LLM agent,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,docsoc1
Looks like Mistral-7B-Instruct-v0.2 is coming soon...,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,distant_gradient
Here's My totally accurate flowchart on why we need new Pretrained model,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.75,vatsadev
dolphin-llama2-7b,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.99,faldore
Hermes LLongMA-2 8k,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.96,EnricoShippole
How to make your own local version of chatbase?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,EnPaceRequiescat
"Talk me off the ledge, I want to buy more hardware for local LLMs...",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.86,silenceimpaired
"Currently working on a ChatbotAPI for building custom chatbots running on local LLM models. When released it will ship with a frontend chat interface meant be a tool anyone can use when programming, working or just chatting with local models. Design is inspired by the terminal look / mIRC chat days",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.98,Severin_Suveren
Glossary for machine learning and looking for something LLM specific.,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,wreckingangel
Local self ask,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,KeksMember
Looking for explanation: How to improve latency/quality with code completion llama models (Copilot Quality reachable?),Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.83,Vegetable-Speech417
Local LLM chat agent with advanced RAG and memory,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,Bosezz
Samsung revealed special DRAM for AI,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.61,wjohhan
License status of semantic search models with respect to licenses for training datasets.,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.66,FormerIYI
Jailbreaking Vicuna,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.98,cobbertine
VectorDB with Llama Embeddings - Few Questions and Doubts,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.79,--lael--
How do i get a UI for LLAVA as they have done in their showcase?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.83,card_chase
Llama 8k context length on V100,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.81,HopeElephant
Exploring the current status quo of LLM UI,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.9,URLSweatshirt
What is the best way to create a knowledge-base specific LLM chatbot ?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.95,AImSamy
llama.cpp CPU optimization,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.82,noiserr
Introducing starcoder.js: Web Browser port of starcoder.cpp,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.98,AnonymousD3vil
Is there a way to make Local GPT say I don't know if the information is not in the index?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.84,Tricky_Witness_1717
Continue with LocalAI: An alternative to GitHub's Copilot that runs everything locally,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.98,tsyklon_
Any Good Speech-to-Speech Voice Changer Models?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.97,HumanityFirstTheory
Possible game changing algorithms for LLMs such as forward-forward?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,vlodia
Why do most tutorials do instruction tuning of base model instead of instruction-tuned models?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.97,Separate-Still3770
saily_100B gguf - text results,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,Aaaaaaaaaeeeee
fine-tuning LLaMa variants for text generation task,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,1azytux
Thunderbolt and multiple eGPUs,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,throwaway075489
Whats the best preforming local model to run on a NVIDIA 4070?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.6,CrankyHarbor256
Preset Arena experiment: last 48 hours. Please vote!,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.98,oobabooga4
I made an inference sever that supports repeating LLM layers,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.97,Silphendio
An Alternative Approach to Building Generative AI Models,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.85,buildinstuff5432
Recent DPO datasets overview,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.99,dvanstrien
"People of LocalLlama, I need your roleplay and worldbuilding data",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.74,vatsadev
"New Llama 13B model from Nomic.AI : GPT4All-13B-Snoozy. Available on HF in HF, GPTQ and GGML",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,The-Bloke
Is there a timeline of benchmarks?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,Choice-Sea1917
Looking for resources for creating custom RAG implementations,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,Jvaughan22
5x 3090 LLM rig opportunity – stupid decision?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.89,inquisitive-spaniard
Serverless Awesome Prompt Experimental Tool,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.95,dreamai87
A (possibly stupid) thought about context extension.,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.83,Captainbetty
Anyone got some good resources for working with LLama CPP on python?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.72,Nano_9a9o
Kani: A Lightweight Highly Hackable Open-Source Framework for Building Chat Applications with Tool Usage (e.g. Plugins),Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.88,zhuexe
A small test I did with falcon-180b-chat.Q2_K.gguf (at home on consumer grade hardware),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.99,frapastique
Xwin-LM - New Alpaca Eval Leader - ExLlamaV2 quants,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.86,Unstable_Llama
Local LLM & STT UE Virtual MetaHuman,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.92,BoredHobbes
Pre training using textbooks ,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.8,keeplearning24
API Server,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.67,tomd_96
Has training an LLM with incrementally more difficult text been attempted?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.95,Responsible-Dig7538
PDF annotation tool,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.88,AceFromSpaceee
Beginner's guide to finetuning Llama 2 and Mistral using QLoRA,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,HatEducational9965
Best datasets for local training?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.84,xRolocker
Cheapest GPU VPS for 24/7 hosting,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.91,Koliham
Streamlit launches LLM Hackathon 🧠,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.89,carolinedfrasca
"Gemma locally on iOS, Android, web browsers, and GPUs with a single framework",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.77,SnooMachines3070
LangCheck: a multi-lingual toolkit to evaluate LLM applications,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.99,kennysong
SkyServe: Self-Hosted AI Serving on Any Cloud,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.88,skypilotucb
This research may explain why fine-tuning doesn't incorporate new factual knowledge,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.89,phree_radical
Testing Mixtral 8x7b vs. GPT-4 for boolean classification,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.87,JShelbyJ
LLongMA 2: A Llama-2 8k model,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,EnricoShippole
Update from SciPhi - Introducing SciPhi-Self-RAG-Mistral-7B-32k,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.99,docsoc1
"Opentensor and Cerebras announce BTLM-3B-8K, a 3 billion parameter state-of-the-art open-source language model that can fit on mobile devices",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,CS-fan-101
Prompting Mixtral 8x7b Instruct for RAG?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.86,glassroofdevil
Is there any way to make use of multiple CPU threads for model running in the browser?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,xuisel
How to recover document structure and plain text from PDF?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.87,shibe5
how much does it cost for you to use mistral at production level??,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.65,GlitteringAdvisor530
Unexpected nice side effect of working with open source,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.76,phoneixAdi
How to host your LLMs for free,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.55,Routine_Attitude9717
"If I can't afford to buy the necessary hardware to run a high performance model, is there a service that I can use on a monthly basis to host it for me?",Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.94,OKArchon
What's the best 1-4B LLM for a phone?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.92,Whydoiexist2983
Does anyone have experience running LLMs on a Mac Mini M2 Pro?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,jungle
Where do you fine-tune your LLMs?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.96,Acceptable_Bed7015
What's the best UI for completion (base model),Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.75,shadows_lord
Yet another RAG system - implementation details and lessons learned,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,snexus_d
Using Llama2 in a SaaS application,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,abandonedexplorer
Simple Utility in Go to download HuggingFace Models,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.95,bodaaay
Processing sensitive info with Mistral for cheap,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.73,Critical_Pop_2216
"Simple Local Models Answers Evaluation Colab example (with GPT4, no langchain etc.)",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.94,Shir_man
Can LLMs do static code analysis?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.98,catid
Deducing Mistral Medium size from pricing: Is it a 195b parameter - 8x30b MoE model?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.93,Time-Winter-4319
Validity of metrics,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,randrayner
"Unbiased, up-to-date Code LLM Leaderboard?",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.8,Salt-Operation4547
Has anyone for experience getting a tesla m40 24gb working with pci pass-through in VMware in latest Ubuntu or hell even windows?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,TeknikL
ChatMusician: an LLM that can generate and understand music intrinsically,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.92,llamaShill
Optimizing Response Speed from LLaMa and Hosting Strategy for a Multi-layered App Architecture on AWS EC2,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,JapaniRobot
Current best options for local LLM hosting?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.99,PataFunction
Official WizardCoder-15B-V1.0 Released! Can Achieve 59.8% Pass@1 on HumanEval!,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.98,cylaw01
OCR techniques for RAG PDF extraction,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,deeepak143
Best model for writing that will run on 24GB vram?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.87,yupignome
Testing out 13B models,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.97,Hopeful_Donut4790
"Anybody tried Codellama-70B, SQLCoder-70B; SQLCoder-7B? And Alphacodium framework? And perplexing HumanEval scores",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,Consistent_Bit_3295
QLoRa fine-tuning Mistral-7B-Instruct-v0.2 with 8GB RAM MacBook using MLX and MLXT,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,chenhunghan
Give some love to multi modal models trained on censored llama based models,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,saintshing
Dolphin or Mistral function calling,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,1EvilSexyGenius
How to get started with LLMs (Undergrad Student),Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.89,Appropriate_Fix_8347
Best models out there for improving article paraphrasing?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.88,07_Neo
I was able to load WizardLM-30b onto my RTX 3090,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.87,klop2031
Most straight-forward repo/library for full-fine-tuning,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.84,Koliham
Chat UI for both LLMs and Diffusion Models?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.92,Automatic-Net-757
understanding the llm eco-system,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,olaconquistador
Has anyone got this running in Godot?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,Official_CDcruz
"""GGUFv1 is no longer supported"" Error: Unable to load model",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,Efficient_Eye_9061
Pay as you go for opensouce models,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.88,OpeningRecognition69
Alternatives to LLamaSharp?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.85,kimberly1818
Best Model,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.69,Figai
How do you go around managing your datasets?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.76,Test-Elegant
Tool for deploying open source LLMs on your own cloud,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.96,torque-mcclyde
"Apache Tika: An underrated alternative to Unstructured/Nougat for text extraction (for RAG, LLM fine-tuning, etc.)",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.91,replicantrose
Would you use a free and anonymous (no login) website for embeddings?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.89,Simusid
70b q3_K_S at 1.4 t/s 32gb cpu ram showcase,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.94,Aaaaaaaaaeeeee
OpenAssistant RELEASED! The world's best open-source Chat AI!,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.93,redboundary
Is there any LLM that can be installed with out python,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.5,Ok_Calligrapher_9676
Possibility of deploying local LLM on 7 yr old laptop,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.86,mindseye73
Github Copilot alternative with OpenAI-compatible API?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.94,Alternative_Card_989
llama-gaan-2-7b-chat-hf-dutch 🇳🇱,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.97,gijs4g
"What are the recommended LLM ""backend"" for RAG",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.94,wandering-ai
LLongMA-2 16k: A Llama 2 16k model,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,EnricoShippole
PDF to text for llama in good quality?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.67,VentrueLibrary
Good settings to use for airoboros l2 70B?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,nsfw_throwitaway69
Embed Llama into a Mac Email Client. Best ways?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.86,anrgy971
"Papers to read, any suggestions?",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.91,Sebba8
Host the model on multiple GPUs via TGI,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.94,mzbacd
CodeLlama 70B Local Deployment with JIT Compilation,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.93,SnooMachines3070
Regarding long context and quadratic attention,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,tt19234
"Need some pointers for RAG, chunks retrieved don't seem very relevant.",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,subdivisionbyzero
What's your environment setup for running LLMs?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.87,RedditPolluter
"Finetune, RAG or live search",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,e-nigmaNL
Run Llama to ask questions about your files,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.97,adriacabeza
Long context (16K+) 7B training on a single GPU?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,mcmoose1900
OpenHermesPreferences - a dataset of 1M AI preferences for RLAIF and DPO,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.89,lewtun
Falcon 180B GPTQ Model on Multi-GPU Setup with RunPod,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,Wrong_User_Logged
Best guide for text finetuning?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.93,airhorny
Looking to recreate a cool AI assistant project with free tools,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.88,Fantastic-Air8513
Automatic hallucination detection using inconsistency scoring,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,Separate-Still3770
Creating personalized dataset is way too hard to do alone (in order to finetune some model in future).,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.88,szopen76
Llama 2 as a local copilot!!,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.94,tarun-at-pieces
LLMs can't self-critique,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.81,ninjasaid13
Deploying LLMS on CPU for production?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.84,Neptun0
Best way to upgrade my pc to improve t/s,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,DarthByron
"Lemur-70B+Chat (LLaMA2-70b from Salesforce& HKU, trained on 100B code and 300K instructions; 61,5 HumanEval and promises the best code/natural language balance to date)",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,Ilforte
Is there an API for Dolphon mixtral 8x7b or a way to run it cheaply?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.59,Analysis_I
"🚀 Completely Local RAG with Ollama Web UI, in Two Docker Commands!",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.95,tjrbk
How to Create Your Own Free Text Generation Endpoints,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.8,vm123313223
Which model will gives the fastest results (like turbo) ?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.78,rmsisme
Any services that can help host LoRA (QLoRA) of Mistral 7B?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.88,distant_gradient
Converting text to JSON,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,LancelotGFX
🚀 Run Local LLMs with a User-Friendly Web UI in Two Docker Commands!,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.96,tjrbk
Testing LLaMA 13B with a few challenge questions,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,Technical_Leather949
"Summary post for higher context sizes for this week. For context up to 4096, NTK RoPE scaling is pretty viable. For context higher than that, keep using SuperHOT LoRA/Merges.",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.98,panchovix
What are the best educational resources?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.95,tief1ghter
How to run mistral-7b-openorca on Android phone?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.79,ZotD0t
Are there any existing guides on how to deploy vLLM on a GPU cluster?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,MonkeyMaster64
Someone know any projects about an alexa like device build with llama,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.83,kroryan
How Can I Calculate the Token Generation Rate of an LLM Based on Given GPU Specifications?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.83,PickkNickk
What is the best API right now for self-hosted LLM usage?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.55,InkognetoInkogneto
"Newbie, searching among text intelligently",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,soumdeal
Compare TheBloke_WizardLM-13B-V1.0-Uncensored-GPTQ with TheBloke_WizardLM-13B-V1-0-Uncensored-SuperHOT-8K-GPTQ,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.9,Spare_Side_5907
Tensorrt-llm publicly available,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,whata_wonderful_day
The LLM GPU Buying Guide - August 2023,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.94,Dependent-Pomelo-853
Qwen-1.5-8x7B,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.79,Amgadoz
Is it better to have one large parquet file or lots of smaller parquet files?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,fendiwap1234
Open Source LLMs and Langchain tools,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.92,tail-recursion
All Model Leaderboards (that I know),Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.99,YearZero
Alternative to LangChain for open LLMs?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.96,NodeTraverser
The Choice of a Particular Generation Parameters Preset Can Make or Break a Model!,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.93,Iory1998
llama.cpp on raspberry pi keyboard,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.97,Sl33py_4est
testing llama on raspberry pi for various zombie apocalypse style situations.,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.98,Purple_Session_6230
Phi-2 & Tiny LLama on Raspberry Pi 5,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.93,flopik
I've created Distributed Llama project. Increase the inference speed of LLM by using multiple devices. It allows to run Llama 2 70B on 8 x Raspberry Pi 4B 4.8sec/token,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.98,b4rtaz
Raspberry Pi 5 8GB + NVIDIA 660ti,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.88,PSRD
Cluster raspberry pi,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.81,Terrible_Vegetable4
Mistral 7B on the new Raspberry Pi 5 8GB model?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.9,DiverDigital
Github link: Local Voice Assistant running on Raspberry Pi 4,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.93,Ok-Recognition-3177
"Raspberry Pi 8GB running TinyLlama, can anyone report the user experience?",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.92,Actual-Bad5029
"Oobabooga webui, Phi-2, and Mistral on Raspberry Pi 5, Orange Pi 5 Plus, and Jetson Orin Nano",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.9,SiON42X
codellama on #raspberry pi spitting out #metasploit commands. The aim is to spit out oneliners and outputting into bash.,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.92,Purple_Session_6230
Any coding LLMs that can run on Raspberry Pi 400? Or AM 7600 XT (intel i7 4th gen + 16 GB RAM),Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.5,shakespear94
llama 13b on raspberry pi - slow but still works. This has just opened up other modals for use on rasperry pi 4 x 8gb. Next experiment is to try getting auto-llama-cpp to run using this modal. Even if its slow it would be a great addition to my toolkit.,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.94,Purple_Session_6230
A local LLM is the ultimate doomsday device,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.9,Ninjinka
Mistral - disappointing CPU-only performance on AMD and Windows,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.57,lakySK
Looking for a lightweight Model ideally fine-tuned for generating compliments or similar,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.9,PSRD
Anyone is using llamacpp for real?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.27,russellsparadox101
Ai accelerator hardware is slowly becoming available,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,Combinatorilliance
"OnnxStream running TinyLlama and Mistral 7B, with CUDA support",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.9,Pristine198
OrangePi 32GB worth it?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.81,PSRD
PrivateGPT on RPi?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.71,jameshayek
The Desirability of Pruning for a Children's AI toy?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,danl999
Learning. Slowly.,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.82,FatGuyQ
Is Llama 2 7B or 7B Chat better for a talking Teddy Bear?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.93,danl999
Tiny LLM for android ereader?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.72,kwerky
Running LLM on CPU ?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.9,WERE_CAT
3B models on a Pi 4 8GB?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.97,MoffKalast
What would be the smallest open source llm models that are still of reasonable function? ,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.67,Maelstrom100
Phi-2 on Pixel 3! (llama.cpp PR),Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,Aaaaaaaaaeeeee
Commercial model + API question,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.38,akuhl101
Can Anybody help me with my school project,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.75,Wannabedankestmemer
GPT4All now supports GGUF Models with Vulkan GPU Acceleration,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.96,NomicAI
Help with objective tokens per second measurement,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,zDraco_Meteor
"Finetuning multimodal vision models? (Llava, and BakLLaVA, Obsidian)",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.76,Neptun0
What kind of mini PC can handle a local LLM?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.86,Malin_Kite
Minimum acceptable tokens per second poll,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.6,128username
Refact (1.6B) on Pixel 3!,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.98,Aaaaaaaaaeeeee
"Old Comp Running LLM! I got llama-2-7b-chat.Q2_K.gguf running on 10-year-old iMac, response in 40-sec at 1-t/s",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.94,Actual-Bad5029
Cost of running llama2 7B locally,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,OneConfusion3313
PockEngine: Sparse and Efficient Fine-tuning in a Pocket,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.96,ninjasaid13
Can time compensate for lack of power?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.92,Sandy-Eyes
"GPUs and Local Large Language models, Are there Possible Tricks for the big ones? AMD vs Nvidia? NVLink? Rocm? XGMI?",Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,gabrielesilinic
Local/Dev environments. What are you using?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,Torocatala
"llama_mpi, anyone use it?",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.92,ccbadd
I've made a customisable SMS personal assistant which has infinite and persistent semantic memory.,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.89,Gromchoices
How much / why does quantization negatively affect LoRA training on LLaMA?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.84,jubilantjerry
"My gaming rig is collecting dust, should I convert it into an LLM server?",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,michaelthatsit
Tuning an LLM on my own notes over time,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.93,michaelthatsit
Apple's Metal is getting bfloat16 support,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.96,MrBeforeMyTime
🐺🐦‍⬛ LLM Format Comparison/Benchmark: 70B GGUF vs. EXL2 (and AWQ),Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.99,WolframRavenwolf
Comparing LLaMA and Alpaca models deterministically,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.99,WolframRavenwolf
"A completely open-source AI Wearable device like Avi’s Tab, Rewind’s pendant, and Humane’s Pin",Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.93,No-Camel-3819
A new type of transistor is more efficient at (some) machine learning tasks,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.95,neph1010
"Who needs ""Galaxy AI"" or ""Gemini"" when real models run fine on phones?",Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.91,ForsookComparison
Local LLaMa on Android phone,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.99,AstrionX
What's the best 1-4B LLM for a phone?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.92,Whydoiexist2983
Best model to run locally on an Android phone?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,Ok-Recognition-3177
Ai on a android phone?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.88,Gaming-invisibleman
Talking to an AI as if you were talking them on the phone.,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.95,Erdeem
Why isn't anyone building an Oogabooga-like app for Android and iPhone?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.5,Winter_Tension5432
Has anyone use Snapdragon 8 gen 2 phones to run 7B/10B models?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.95,hoseex999
How to run mistral-7b-openorca on Android phone?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.84,ZotD0t
Running Wizard 7b (Q2) on an 8gb Android Phone,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.89,Due-Ad-7308
Running a small model on a phone?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.88,ScoobySnackzz12
Most User-friendly Method of Running Local Models on Android Phones?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.91,OldAd9530
Is there currently an off-the-shelf end-user application to run inference on one machine and output it on a phone?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.61,jon-flop-boat
What is the best config to run Mistral 7b on an Android phone?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.81,gianpaj
Is there anything to really chat with an AI as you would do with somebody over the phone ?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.97,alexthai7
"(Opinion) Apple's LLMs will be able to run on iPhone, served by a MacBook / Mac Studio over AirPlay",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.54,OldAd9530
"Running RedPajama and other open LLMs on phones, browsers and AMD/NV/Intel GPUs",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.98,yzgysjr
Gemini Nano is a 4bit 3.25B LLM,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.98,Amgadoz
What are your thoughts on the future of LLMs running mobile?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",1.0,Tree-Sheep
"Besides curiosity, what do you use local LLMs for?",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.87,RedditPolluter
GPT-4 details leaked,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.97,HideLord
"I love running locally, but",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.72,__Maximum__
Absolute cheapest local LLM,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.93,SporksInjected
Found an iOS app for TestFlight that allows you to run LLMs locally,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.94,HemisphereGuide
I wonder theres way to run LLM without loading on ram,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.69,wjohhan
"Gemma locally on iOS, Android, web browsers, and GPUs with a single framework",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.78,SnooMachines3070
I cancelled my Chatgpt monthly membership because I'm tired of the constant censorship and the quality getting worse and worse. Does anyone know an alternative that I can go to?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.84,SerpentEmperor
How much more can the current model sizes improve?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.95,Admirable-Star7088
Fine tunning LORA/QLORA LLM,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,Muted-Mine-5236
Starling-RM-7B-alpha: New RLAIF Finetuned 7b Model beats Openchat 3.5 and comes close to GPT-4,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.92,Legcor
LLM on smartphones with large RAM,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.81,rapidashlord
What's your 2024 AI Predictions?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.95,dulldata
[Project] MLC LLM for Android,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.98,crowwork
Local LLM with mobile app?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,Data_Driven_Guy
(WebLLM) Is Decentralised AI Possible?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.6,Accurate-Screen8774
dolphin-2.2-yi-34b released,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.98,Amgadoz
Running LLMs on a Kirin 9000S?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,Tree-Sheep
[Project] MLC LLM: Universal LLM Deployment with GPU Acceleration,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.96,crowwork
2.7B model that performs like Mistral 7B!,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.92,koehr
Why you use others versions of Llama if we have the most powerful which is Llama 2 - 70B,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.42,HighWillord
Cheapest and best way to run LLM online on an on-demand basis?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.87,broodysupertramp
Do the LLMs of the future consume much less?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.75,Terrible_Vegetable4
"Converting ""outdated GGUF"" to modern one?",Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,thegreatpotatogod
Looking for a Handy LLM for Survival & DIY – Any Suggestions?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,cajun_spice
How to run LLMs on Android/IOS?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,LiquidGunay
Apple CoreML,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.95,krazzmann
LLM.swift library lets you interact LLMs on iOS easily,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.95,eastriver0720
Running Phi-2 locally in Android Chrome browser with WebGPU,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.98,SnooMachines3070
Termux bash script for YT summarizing,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,Aaaaaaaaaeeeee
Can Agent-to-Agent communication be encrypted by said agents (not available to humans)?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.69,robertverdes
Running Google's Gemma 2b on Android,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.97,Electrical-Hat-6302
Microsoft's Phi 1.5 (1.7 B parameter) is now a multimodal model,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,Ok-Recognition-3177
Best 3B LLM model for instruction following ?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.92,Fit_Check_919
Anyone tried Guardrailing.ai a local LLM,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.75,kaotec
Look ahead decoding offers massive (~1.5x) speedup for inference,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.99,lans_throwaway
some tests on rock 5b ARM SBC(RK3588),Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.72,Dyonizius
OpenAI API update (mostly for coders),Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.85,CodeGriot
HackerNews AI built using function calling,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.9,ashpreetbedi
Do any of you have slight concerns that these models may have some subjective experience and we are effectively creating and killing people every time we train and delete them?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.44,30299578815310
GPT4ALL - best model for retrieving customer information from localdocs,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.92,leonbollerup
Yup. Works great!,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.95,FPham
I try to run the mistral 7B in an iphone,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.93,engkufizz
Llama2 Qualcom partnership,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.86,AstrionX
Why is there no copilot for messengers?,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,AbstractContract
Any suggestions for an open source model for parsing real estate listings?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",1.0,benderlio
Best CPU Model to extract data from raw text?,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.67,okyaygokay
Starting Model,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.83,Figai
Using a local LLM for large-scale text analysis,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.79,panthaduprince
Looking to recreate a cool AI assistant project with free tools,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.85,Fantastic-Air8513
Adding a new language to Mistral 7B,Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.88,DrJokeTech
Local Autonomous Coding?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.64,Sl33py_4est
So I was thinking about the maths problem.,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.5,aseichter2007
Comparing 4060 Ti 16GB + DDR5 6000 vs 3090 24GB: looking for 34B model benchmarks,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.81,regunakyle
Small First Aid / survival LLM,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.78,Balance-
Complete noob has some questions,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.75,Environmental-Land12
Wizard-Vicuna 7b (Q2) running at 2.5 tokens/second on Galaxy S23 Ultra,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.94,Due-Ad-7308
Llama.cpp Python Tutorial Series,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.92,ChristopherGS
Best vision model for dense OCR?,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",1.0,madmax_br5
Exllama outside of text generation webui?,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",1.0,turamura
Yet another quantization method: SpQR by Tim Dettmers et al.,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.99,rerri
Need help standing up LLM that can produce strict JSON,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.5,No-Barber6403
"Smaller models (<7b): How much ""intelligence"" can we extract from them? Mistral 7b shows there's still room for improvement, but are we near the upper-bound capacity of 7b models?",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.94,nderstand2grow
Refact (1.6B) on Pixel 3!,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",1.0,Aaaaaaaaaeeeee
"We're not in Kansas anymore, Toto - CVEs are popping up in local LLM platforms",Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.91,Longjumping-City-461
Any services that can help host LoRA (QLoRA) of Mistral 7B?,Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",1.0,distant_gradient
CPU and RAM for inference,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.89,Slimxshadyx
"Search/Look-up text messages, Lora or Vectors?",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",0.67,cloudsthro
MLC vs llama.cpp,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.7,Robert-treboR
Small request - how would you recreate goblin.tools locally?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.81,ethereal_intellect
How do you prevent your LLM from overgenerating?,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.92,lukeborgen
"LlamaTale v0.12.0 - OpenAI support, Zone generation, improved combat and NPC Idle actions",Topic 9,"embeddings, model, fine, tuning, v, apple, small, context, vector, mac",1.0,neph1010
Recommend a Local LLM for low spec laptop,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.73,Little-Shoulder-5835
Releasing local.ai - an LLM local playground with minimal setup,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.99,louisgv
Clinical medicine LLM for iOS app,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.64,No_Slip497
ggml,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.94,tlpta
"All AI Models, from 3B to 13B running at ~0.5 tokens/s, what could be causing this?",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.89,OfficialHaethus
Where do you think local LLMs will be 2 years from now?,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.67,Necessary_Ad_9800
OpenAI API Question,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.67,pirasanna_9
Help me discover new LLMs for school project,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.71,shadowofdeath06r
Single board computers with 64 GB RAM,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.91,jsalsman
We living in the future now - I have a Local LLM running off my Steamdeck via Kobold w/ CLBlast.,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.98,SouthRye
Dragon Ball Fight Simulation: Son Goku vs Super Buu (GPT4 x Vicuna 13B 4bit),Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.77,reneil1337
Formatting Training Datasets? Getting pwned,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",1.0,TaleOfTwoDres
Why is the idea of more than one participant so foreign to LLMs other than GPT4?,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.33,TradingDreams
Llama.cpp with 13B is hallucinating in my domain?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.82,fhirflyer
Starling won't stop spitting training data!,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.86,TopeQuant
Suggestions for a simple/unintelligent local chat bot,Topic 8,"local, llm, model, best, hardware, current, gpu, app, anyone, working",0.8,GlobalRevolution
How has this not been mentioned here before? Layla,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.79,Derpy_Ponie
Good starting model for fine tuning and how to fine tune and quantise VLLMs,Topic 6,"gb, model, llama, run, ai, hardware, running, android, ram, phone",0.71,Figai
Freezing during inference,Topic 7,"llm, local, llama, api, rag, run, gpu, running, way, get",0.6,_supert_
"Training a model on French Law, any insights?",Topic 5,"model, llama, language, small, mistral, memory, best, embeddings, token, whats",0.91,cocoadaemon
"I have many questions and I seek answers, please help me.",Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.59,CrimsonRedstone
"I finetuned on Sam Altman and Paul Graham's blog posts, I'm pretty happy with the results",Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.77,Chance_Confection_37
What is and isn't possible at various tiers of VRAM? And not just in LLMs?,Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",1.0,ThrowawayProgress99
Great interview with Tim Dettmers (bitsandbytes / QLORA),Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",1.0,PookaMacPhellimen
Koboldcpp now has Linux binaries / Micromamba (conda) based runtime,Topic 1,"hardware, model, llm, best, use, linux, performance, search, guide, text",0.98,henk717
Best Mistral or Mixtral fine-tunes for Function calling and JSON extraction ?,Topic 0,"model, llm, device, time, llamacpp, running, inference, new, performance, mobile",0.83,RageshAntony
[Webui-API] Struggling to Proofread Translated Novels. Any Help Appreciated.,Topic 4,"model, question, help, rag, setup, parameter, use, text, best, exllama",0.81,Demigod787
"Finetuning to beat ChatGPT: It's all about communication & management, these are already solved problems",Topic 3,"llm, run, inference, rtx, using, finetuning, local, pi, llama, training",0.93,Pan000
Comparing LLaMA and Alpaca models deterministically,Topic 2,"model, llm, open, small, source, locally, running, local, llama, android",0.99,WolframRavenwolf
