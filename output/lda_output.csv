Title,Assigned Topic,Topic Keywords
(Meta research) MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
MediaTek Leverages Meta’s Llama 2 to Enhance On-Device Generative AI,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
My open-source & cross-platform on-device LLMs app is now available on TestFlight & GitHub. Feedback & testers welcome.,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
I've created Distributed Llama project. Increase the inference speed of LLM by using multiple devices. It allows to run Llama 2 70B on 8 x Raspberry Pi 4B 4.8sec/token,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Run models on a real Android device with Qualcomm AI Hub,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Native LORA finetuning on Apple Devices (New MLX Framework) 😲!!,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
llm on mobile devices - reading pdf usecase,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"Opentensor and Cerebras announce BTLM-3B-8K, a 3 billion parameter state-of-the-art open-source language model that can fit on mobile devices",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
FP4 quantization state not initialized. Please call .cuda() or .to(device) on the LinearFP4 layer first.,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Has someone tried LLMFarm for native inference on iOS devices?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
How do I run Stable Diffusion and LLMs from my PC on my mobile device? Offline and private ways?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"I'm about to open source my Flutter / Dart plugin to run local inference on all major platforms. See how it runs on my personal Apple devices: macOS (Intel & M1), iOS, iPadOS. Next up: Android, Linux & Windows. AMA.",Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
What am I doing wrong?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
I wonder theres way to run LLM without loading on ram,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
A local LLM is the ultimate doomsday device,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"Fine tuned coqui XTTS voice, how to use the model.pth?",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Maybe we will be able to run far larger models on Apple hardware than previously thought,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Apple CoreML,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Any way to optimally use GPU for faster llama calls?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Mbp m3,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Simple demo app of TinyStories-1m that runs locally on iOS,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
GPU-Accelerated LLM on a $100 Orange Pi,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Why is the idea of more than one participant so foreign to LLMs other than GPT4?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Two RTX 3060 for running llms locally,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Speculation. Could Apple pull off an end run around everyone else and offer low cost large model inference?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Merging LoRA with Mistral models?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
[Project] MLC LLM for Android,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
TinyLlama-1.1B: Compact Language Model Pretrained for Super Long,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"Is anyone inferencing on something like an Intel nuc, barebone or similar formfactor?",Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Deploying LM/LMM to edge device,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Looking for a lightweight Model ideally fine-tuned for generating compliments or similar,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
oobabooga Update broke loading u/The-Bloke huggingface models?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Optimum Intel OpenVino Performance,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Text generation doesn't stop after LoRA fine-tuning,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Guide: Installing ROCm/hip for LLaMa.cpp on Linux for the 7900xtx,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Is Llama 2 7B or 7B Chat better for a talking Teddy Bear?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Why are MLX and Ollama way faster than PyTorch on M1 Mac?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Skill Issue while running GPU accelerated llama,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Silent Release: Llama2 7B on Snapdragon Gen2 with 8 tok/s,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
What are your thoughts on the future of LLMs running mobile?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
A fine tuned Llama2-chat model can’t answer questions from the dataset,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Should I start this project?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
MLC-LLM Chat vicuna-Wizard-7B-Uncensored-q3f16_0,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Is it possible to process layers one by one on a low vram gpu?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
M3 Max/Ultra vs RTX3090 vs RTX4090 with large context windows?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Home setup for future home model use,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Llama 2 as a local copilot!!,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Llama finetuning question,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Apple releases MLX for Apple Silicon,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Awful quantisation outputs with V100,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
CLblast is nice on crap systems!,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Having a Local LLM interact with an API,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Has Anyone Successfully Utilized the Neural Networks API on Android for LLMS with EdgeTPU?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Best model to convert voice commands to JSON?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"When LLM doesn’t fit into memory, how to make it work?",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
No CUDA GPUs are available error on text generator webui,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Let's create a 65B benchmark in this thread,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Apple has an excellent hardware base for local generative AI,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Best backends for running models on Android?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Llama 2 7b-Instruct on 2 RTX 2080 Ti GPUs,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Is Llama.cpp Using GPU's on my M2 Max?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Single RTX 4090 FE at 40 tokens/s but with penalty if running 2 get only 10 tokens/s. Confirmed with Xwin-MLewd-13B-V0.2-GGUF.,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
2x Teslas in an OEM system,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Most advanced LLM for a Jetson Orin Nano?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
What 7b llm to use,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Accelerate not working with merged model?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Koboldcpp linux with gpu guide,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Success with a local voice chat agent,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
"Finally got a model running on my XTX, using llama.cpp",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Any interesting LLM project ideas?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"Vicuna 13b on RK3588 with Mail G610, OpenCL enabled. prefill: 2.3 tok/s, decode: 1.6 tok/s",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
ONNX to run LLM,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Introducing Vaartaalaap: A Chatbot UI for Local LLM Servers,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"Could I serialize models in current ""state""?",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"MiniCPM: An end-side LLM achieves equivalent performance to Mistral-7B, and outperforms Llama2-13B",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
A new type of transistor is more efficient at (some) machine learning tasks,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
How to run base models w. finetuned adapters in LlamaIndex or Langchain?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Slow LLM speeds on RTX 4090,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
LLM.swift library lets you interact LLMs on iOS easily,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
[Project] Making AMD GPUs Competitive for LLM inference,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"Need Help Optimizing Language Model Performance on Nvidia Jetson AGX Xavier
",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
I made a language-agnostic function calling (open source),Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
flan T5-Large just gives the context as the response,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Running Llama2 on Android,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Utilizing two different size GPUs for fine-tuning,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Estimated Time for SFT Fine-Tuning of Mistral-7B Model,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Exploding loss when trying to train OpenOrca-Platypus2-13B,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Anyway to use Runpod on android?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"I love running locally, but",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
I love hallucinations,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Speed difference not matching file size between quants?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Which is the smallest Llama model out there?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Help needed on building doc translation LLM,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Why do prompts work so differently depending on the model used?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
How to compile models for MlC-LLM,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Weird Dual GPU setup in Ooba,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Guide for oogaboooga on amd using rocm gpu on linux ubuntu and fedora,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
I built my own android chatting frontend for LLMs.,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
An Alternative Approach to Building Generative AI Models,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"No-code, uncertainty-aware LLM classification and fine-tuned semantic search for Mac",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Unable to train LLaMA2-7B-HF in an RTX 3050,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Lightweight LLama variants for Mobile applications,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Half Precision (e.g. no quantization) Phind V2 CodeLlama 34B running on Mac M1 at 8.6 tokens per second,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Mark Zuckerberg on upcoming LLaMA v2,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Model parallelism with LoRA,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
(New Model) Rift Coder 7B. Python & TypeScript Fine-Tuned Code Llama for IDE Use,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Llama2 Qualcom partnership,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
A Free AI Scribe Project I am Working on! Please Provide Feedback!,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
LLaMA for poor,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Using LLMs to build custom Operating Systems?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
"Used GPT4ALL for the first time and wondering if I could somehow feed it (technical) PDFs and turn it into ""sidekick"" for embedded programming?",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Guide to running llama.cpp on Windows+Powershell+AMD GPUs,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Commercial model + API question,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Since the launch of the AI Pin, have there been any alternatives announced that are open platform?",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Dual 3090 ti GPU's on Ubuntu Desktop x64 help,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Fine-tuning for custom domain knowledge,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Exllama on windows using CPU,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
"Documentation Paradigm for Large Language Models (LLMs): Log Today, Train Tomorrow",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
LLaMA-4bit inference speed for various context limits on dual RTX 4090 (triton optimized),Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
num_beams > 1 breaking my model (Open-LLaMA7b - Alpaca-finetuned),Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Code Llama - The Hugging Face Edition,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Can't use multi-gpu with 8x A100 80GB,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Some advice on running Guidance?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
[HELP] It's there a way to make Llama 2 model generate text token by token or word by word like what ChatGPT does?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Best prompt and model for fact-checking a text (disinformation/fake-news detection),Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Most performant option to run 13b LLM locally as a personal assistant for under $300 USD,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Is the Nvidia Jetson AGX Orin any good?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Time to first token (TTFF) with llama.cpp vs. vllm,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Anyone working on a ""tiny"" version of Mixtral-8x7b",Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
How to fine-tune Llama 70B fp16 on 8x A100 80GB?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
I am looking for information regarding Running llama on a zen4 or xeon 4th generation cpu? Or alternative no gpu suggestions (for 180b falcon),Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Running Llama-65B with moderate context sizes,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
CogAgent: A Visual Language Model for GUI Agents,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Anyone working on linking local Ai with Home Assistant?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
HF transformers vs llama 2 example script performance,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Running [Crataco ]Pythia Deduped GGML on 4 GB Ram Laptop,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
KoboldCpp - Combining all the various ggml.cpp CPU LLM inference projects with a WebUI and API (formerly llamacpp-for-kobold),Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"I will do the fine-tuning for you, or here's my DIY guide",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"Working on a QLORA hub for model personalities, help needed",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Error in load_in_8bit when running alpaca-lora using 3080,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Yet another quantization method: SpQR by Tim Dettmers et al.,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Falcon 180B GPTQ Model on Multi-GPU Setup with RunPod,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
A simple Huggingface Downloader,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
llama2-7B/llama2-13B parameter model generates random text after few questions,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Can we create a megathread for cataloging all the projects and installation guides of Llama?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Why does my fine tuning of Mistral 7B Instruct not stop?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Help to use pipeline conversational on mistral instruct v0.2,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Improving Falcon-180B Performance,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Building an IDE with native support for Open Source models,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Best setup and Settings for a Beginner?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"Trying to load togethercomputer_LLaMA-2-7B-32K with fully loaded context but it OOMs, but I should have enough VRAM?",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Building koboldcpp_CUDA on Linux,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Retrieval Augmented Generation optimised Llm's,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Guanaco fine-tuning for classification,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Help with objective tokens per second measurement,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Finetuned Mistral outputting multiple Q and A responses.,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Codelamma 7b code completion giving multiple responses and i only want one?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
I am not special. Though I seek guidance from those that are.,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Hugging Face community blogpost: 🕳️ Attention Sinks in LLMs for endless fluency (related to StreamingLLM),Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
I wish there was a market for buying access to proprietary LLMs to run locally,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Training LLaMA-2 for Keyword Extraction,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Limiting GPU memory allocation by LLM during inference/generate step.,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Keep running out of memory when pre-training (without LoRA) a model 7B on 2 A100 80GB GPU?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Was Joi from Blade Runner 2049 a local LLM?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
How do I load a gptq LLaMA model (Vicuna) in .safetensors format?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"New Oobabooga Standard, 8bit, and 4bit plus LLaMA conversion instructions, Windows 10 no WSL needed",Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Multi GPU splitting performance,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Trouble creating document Q&A chat bot,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
LOL why did this work on InternLM 20b?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
How to perform multi-GPU parallel inference for llama2?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
SillyTavern 1.10.0 Release,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Engaging topics for conversations in a small local workshop,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
New badass model OpenAssistant/llama2-13b-orca-8k released 🎉,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
"Can we discuss MLOps, Deployment, Optimizations, and Speed?",Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Increasing speed for webui/Wizard-Vicuna-13B with my Mac Pro M1 16gb setup?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
LLMs that can run on CPUs and less RAM,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Thunderbolt and multiple eGPUs,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
PrivateGPT - Asking itself questions and answering?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Text Generator webui - how do I select which GPU to use?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Help for the n00b? Optimal loader parameters...,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
TIL Sharding a model into smaller chunks may make it possible to load in Free Colab instances without running out of memory,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Seeking Orientation on Developing an LLM-Based QA Chatbot,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Train model from scratch (llama.cpp) - any experiences?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Splitting models and device map.,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
"Different results with float16. [Actually, gemma-7b-it does not work with float16]",Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Desktop 3xP40 Rig,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Can't load new Landmark models in ooba, complains that trust_remote_code not enabled when it clearly is",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Appreciation and Inspiration!,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Performance report - Inference with two RTX 4060 Ti 16Gb,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Why Mistral-7b is repeating the promp everytime I ask him something?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Phi-2 & Tiny LLama on Raspberry Pi 5,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"Training Large Language Models: Fluctuating Training Loss But Smooth Eval Loss, What's Happening?",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Newbie , installed dalai with llama locally, trying to make sense of responses",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
How to install LLaMA: 8-bit and 4-bit,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Vicuna on AMD APU via Vulkan & MLC,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
New trained storytelling LoRA returns really interesting results.,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Just sharing some quick tips to painlessly install PrivateGPT on your windows machine with Ubuntu as WSL (Windows Subsystem for Linux),Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Project: Using Mixtral 8x7b Instruct v0.1 Q8 to Generate a Synthetic Dataset for LLM Finetuning,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Llama.cpp with 13B is hallucinating in my domain?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
How to Get Around Context Limits with Data Files,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Looking for for folks to share llama.cpp settings/strategies (and models) which will help write creative (interesting), verbose (long), true-to-prompt stories (plus a short discussion of --multiline-input flag)",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Perplexity Testing Mac vs Windows Pt 2- Mac still 3x lower,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Quick hardware comparison Ryzen 5700X, RTX 3090, Mac Studio M1",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
LLM for chatting and command recognition,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Perplexity Testing Mac vs Windows Pt 4: CPU test time. Results continue to point to a fundamental difference of Metal inference,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Perplexity Testing Mac vs Windows Pt 3: Adding context for context; something is definitely different,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Error while finetuning,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Comparing LLaMA and Alpaca models deterministically,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"Comparing Image/Chart/Illustration Descriptions generated by GPT-4V, LLaVa, Owen-VL for RAG Pipelines",Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"🐺🐦‍⬛ LLM Comparison/Test: 6 new models from 1.6B to 120B (StableLM, DiscoLM German 7B, Mixtral 2x7B, Beyonder, Laserxtral, MegaDolphin)",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
airoboros-65B-gpt4-1.2.ggmlv3.q8_0.bin - harry potter erotica,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Difference between merged model (with adapters) and load adapters on top,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"Is Shared memory on a laptop better than regular desktop DDR5. The 4090 has 64GB of ""total"" memory?",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Best Local LLM For low end device coding,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Inconsistent Token Speed on Llama 2 Chat 70b with Exllama,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"Advice for model to run on laptop gtx 1060 (6gb), i7 8750H, ram 16gb",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"Low token/s count on 7b models, is this normal",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
NVidia vGPU on esx,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
CodeLlama 70B Local Deployment with JIT Compilation,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Speculative Decoding in Exllama v2 and llama.cpp comparison,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Your Mixtral Experiences So Far,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Complete guide for KoboldAI and Oobabooga 4 bit gptq on linux AMD GPU,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Understanding embeddings,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Can I use embedding models from the MTEB board with generate embedding api of ollama to generate embedding?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Llama2 Embeddings FastAPI Service,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Embeddings or Instructor Embeddings?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Fine tuning embeddings,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
nomic-embed-text-v1.5: Resizable Production Embeddings with Matryoshka Representation Learning - Scalable Vector Embeddings from 64 to 768 dimensions,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
One Embedding To Rule Them All?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
How does resize_token_embeddings() refactor embeddings for newly added tokens?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
First OS embedding model with 8k context,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
What Embedding Models Are You Using For RAG?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Chunking Text & Normalizing embeddings (C#),Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
I'm confused about embeddings,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Embeddings?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Introduction to Matryoshka Embedding Models with Sentence Transformers,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Code LLaMA embeddings?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Finding better embedding models,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Why Aren't Custom Embeddings Helping More?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Testing Google Multimodal Embeddings,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
RAG Embeddings,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
What hardware is better for embedding,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
New Model: Nomic Embed - A Truly Open Embedding Model,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Are there any boilerplate RAG/embedding programs? (python),Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Embeddings vs Context,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
txtai 6.0 - the all-in-one embeddings database,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
RAG in a couple lines of code with txtai-wikipedia embeddings database + Mistral,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
What are the benefits of using open source embeddings model?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Local Rag/embedding clarifications,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
TF-IDF + Embedding for RAGs?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
How important is choosing an embedding model?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Lora vs Embeddings (Vector DB?) Knowledge Training,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Hi I'm seeking for any embedding model for vietnamese,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Looking for open-source contributors for text-embedding server for inference,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
PEG (Progressively Learned Textual Embedding),Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Minimal local embedding?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
What embedding model do you guys use?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Interpret Llama.cpp embeddings,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
What makes a good embedding model?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Chunking and embedding XML,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Does including timestamps in a transcript degrade embedding performance?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
NEFTune: Noisy Embeddings Improve Instruction Finetuning,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Question about embeddings and vectordatabase,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
LLM for RAG - embedding and chat not compatible?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Hosting your own embeddings API,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"Infinity, a project for supporting RAG and Vector Embeddings.",Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
What's the SOTA for open-source search embeddings?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Keeping the input embeddings entirely on CPU in PyTorch,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
OpenAI Embeddings API alternative?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"Help me choose: Need local RAG, options for embedding, GPU, with GUI. PrivateGPT, localGPT, MemGPT, AutoGen, Taskweaver, GPT4All, or ChatDocs?",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Which 30b model should I use for embeddings?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Creating your own embedding model to boost retrieval performance,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
What is the best embedding search?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Would you use a free and anonymous (no login) website for embeddings?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Embeddings for Q&A over docs,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Is it possilbe to create embeddings using LLama2 model?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Kalosm v0.2.0: A simple open source framework for embedded AI,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Need guidance to achieve semantic search using embeddings for non English text,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
UAE: New Sentence Embeddings for RAG | SOTA on MTEB Leaderboard,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
PubMedBERT Embeddings with Matryoshka Representation Learning enabling dynamic vector sizes,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Can we do similarity search with different embedding models?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
VectorDB with Llama Embeddings - Few Questions and Doubts,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Why does positional encodings add to token embedding instead of having a separate embedding dimension just for the position?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
h2ogpt on CPU: any experience on how long embeddings creation will take?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"[Text embedding] Why do I need to run a model to generate embeddings for an entire book, if I can run it for every possible token and perform mean pooling?",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Best Practices for Semantic Search on 200k vectors (30GB) Worth of Embeddings?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
'Missing tok_embeddings.weight' when using GGML models,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Searching for basic chunking - embedding example,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
New embedding models from Jina AI,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"Fast Vector Similarity Library, Useful for Working With Llama2 Embedding Vectors",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"Embeddings for Search, alternatives?",Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Question about multiple sources with vector embeddings & local LLM.,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"As of 2024, what is the state of non-english models (embedding/LLM) for RAG?",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Is local LLM models (Zephyr, Mistral, etc..) good enough to generate embeddings? (using Ollama)",Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Why is there not as much cutting edge research into embeddings?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
embedding from RedPajama INCITE chat 3B,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Is Embeddings are giving better answer than finetuned model?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Can I use a LocalLLaMA to create embeddings from my text?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Is words represented as embedding fed to the LLM's best way to train and do inference?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"Retrieval, Extensible search, Embeddings, or Teaching",Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Build Enterprise Retrieval-Augmented Generation Apps with NVIDIA Retrieval QA Embedding Model,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
How do you determine which embedding models will fit into memory available?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
How can I use embeddings from llama.cpp using OpenAI in Python?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Is there any research on using embedding as tokens to dramatically increase transformers context limits?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
PubMedBERT Embeddings - Semantic search and retrieval augmented generation for medical literature,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
What are the text chunking/splitting and embedding best practices for RAG applications?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
German language embedding model for fine tuned Mistral 7B model ( Leo LM &EM_German) for RAG based implementation.,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Passing embeddings to llama with ctransformers for long term memory,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Best OS/Paid Embedding Model for Longer Token Length + Retrieval,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Reality check on good embedding model (and this idea in general),Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
"bge-m3 - a multilingual embedding model, from the authors of bge-large-en-v1.5 that topped for a long time the MTEB leaderboard",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Langchain Vicuna Server - Added Support for GPTQ-4bit and Experimental Vicuna Embeddings (Hugging Face only),Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"Any similar no code, easy to use tools/methods for knowledge embedding / RAG that work with local LLMs",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Seeking Opinions on e5-large-v2 and instructor-xl Embedding Models for Multilingual Applications,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"Used GPT4ALL for the first time and wondering if I could somehow feed it (technical) PDFs and turn it into ""sidekick"" for embedded programming?",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
fastText: Embeddings for 157 languages for identification and similarity search tasks,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"HuggingChat, the open-source alternative to ChatGPT from HuggingFace just released a new websearch feature. It uses RAG and local embeddings to provide better results and show sources.",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Sojee: My own little dual-stage prompt embedding chatbot that can be quickly customized to any particular corpus.,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Yet another RAG system - implementation details and lessons learned,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Any ideas? Pdf -> Q&A form -> embedding -> vector db -> similarity search -> chat gpt 3.5 answer,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Today is the first day I’m getting results comparable to GPT4 on OpenSource LLM workflows.,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Introducing LocalGPT: Offline ChatBOT for your FILES with GPU - Vicuna,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Huge issue with TruthfulQA contamination and license issues on HF 7B leaderboards,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Apple silicon local inference - can M3 pro max do it all?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
What's a **fair** way of computing similarity across sequence length?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
RAG beginner questions,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
LLongMA 2: A Llama-2 8k model,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Whats in your RAG setup?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
R2R – Open-source framework for production-grade RAG,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
This can make a huge difference. Extending context from 4k to 400k. Llama-2-chat-7b.,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Hermes LLongMA-2 8k,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Open/Local LLM support for MineDojo/Voyager,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
LLongMA-2 16k: A Llama 2 16k model,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
MoE-LLaVA: Mixture of Experts for Large Vision-Language Models - Peking University 2024 - MoE-LLaVA-3B demonstrates performance comparable to the LLaVA-1.5-7B !,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
"I'm building an Open Source (and optionally local) Google for LLM Agents, any requests?",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Anyone who has worked with using LLMs for creating recommendation engines,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Is my Data safe when using trust_remote_code?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
My experience on starting with fine tuning LLMs with custom data,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Create/Query vector database with LLM,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Survey about Retrieval Augmented Generation (RAG) in Real Production,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"Gemma finetuning 243% faster, uses 58% less VRAM",Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Extending LLM's Context with Activation Beacon [Model/code release],Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Chat with RTX on a large folder of PDFs - how long will this take?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Launching AgentSearch - A local search engine for your LLM agent,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Swiss Army Llama: Do tons of useful stuff with local LLMs with a REST API,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Some questions of implementing LLM to generate Q/A pairs based on local documents,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"txtai 6.3 released: Adds new LLM inference methods, API Authorization and RAG improvements",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Tokenizer of GGUF with LlamaCPP,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
RAG + real TXT book + Yi34b-chat = creative writing beast,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Explain Re-Ranking,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
LLongMA-2 13b 8k,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Fantasy writing assistant model?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
The last LLM (or Over-hype will cause another Winter),Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Cross pollination of ideas from Stable Diffusion: Textual Inversions,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Expand the Context Length with RoPE from a β-based Encoding perspective,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
How to process queries that require real-time information with RAG?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Problems using quantised llama models with cpu,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Training an LLM on multiple documents: first steps.,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Semantic text similarity,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Mistral Vision/Audio LoRAs & a Lossy 260K+ Token Context Window Prototype,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
AI and RAG in the context of a public library,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Scalable distributed computing open source RAG framework using Ray,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"Google is training with 8 bit ints, next will move to 4 bit ints.",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
GPU-Accelerated LLM on a $100 Orange Pi,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Rag vs Vector db,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Reor: an AI personal knowledge management app powered by local models,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Is it feasible to use an open source model & vector database to manage a growing library of ebooks & papers?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Chatting with internal company documents,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Simple ollama rag in python,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
FAQ Retrieval design,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Can't handle efficiently RAG with large PDF,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Fundamental limitations of *current* LMM approaches,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Impact of regulations on open source LLM,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
New to LLMs: Seeking Feedback on Feasibility of Hierarchical Multi-Label Text Classification for Large Web Archives,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
How to handle dependencies between text changes in RAG,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
How to build a multimodal RAG FAQ app with pptx documents,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
LLM with Built In Vector Search and Database,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
My meta-llama/Llama-2-7b-hf fine-tuned model does not learn to use the additional special tokens,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Llama-2-13b and document QA,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
how to embed code?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
LaVague: Open-source Text2Action AI pipeline to turn natural language into Selenium code,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Trying to understand what tokens are focused in to start inference with attention weights,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
How do I automate the back-and-forth process of running and eliminating bugs from code generated by LLMs?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Favorite RAG tools/frameworks [Early Feb 2024],Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Guide on building training datasets from unstructured text sources?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Can I use LLM to compare 2 pdf documents and find changes ?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
How would you do it? Handling multi-turn QA conversation with matching of questions to vector database.,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"A python package I created for llms application including my own implementation of long short term memory and a web search tool for llm, it supports both Openai-like API or loading local models directly from different formats.",Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Looking for sentence-transformer libraries that I can use locally with JavaScript,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
What are interesting open source resources/ projects for building LLMs for India/ Indic languages?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"MultiToken: Embed arbitrary modalities (images, audio, documents, etc) into large language models",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
any ideas on how sites like Grok and Perplexity return the most recent relevant result?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Building a super-simple memory service for language models,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Is there any demand for a Shared Public Contextual Database for RAG?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
"ChatGPT is a Lazy Piece of Shit, CodeBooga Rules",Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Using Generated Q/A Pairs for Fine Tuning? Is It Worth It?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"Ok, I’m just curious of the security risks?",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
End-to-End Encrypted Local LLMs,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Llama 8k context length on V100,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"What are the differences/similarities between Llama 7B, 13B and 70B?",Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Any other fun local AI tools other than ooba and automatic1111?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"Indexing 900Tb as fast as possible, any advices ?",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
RAG Dataset - 3.7M LA Times headlines and links (1914-2024),Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Dataset Distillation,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Does labeling datasets stored in a Vector DB make sense?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Current best codebase for pretraining a model from scratch?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Live Llava on Jetson Orin,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Thinking about a comprehensive RAG,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"Finetune, RAG or live search",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
What is the best model to talk about ai with?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Best DB structure for my use-case...,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Are any Semantic Search Experts Willing to Opine on the RAG Search Building Code?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Train LLM to think like Elon Musk with RAG?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Project Launch: GatoGPT - Local LLM inference & management server with built-in OpenAI API,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"A .gguf chatbot gradio interface experiment, to sequentially chain prompts, scripted in csv file : gpt-sequencer",Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Which database to use for semantic search?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Managing Follow up question for retriever based chatbot,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Best approach for large custom knowledge base LLMs?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
RAG,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Text-gen-webui + RAG,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Retrieving a list of movies from a natural language query, given their plots",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Eploring Methods to Improve Text Chunking in RAG Models (and other things...),Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Long context Fine tune and AutoGPTQ quantization with rope?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
What's the best use case for phi-1 (~1bn param GPT3.5)?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
MoE expert logging?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Running LLaMa on Google Colab/cloud differences w.r.t local system,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Confining LLaMA 2's context for RAG QA,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Do you think there will ever be a time where we reach GPT 3.5 quality LLMs in under 1 billion parameters?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Which Linux distribution is best for LLM development?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Is it feasible to use a NAS to store a vector database and access it in realtime using a local LLM?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"Looking for something better than TinyLlama, but still fits into 12GB",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
RAG oriented fine-tune... Searching for coherence,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
16k context for OpenAI GPT-3.5 API,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Tech Stack recommendations for running RAG locally on a Macbook with M2,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
How to reduce Hallunications of the outputs generated by the LLMs.,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
RAG with KG,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
JSON data for RAG based approach,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"NF4 inference quantization is awesome: Comparison of answer quality of the same model quantized to INT8, NF4, q2_k, q3_km, q3_kl, q4_0, q8_0",Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Can llama 2 continue pretraining using qlora?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Similarity RAG Search?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Document search and retrieval apps (preferably full-stack),Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Need help - RAG on large unstructured PDFs,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Llama on Azure endpoint online and privacy,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Tips on optimization when loading GGUF models?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Trouble understanding the implementation of how Multi-modality is “solved” through alignment of CLIP and LLM,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Feed LLM with local knowledge,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
How to deploy/host custom LLM app for production?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
RAG over Knowledge Graphs,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
How to Get Around Context Limits with Data Files,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"The best model for ""Talk to your data"" scenarios?",Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Llama2 Qualcom partnership,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
How to improve results when using Dolphin mixtral with BakLLaVa multimodal projector?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Best way to interact with local LLMs from distributed app?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"txtai 7.0 released: Adds support for graph search, advanced graph traversal and graph RAG",Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Replacing LLM of Suno Ai's Bark TTS model with Mistral or TinyLlama?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
With LLMs we can create a fully open-source Library of Alexandria.,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Can we talk about back and front end settings?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Fine Tuning Step Wise Instruction,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Url scraping for llama,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Use Llama2 to Improve the Accuracy of Tesseract OCR,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Small llm model within 100M to 1B parameter,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Llama 2 chatbot performance for multiple users,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
What GPU factors boost local performance the most?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Can we extend falcon context length like llama?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
What's the best description you've seen for what fine tuning is and isn't?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
need your input on this,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
I finetuned Phi-1.5 and Phi-2 on MathInstruct using MAmmoTH but...,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
How to run base models w. finetuned adapters in LlamaIndex or Langchain?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
LLMS4Rec through RAG?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
CLEX: Continuous Length Extrapolation for Large Language Models,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Shit hardware and shoestring budgets,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
About to buy Hardware for 7k,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Ai accelerator hardware is slowly becoming available,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"Talk me off the ledge, I want to buy more hardware for local LLMs...",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Advice on cheap hardware,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
200 000 dollars what in hardware can i buy and build with my students,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Options for running Falcon 180B on (kind of) sane hardware?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Hardware guide,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Hardware Design for LLM Inference: Von Neumann Bottleneck,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Hardware,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Recommended hardware (Windows or Linux)?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Hardware Recommendations,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Multi GPU hardware selection,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Hardware needed to run Nous-Copybara 34b quickly?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Hardware problem,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
LLM's in production hardware requirements,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Apple has an excellent hardware base for local generative AI,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"P40 is slow they say, Old hardware is slow they say, maybe, maybe not, here are my numbers",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Hardware leaderboard?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Microsoft and AMD collaborating to improve LLM performance on AMD hardware?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Will this hardware upgrade make sense?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
[Hardware] M2 ultra 192gb mac studio inference speeds,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
recommend hardware for running big LLMs locally?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
How do you guys cope with ai addiction? Hardware advice for someone who realizes is too obsessed.,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
This is the new king of LLM hardware with 576 GB of RAM.,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
25k reports to analyse - Whats the best model/hardware.,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
What are the capabilities of consumer grade hardware to work with LLMs?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Technical question about hardware limits,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Hardware for LLM,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Hardware for local LLaMA for 1.000 - 1.500€?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
What hardware is better for embedding,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Requesting some performance data for pure CPU inference on DDR5-based consumer hardware,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Maybe we will be able to run far larger models on Apple hardware than previously thought,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Hardware for Startup,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Mistral how to run it via API on my hardware?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Best performance for 40-60K€ hardware: lost in the possibilities,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
What is the current best 3B model for low end hardware,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Scaling LLM server hardware question,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Hardware question: combining a 3090 and a p40,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
what kind of hardware would it take to run falcon 180b,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Looking for CPU Inference Hardware (8 Channel Ram Server Motherboards),Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
🖥️🔮 Future Hardware Options for LLMs: Nvidia vs. Apple?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Hardware Q's: Best model performance with 75+ 30 series GPU's?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Benchmarks for LLMs on Consumer Hardware,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Building a AI and Data Science Rig - Using Server Hardware + Ampere GPU's?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
program that does sensible model recommendations based on the current machine hardware?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
A small test I did with falcon-180b-chat.Q2_K.gguf (at home on consumer grade hardware),Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Cheap options hardware for running LLM ?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Is there a way to benchmark hardware performance?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"Quick hardware comparison Ryzen 5700X, RTX 3090, Mac Studio M1",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Hardware explanation,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Prospects for future hardware releases,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
On what hardware/setup are you running your local LLM?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Utilize my current hardware or upgrade?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Hardware needed for LLaMa 2 13b for 100 daily users or a campus of 800 students.,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
What's the biggest size Llama-3 could go to whilst running on consumer hardware?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Is there any way currently to try out and utilize Local LLM's without Enterprise Grade hardware?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
A Review: Using Llama 2 to Chat with Notes on Consumer Hardware,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Great info resource for AI on AMD hardware (Linux),Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Best hardware for <= $7k that allows for speed > GPT3.5,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Question about fine-tuning ~1B LLM on low-end hardware.,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Best hardware for inference requiring 64GB of memory?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Seeking Recommendation - Cooling Hardware for NVIDIA Tesla Cards,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Is it possible to get 100% deterministic results across different hardware?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Could someone summarise the hardware requirements for local models?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Hardware for scaling LLM services,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Looking for hardware and model recommendations -- data center install,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Is it worth it to build/buy a machine now or wait for more AI hardware releases? ,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Help me understanding needed hardware specs/build for running 120B models locally.,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Hardware requirements for GGML quantization,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
How to calculate hardware required for training a model?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Effects of long term use on hardware,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
What are some models you have been running? On what hardware? and why?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Is it possible to train BabyLM on consumer-grade hardware?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
What kind of hardware do you need to run LLaMA locally?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Hardware advice - would this be a good starting PC for home LLM / machine learning?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
What is the tiniest GPT model one can fine tune on home hardware?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Do you run your LLM on your hardware or in a VM?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Will Llama 3 be equivalent in terms of hardware requirements to Llama 2?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Hoping for some advice based on my hardware,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Advice on running models on my hardware (4090 R9-3950x 128GB RAM),Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
What are the hardware requirements for the 70B model in actual production?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Does anyone have a price comparison breakdown of running llms on local hardware vs cloud gpu vs gpt-3 api?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Seeking advice on hardware and LLM choices for an internal document query application,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Hardware requirements to build a personalized assistant using LLaMa,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Question: Hardware development for large transformer models.... ??,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
What hardware do I need for fine tuning/training?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Are there benchmarks out there for comparing hardware?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Need help planning a project with hardware restrictions.,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
How to squeeze more speed with my hardware?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
What's the current best model if you have no concern about the hardware?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Hardware resources needed for training vs running local LLMs?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Hardware recommendations for running stable diffusion and 65b ggml at the same time.,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"With limited hardware (laptop), what kind of local AI would be most viable?",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
What's the best koboldcpp command line/settings for this hardware?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
What are the variables to take into account to calculate the best model that fit in my hardware?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
I am very impressed by Claude.AI. What kind of hardware and models do i need to replicate it?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"GGML hardware questions, regarding a mid 2010s quad CPU Xeon server based build, with intent of 70b unquantized",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"Seeking Advice: Optimal Hardware Specs for 24/7 LLM Inference (RAG) with Scaling Requests - CPU, GPU, RAM, MOBO Considerations",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"If I can't afford to buy the necessary hardware to run a high performance model, is there a service that I can use on a monthly basis to host it for me?",Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"If I don't care about inference time at all, can I run larger models on weaker hardware? I'm fine with like 5-6 tokens a minute.",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Will local hardware ever be able to compete with companies Google or Bing in the search engine space?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Tom's Hardware wrote a guide to running LLaMa locally with benchmarks of GPUs,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Question: Option to run LLaMa and LLaMa2 on external hardware (GPU / Hard Drive)?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
How can I determine my hardware requirements (especially VRAM) for fine-tuning a LLM with a PEFT method? Is there a formula?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Is it possible to run multiple models simultaneously yet? If so what kind of hardware would I need to be able to pull it off?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
A fun experiment running concurrent inference in multiple MacBook Pro's to evaluate model and hardware performance. M2 Max 64GB (34B model) vs M3 Max 128GB (67B model),Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"What is the major difference between different frameworks with regards to performance, hardware requirements vs model support? Llama.cpp vs koboldcpp vs local ai vs gpt4all vs Oobabooga",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
There's a 20% off Ebay coupon right now. It works on a lot of stuff like GPUs. Worth a try to save money on GPUs and server hardware if you are trying to pick up a EPYC machine.,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"Asking for hardware recommendations for a personal machine capable of running +70B models. With cloud options I have to re-download the model every time. Should I bite the bullet and get Mac Studio M2 Ultra ($7000 after tax), or build a PC? What specs do you recommend?",Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Will we see consumer grade AI accelerator cards 2024?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Building the best LLM rig for $10000,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
review of 10 ways to run LLMs locally,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Testing LLamA 2-7B's character impersonation abilities before I buy hardware to run it locally. The idea is to include this character (a perpetually grumpy individual prompted to be so) in an RPG game to generate dialogue through event prompts.,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
One-bit quantization is a thing now,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Anything going on with minimal (virtualized?) standardized hardware? (Kind of like demoscene stuff). Probably has benchmarking and fast-iteration applications? I'm talking setups that would run on any random laptop (without a gpu). Maybe throw in Nix for reproducibility?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"Helpful VRAM requirement table for qlora, lora, and full finetuning.",Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Why run LLMs locally?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"What do you think about the LLM market in the next 12 months? Comments, reasoning, data points are highly welcome! I think LLM's with equivalent to today's GPT-4 performance will get 10x cheaper. Do you think ASIC hardware may appear for crunching transformer models (so no Nividia needed)?",Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Why are you running local models? What are you doing with them?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Is anyone else super eager to upgrade their computer but they're also trying to patiently wait to see what might come out? What's your game plan?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
What is stopping us from creating an open source GPT-4 & Gemini Ultra? (Or better),Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
"I have a dataset of around 100 stories, each around 1000-2000 words each. Can I train a model to generate new stories in similar style and content?",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
is 4 rtx 4090s better than single a6000 ada?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Fastest 7GB Model?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Tim Cook speaks about AI at the Apple shareholder meeting. More on Generative AI later this year. Also that there is no better computer than the Mac for AI.,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Budget of 5-10k to get a good performance,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
"Given the rapid pace of progress, how far are we until local models approach gpt4? Can local models ever be equivalent?",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"🐺🐦‍⬛ LLM Comparison/Test: 2x 34B Yi (Dolphin, Nous Capybara) vs. 12x 70B, 120B, ChatGPT/GPT-4",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
If CPU to GPU memory transfer is a bottleneck why is there no unified silicon from NVIDIA?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
How long would it take for Local LLMs to catch up with gpt-4? Few or several years?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"Let's discuss how technology and society would change with high-end, low-power LLMs",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Build my own version of HackerGPT,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Benchmark post,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
How much more can the current model sizes improve?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Suggestions for a > 24GB VRAM build?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Can I run Mixtral 8x7b?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Open Source RAG - 500 users,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
5x 3090 LLM rig opportunity – stupid decision?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Opinions on Mixtral 0.1 8x7b and Mistral 0.2 7b,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Models Megathread #2 - What models are you currently using?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
"How do the 7B, 13B, 30B, and 65B parameter models compare?",Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
256 GB RAM + Mid GPU vs High End GPU? ,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
I have some questions,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Can you help me understand where the advantages in having an opensource model with the power of GPT-4 would lie?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Fine-tuning on AMD? (7900XT),Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Meta - Other than RP what are you folks doing?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
70B build at $4000,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"If you owned a nvidia tesla a100, what would you do with it?",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
What's the best way to beat GPT-5 with Open Source & Distributed Compute?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Running Llama 70B - Costs and Approaches,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
AMD compability?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Can the RX7900XT (Or any other gpu with 20gb of vram) now run 30b models?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"Would it be possible to ""live train"" an llm on the current conversation, so it's basically ""self learning""?",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Replacing ChatGPT 4 - $20 Subscription,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
FlashDecoding++: Faster Large Language Model Inference on GPUs,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
If I want to train a local model on par with chatGPT how difficult would it be and how much would it cost?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Is fine tuning QLoRA still state of the art?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Chat with RTX is VERY fast (it's the only local LLM platform that uses Nvidia's Tensor cores),Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
What is the best current Local LLM to run?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
PC configuration to run a llama2 70B,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Anyone running dual 3090?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
I've gotten allocation on an enterprise server. Which model type has fastest inference on pure CPU/RAM ?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Any hope for the future of open source?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Especialized Chips to run AI only?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Is there any organization working on an open source 175B model? If not what is the extra compute required compared to current 70B models.,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
[Project] MLC LLM for Android,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Refusals and excuse-making is the poison pill for open source LLMs.,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
2024: Small Models Will Be Insane,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
What would be the smallest open source llm models that are still of reasonable function? ,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
What services do you guys use for hosting models?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Squeezing performance out of models with no video card.,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Question about data privacy,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
What should I monitor while running LLMs?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Local LLM Specifications,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
The Dilemma of AI Accelerators: Bridging the Gap for Affordable Solutions,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Collaborative renting server for LLM,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
"If I have $200 to burn, should I buy 128gb worth of ram or a used gtx 1080?",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"Is Shared memory on a laptop better than regular desktop DDR5. The 4090 has 64GB of ""total"" memory?",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Finetuned llama 2-7b on my WhatsApp chats,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
2-bit Mixtral via HQQ ( requires only 13B of runtime RAM ( runs on RTX 3090 ),Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Help wanted with a project,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Mixtral_7Bx2 MoE GGUF models Q2-Q5,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Euryale 70B vs Nous Capybara 34B for chatting?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Most cost effective GPU for local LLMs?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Experiences with Caching in llama.cpp,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
[Project] MLC LLM: Universal LLM Deployment with GPU Acceleration,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
What is the optimal model to run on 64GB +16 VRAM?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Silicon-Maid-7B is surprisingly good for its weight.,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
LocalGPT on NVIDIA A30 24GB.,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Goliath-120B - quants and future plans,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
RTX 3060 6GB GPU: Am I doomed or is there some some version of LLaMA I could run?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
AL/ML Model - ADVICE REQUESTED,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Let's discuss Tinygrad,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Don't Buy an AMD 7000 Series for LLaMA Yet,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"HW Resources regarding Inference vs. Training, Fine-Tuning, etc.",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Is it possible to host two different models at the same time on the same machine?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Quip# quantization of Tess-M,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
[Question] - Chat UI for Mistral API,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Some rumors are claiming this Mistral-Medium got leaked,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Three things I think should get more attention,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Question about Training Llama 13B GGML Models on Local Documents,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Discussion about Hadware Requirements for local LlaMa,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Which Macbook Pro to buy for running an LLM locally? I created a buyer's guide to help you decide.,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
24GB GPU OOM while qlora peft finetuning Llama2 7B,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Local LLM + Image Gen = Like GPT 4 & Dalle 3 ?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Fine tuning for dummies,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Cheapest and best way to run LLM online on an on-demand basis?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Free ChatGPT (not the paid version) locally?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Fine-tuning Xwin-LM 70B?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Looking to build a new system for local AI model/experimentation and possibly training,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"Upcoming APU Discussions (AMD, Intel, Qualcomm)",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"Any <13B model able to answer the simple ""Apples today"" vs ""Apples yesterday"" trick question?",Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Falcon 180B minimum specs?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
GPU bloat stifles AI,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
would 500 parallel GPU workers be useful to you?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"Local LLM for ""Hot Dog or Not Hot dog"" kind of fact checking",Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Does a token calculator exist?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
How to add new languages to existing models?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Qualcomm Ai accelerators: ANYONE know ANYTING other than this marketing material? sounds quite intriguing...,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
llama.cpp generation with (older) GPU is *slower* than pure CPU?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Thoughts after building a text-adventure game using local models,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
100k context windows. How soon before you can run them locally?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Need help selecting platform for fine tuning 7B model.,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
LLM GPU buyer beware?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Nous-Hermes-2-Mixtral-8x7B DPO & SFT+DPO out! Matches perf of Mixtral instruct + supports ChatML (and thus System prompt!),Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
bGPT - Byte-Level Transformer,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Blockwise Parallel Transformer for Long Context Large Models,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
I'm convinced now that “personal LLMs” are going to be a huge thing,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Shoutout to a great RP model,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Fine tuning on Apple Silicon,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Thinking about purchasing a 4090 for KoboldCPP... Got some questions.,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Serious inquiry: I've been tinkering a lot with finetuning and was wondering if it would be worth to buy a V100 of my own,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Benefits of self-hosting that carry over to professional work?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"Help me choose: Need local RAG, options for embedding, GPU, with GUI. PrivateGPT, localGPT, MemGPT, AutoGen, Taskweaver, GPT4All, or ChatDocs?",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"Xbox series X, GDDR6 LLM beast?",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
What would be the best way to go about making a C# model?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Inference with an iGPU,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Is renting GPUs only possible because we still don't have a killer open source model?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"Deepseek 67b is amazing, and in at least 1 usecase it seems better than ChatGPT 4",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
What's the latest on 1B models and Mac Mini M2 efficiency?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
I have 200+ 30 series GPUS and I'd like to use them to generate income using LLaMA where do I go to get started?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Can time compensate for lack of power?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
GPU Recommendations,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
End-to-End Encrypted Local LLMs,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Autogen + mistral small/moe/mixtral,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Is there a high quality desktop UI for MLC Chat?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Best model-setup for CPU-only?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
A local LLM is the ultimate doomsday device,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Deploying LM/LMM to edge device,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Best Local LLM For low end device coding,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
I've created Distributed Llama project. Increase the inference speed of LLM by using multiple devices. It allows to run Llama 2 70B on 8 x Raspberry Pi 4B 4.8sec/token,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"Hugging Face, the GitHub of AI, hosted code that backdoored user devices",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Run models on a real Android device with Qualcomm AI Hub,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
(Meta research) MobileLLM: Optimizing Sub-billion Parameter Language Models for On-Device Use Cases,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Native LORA finetuning on Apple Devices (New MLX Framework) 😲!!,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
What is Apple thinking? Why are they so radio silent about LLMs despite having the edge over Microsoft/Google to bring local LLMs to iDevices?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
LLM Models for Edge Devices,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"A completely open-source AI Wearable device like Avi’s Tab, Rewind’s pendant, and Humane’s Pin",Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Someone know any projects about an alexa like device build with llama,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
llm on mobile devices - reading pdf usecase,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Splitting models and device map.,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
MediaTek Leverages Meta’s Llama 2 to Enhance On-Device Generative AI,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"Introducing LlamaEdge — lightweight & portable LLM tools for your local, edge & server devices.",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"Opentensor and Cerebras announce BTLM-3B-8K, a 3 billion parameter state-of-the-art open-source language model that can fit on mobile devices",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
FP4 quantization state not initialized. Please call .cuda() or .to(device) on the LinearFP4 layer first.,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Has someone tried LLMFarm for native inference on iOS devices?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
My open-source & cross-platform on-device LLMs app is now available on TestFlight & GitHub. Feedback & testers welcome.,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
How do I run Stable Diffusion and LLMs from my PC on my mobile device? Offline and private ways?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
How to implement gpu/cpu offloading for text-generation-webui? [custom device_map],Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"humane ai pin dropping next week, how come I haven't heard more news abt this? seems like very capable tech packed into such a small device if it indeed works as they are marketing it--- i'm curious to see what type of latency it has for voice commands.",Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
What am I doing wrong?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"I'm about to open source my Flutter / Dart plugin to run local inference on all major platforms. See how it runs on my personal Apple devices: macOS (Intel & M1), iOS, iPadOS. Next up: Android, Linux & Windows. AMA.",Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
"Fellow nerds of reddit, I somehow fried my rig using TextGen Webui.",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
2x Teslas in an OEM system,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Mbp m3,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Difference between merged model (with adapters) and load adapters on top,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Two RTX 3060 for running llms locally,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Gemini Nano is a 4bit 3.25B LLM,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Why is the idea of more than one participant so foreign to LLMs other than GPT4?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Any way to optimally use GPU for faster llama calls?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Merging LoRA with Mistral models?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
"Fine tuned coqui XTTS voice, how to use the model.pth?",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
I wonder theres way to run LLM without loading on ram,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
GPU-Accelerated LLM on a $100 Orange Pi,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"You can mix different brand GPUs for multi-GPU setups with llama.cpp. Here are some numbers for a Nvidia/Intel mix. Also, the A770 works really well now.",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
oobabooga Update broke loading u/The-Bloke huggingface models?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Text generation doesn't stop after LoRA fine-tuning,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Maybe we will be able to run far larger models on Apple hardware than previously thought,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
[Project] MLC LLM for Android,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Apple CoreML,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Speculation. Could Apple pull off an end run around everyone else and offer low cost large model inference?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"Is anyone inferencing on something like an Intel nuc, barebone or similar formfactor?",Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Guide: Installing ROCm/hip for LLaMa.cpp on Linux for the 7900xtx,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
A fine tuned Llama2-chat model can’t answer questions from the dataset,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
TinyLlama-1.1B: Compact Language Model Pretrained for Super Long,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Is Llama 2 7B or 7B Chat better for a talking Teddy Bear?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Awful quantisation outputs with V100,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Skill Issue while running GPU accelerated llama,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Home setup for future home model use,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Llama finetuning question,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
What's your best monitor tool?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Optimum Intel OpenVino Performance,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
ExLlamaV2: The Fastest Library to Run LLMs,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Looking for a lightweight Model ideally fine-tuned for generating compliments or similar,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Out of memory using multiple GPUs,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Best model to convert voice commands to JSON?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Is it possible to process layers one by one on a low vram gpu?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Why are MLX and Ollama way faster than PyTorch on M1 Mac?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Having a Local LLM interact with an API,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Simple demo app of TinyStories-1m that runs locally on iOS,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Is Llama.cpp Using GPU's on my M2 Max?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
No CUDA GPUs are available error on text generator webui,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
MLC-LLM Chat vicuna-Wizard-7B-Uncensored-q3f16_0,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Waiting times for A100 hw?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Should I start this project?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
A new type of transistor is more efficient at (some) machine learning tasks,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Apple releases MLX for Apple Silicon,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
7B models cannot fit in RTX 4090 VRAM (24GB),Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Let's create a 65B benchmark in this thread,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Has Anyone Successfully Utilized the Neural Networks API on Android for LLMS with EdgeTPU?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
What are your thoughts on the future of LLMs running mobile?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"Mistral finetuning , Loss increases crazy",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Best way to make models understand certain language without fine tuning,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
"Guys, why are we sleeping on MLC LLM - Running on Vulkan?",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Koboldcpp linux with gpu guide,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
M3 Max/Ultra vs RTX3090 vs RTX4090 with large context windows?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Is Shared memory on a laptop better than regular desktop DDR5. The 4090 has 64GB of ""total"" memory?",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
What 7b llm to use,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
How to run base models w. finetuned adapters in LlamaIndex or Langchain?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
CLblast is nice on crap systems!,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Why do prompts work so differently depending on the model used?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Estimated Time for SFT Fine-Tuning of Mistral-7B Model,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"""Guidance"" a prompting language by Microsoft.",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Llama 2 7b-Instruct on 2 RTX 2080 Ti GPUs,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
flan T5-Large just gives the context as the response,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Accelerate not working with merged model?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
How long is it taking you guys to run a 7b llama 2 model?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Any decent open source voice assistants (ChatGPT or local llm) that allow interrupting?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
ONNX to run LLM,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Help needed on building doc translation LLM,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
NVIDIA RTX 5880 48GB GDDR6 Ada Generation Graphics Card,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Best backends for running models on Android?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"Intel AI built into CPU's, is it at all useful for text or image generation? What is it?",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Guide to running llama.cpp on Windows+Powershell+AMD GPUs,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Weird Dual GPU setup in Ooba,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
I made a language-agnostic function calling (open source),Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
"Finally got a model running on my XTX, using llama.cpp",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Utilizing two different size GPUs for fine-tuning,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Exploding loss when trying to train OpenOrca-Platypus2-13B,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Slow LLM speeds on RTX 4090,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Loading LLaMA2-70B model,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Anyway to use Runpod on android?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Grammar is key to next generation AI game dev?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Half Precision (e.g. no quantization) Phind V2 CodeLlama 34B running on Mac M1 at 8.6 tokens per second,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Vicuna 13b on RK3588 with Mail G610, OpenCL enabled. prefill: 2.3 tok/s, decode: 1.6 tok/s",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"Nearing Q4 23, what's the best web UI frontend?",Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
What are the limits of LLM?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Any interesting LLM project ideas?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"Blind Chat - OS privacy-first ChatGPT alternative, running fully in-browser",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
What is the current best 3B model for low end hardware,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Which is the smallest Llama model out there?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Unable to train LLaMA2-7B-HF in an RTX 3050,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
open llama failed to predict eos token?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"MiniCPM: An end-side LLM achieves equivalent performance to Mistral-7B, and outperforms Llama2-13B",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"If you train QLora in Transformers (PEFT) and 4-bit, you need to do this --->>>",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Running Llama2 on Android,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
An Alternative Approach to Building Generative AI Models,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"Advice for model to run on laptop gtx 1060 (6gb), i7 8750H, ram 16gb",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Inconsistent Token Speed on Llama 2 Chat 70b with Exllama,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
The Acer Intel A770 16GB GPU is now $250. You won't find a better new 16GB GPU for less.,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Introducing Vaartaalaap: A Chatbot UI for Local LLM Servers,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
[Project] Making AMD GPUs Competitive for LLM inference,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
LLM.swift library lets you interact LLMs on iOS easily,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
"Could I serialize models in current ""state""?",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
3080 16gb laptop + desktop gpus?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
The S24 will have gemini nano onboard!,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
D: What prompts do you use to evaluate new LLM capabilities?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Speed difference not matching file size between quants?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Success with a local voice chat agent,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Silent Release: Llama2 7B on Snapdragon Gen2 with 8 tok/s,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
How to compile models for MlC-LLM,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
LLaMA for poor,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"Using a NUC, SBC, or SFF for LLMs?",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Lightweight LLama variants for Mobile applications,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"Need Help Optimizing Language Model Performance on Nvidia Jetson AGX Xavier
",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Noob: I try to fine-tune a LoRA with a very small dataset (10 samples) on Oobabooga, the model never learns.",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"Open Inference Engine Comparison | Features and Functionality of TGI, vLLM, llama.cpp, and TensorRT-LLM",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
I built my own android chatting frontend for LLMs.,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
What's the latest on 1B models and Mac Mini M2 efficiency?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
NVidia vGPU on esx,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
I love hallucinations,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Fine-tuning for custom domain knowledge,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Dual 3090 ti GPU's on Ubuntu Desktop x64 help,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Qnap TS-264,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Model parallelism with LoRA,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
HF transformers vs llama 2 example script performance,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Long load time using from_pretrained?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
num_beams > 1 breaking my model (Open-LLaMA7b - Alpaca-finetuned),Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
How to speed up tokenizer loading speed for lmsys/vicuna-13b-v1.3? (Takes 3 min),Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
How to find good llama.cpp command line parameters,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Exllama on windows using CPU,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Llama2 Qualcom partnership,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"Offline voice assistant using Ollama API, Mistral 7B, and Whisper",Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
1x rtx 6000 ada or 2x 4090 ?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"I love running locally, but",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Guide for oogaboooga on amd using rocm gpu on linux ubuntu and fedora,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
[HELP] It's there a way to make Llama 2 model generate text token by token or word by word like what ChatGPT does?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Best prompt and model for fact-checking a text (disinformation/fake-news detection),Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Commercial model + API question,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Is the Nvidia Jetson AGX Orin any good?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Can't use multi-gpu with 8x A100 80GB,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Using LLMs to build custom Operating Systems?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
A Free AI Scribe Project I am Working on! Please Provide Feedback!,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
"Since the launch of the AI Pin, have there been any alternatives announced that are open platform?",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"Low token/s count on 7b models, is this normal",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"Is there an equivalent of ChatGPT ""Plugins"" for local LLMs Web UIs? Like Code Interpreter, Plot Generator (using matplotlib), etc. I know Langchain and others claim to use ""Tools"", but those are not as capable as ChatGPT's Plugins.",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Resume training from a checkpoint with different hyperparameters when training with PEFT,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
How to fine-tune Llama 70B fp16 on 8x A100 80GB?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Apple has an excellent hardware base for local generative AI,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"Documentation Paradigm for Large Language Models (LLMs): Log Today, Train Tomorrow",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Experience of setting up LLAMA 2 70B Chat locally,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Ryzen Direct Memory Access in OpenCL mode,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"Anyone working on a ""tiny"" version of Mixtral-8x7b",Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Seeking Recommendation - Cooling Hardware for NVIDIA Tesla Cards,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
llama-cpp-python not using GPU,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"New llama-cpp-python out, with performance patches",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Small local model for git/shell,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Anyone working on linking local Ai with Home Assistant?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Single RTX 4090 FE at 40 tokens/s but with penalty if running 2 get only 10 tokens/s. Confirmed with Xwin-MLewd-13B-V0.2-GGUF.,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"When LLM doesn’t fit into memory, how to make it work?",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
How to optimize fine-tuning of Llama-2 13B?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Some advice on running Guidance?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"Running RedPajama and other open LLMs on phones, browsers and AMD/NV/Intel GPUs",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
How to use Rocm with windows?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Distributed Inference and Fine-tuning of Large Language Models Over The Internet,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Can I run an LLM that takes up no more than 1-4GB of RAM / VRAM and have it answer questions using my notes, or is that unrealistic?",Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Llama 2 as a local copilot!!,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"Used GPT4ALL for the first time and wondering if I could somehow feed it (technical) PDFs and turn it into ""sidekick"" for embedded programming?",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
llama2-7B/llama2-13B parameter model generates random text after few questions,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Running Llamacpp with an RX5700,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Most advanced LLM for a Jetson Orin Nano?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
What is the best API right now for self-hosted LLM usage?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Running Llama-65B with moderate context sizes,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Yet another quantization method: SpQR by Tim Dettmers et al.,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Help to use pipeline conversational on mistral instruct v0.2,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Why does my fine tuning of Mistral 7B Instruct not stop?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Multi-Model Multi-GPU Architectures: LLM + ASR + TTS,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Anyone know the nuances of running a model metal performance shader?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Keep running out of memory when pre-training (without LoRA) a model 7B on 2 A100 80GB GPU?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Code-LLaMa: Stuck after loading,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Improving Falcon-180B Performance,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Text Generator webui - how do I select which GPU to use?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Building an IDE with native support for Open Source models,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Error in load_in_8bit when running alpaca-lora using 3080,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
I am looking for information regarding Running llama on a zen4 or xeon 4th generation cpu? Or alternative no gpu suggestions (for 180b falcon),Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Time to first token (TTFF) with llama.cpp vs. vllm,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Most performant option to run 13b LLM locally as a personal assistant for under $300 USD,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Running [Crataco ]Pythia Deduped GGML on 4 GB Ram Laptop,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"Working on a QLORA hub for model personalities, help needed",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Best setup and Settings for a Beginner?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Retrieval Augmented Generation optimised Llm's,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Guanaco fine-tuning for classification,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
LLaMA-4bit inference speed for various context limits on dual RTX 4090 (triton optimized),Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Engaging topics for conversations in a small local workshop,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Finetuned Mistral outputting multiple Q and A responses.,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
What's the reasonable tks/s running 30B q5 with llama.cpp (13900K + 4090) ?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Issue Loading 13B Model in Ooba Booga on RTX 4070 with 12GB VRAM,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Forget OpenAi function calling (openhermes + outlines),Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Help with objective tokens per second measurement,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
I am not special. Though I seek guidance from those that are.,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
CogAgent: A Visual Language Model for GUI Agents,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
KoboldCpp - Combining all the various ggml.cpp CPU LLM inference projects with a WebUI and API (formerly llamacpp-for-kobold),Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Can we create a megathread for cataloging all the projects and installation guides of Llama?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
QLoRA with GPTQ problems,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Mark Zuckerberg on upcoming LLaMA v2,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Integrating Diverse Language Models in a single interface?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Would an E-GPU work as good on Linux than an internal GPU, same model?",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"Using system RAM as ""swap"" for GPU?",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Issues with the starter code for codellama,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Falcon 180B GPTQ Model on Multi-GPU Setup with RunPod,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Hugging Face community blogpost: 🕳️ Attention Sinks in LLMs for endless fluency (related to StreamingLLM),Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Training LLaMA-2 for Keyword Extraction,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"Trying to load togethercomputer_LLaMA-2-7B-32K with fully loaded context but it OOMs, but I should have enough VRAM?",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
How do I load a gptq LLaMA model (Vicuna) in .safetensors format?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Codelamma 7b code completion giving multiple responses and i only want one?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
A simple Huggingface Downloader,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
New badass model OpenAssistant/llama2-13b-orca-8k released 🎉,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
"New Oobabooga Standard, 8bit, and 4bit plus LLaMA conversion instructions, Windows 10 no WSL needed",Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Building koboldcpp_CUDA on Linux,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Thunderbolt and multiple eGPUs,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Trouble creating document Q&A chat bot,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
How to perform multi-GPU parallel inference for llama2?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Speculative Decoding in Exllama v2 and llama.cpp comparison,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
STOP using small models! just buy 8xH100 and inference your own GPT-4 instance,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
2024: Small Models Will Be Insane,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Phi-2: The surprising power of small language models,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Built a small quantization tool,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Favorite small models?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Experimenting with small language models,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Orca 2: Teaching Small Language Models How to Reason,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Small Benchmark: GPT4 vs OpenCodeInterpreter 6.7b for small isolated tasks with AutoNL. GPT4 wins w/ 10/12 complete, but OpenCodeInterpreter has strong showing w/ 7/12.",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
The LLM Creativity benchmark (initial results: small models + miqu),Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Devs: What small building blocks would help you build better AI apps?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
llm's on small/portable laptop?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"Small model for ""Open Interpreter"" use recommendation?",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Specific small models and parallel use,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"CodeGen2.5: Small, but mighty",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
What's the best/practical use you've found for (Llama 2) 7B small models?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
What are your favorite use cases for small models around 3b and less.,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
LLM Assistant with function calling - Just a small test project I made,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
QLoRA hyperparameters for small fine-tuning task,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
What can we achieve with small models ?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Small dataset for primary LLM training,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"What are your favorite ""small"" models for text comprehension?",Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
"Quick comparison of mistral-small, mistral-medium, GPT-3 and GPT-4",Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Small AI Dev Tools Pt 1: Context Manager,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Small Model similar to Character AI,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Looking for a workstation recommendation to run small/medium LLM models.,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Why only small number of models fight on Chatbot Arena?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Autogen + mistral small/moe/mixtral,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Small First Aid / survival LLM,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Is anyone exploring the idea of a proxy local small model?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
A small test I did with falcon-180b-chat.Q2_K.gguf (at home on consumer grade hardware),Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Looks like someone did the needful and it's a small download. 70b trained on proxy logs.,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Few questions on small language models,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
DiscoLM German 7B V1 - best small german model so far?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Finetune a small 7B model for my native language,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Small Giants: 10 sub-13B “Open Source” LLMs,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"Moondream, a small vision language model based on Phi 1.5",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Local model for code in small machine,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Small local model for git/shell,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Best small fine tuning dataset,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Small llm model within 100M to 1B parameter,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Any chance mistral open-sources their Mistral-small model?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Small models for text classification/basic understanding?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Tired of small model? Did you miss : Tulu V2 DPO 70B,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Using small language models as data validator for topic modelling results,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Brainstorm. Small company use of AI for customer support.,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Training a small model from scratch,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Small explanation question! Types of LLAMA,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
How to train 7B models with small documents?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
ive seen a remarkable increase in performance of small models like 13b or even 1b at what point can these learn to play games?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
CAPPr: guarantee small structured outputs given a list of choices,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Is there a way to fine-tune llama on extremely small dataset?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Running a small model on a phone?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
SanjiWatsuki/Loyal-Macaroni-Maid-7B a strong new roleplay model in a small package!,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Whats the best way to run small llms locally on an ond machine?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Idea about restricting format of LLM output (with small POC),Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Small model fine tuning or QLora on Bigger One?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
A small llama.cpp server playground,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Engaging topics for conversations in a small local workshop,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Any way to verify training method with very small dataset?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Example of a small fine tuning,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Small request - how would you recreate goblin.tools locally?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Are big models always better than small ones for Transfer Learning?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
What are small models that work for you? What are the configs that you use for them?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Looking for a small model to host on my server,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Unpopular Opinion: All these small open-source foundational models coming out are not moving us forward. To truly rival closed-source, we need models with 100+ billion parameters.",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Are there any good math Datasets for Training small models?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Noob: I try to fine-tune a LoRA with a very small dataset (10 samples) on Oobabooga, the model never learns.",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Idea: Alternating token generation from a big and small model?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Can someone explain simply: How/why do small models (llama-2 7b) outperform larger ones (chinchilla 70b)?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Plausible to Train Small Models on MacBook Pro M2 Max?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Question on setting up local inference server for small team,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
How to do correctly speculative decoding on the CPU using small models 1B and 7B?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Seeking Advice on Training a Custom Language Model (small language models for specific domain),Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
What model would you choose if you wanted to fine tune a customer service agent model for a small business?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Small Uncensored LLM model to train cheaply for a specific task.,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Llm Super Coach - Small fun project to use a local LLM to analyze one's diaries in various ways.,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
small script that adds automatically V1/2/3 to your ggmls,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
How do you highlight a small list of key points from a document?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
"Some small pieces of statistics. Mixtral-8x7B-Chat(Mixtral finetune by Fireworks.ai) on Poe.com gets the armageddon question right. Not even 70Bs can get this(Surprisingly, they can't even make a legal hallucination that makes sense.). I think everyone would find this interesting.",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Fine-tuning the 13B wizard model with a small amount of dataset and achieving GPT-4 level results?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"humane ai pin dropping next week, how come I haven't heard more news abt this? seems like very capable tech packed into such a small device if it indeed works as they are marketing it--- i'm curious to see what type of latency it has for voice commands.",Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
"MIT-IBM Watson AI Lab releases MoLM suite with three small sparse MoE models, the largest of which (8B params with 700M experts) performs on par with Pythia 2.8B while its throughput is comparable to Pythia 1.4B",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Phi-2 becomes open source (MIT license 🎉),Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
How much more stupid is the 120B goliath Q3_K_M than the larger options?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
"I recently tested the ""MPT 1b RedPajama + dolly"" model and was pleasantly surprised by its overall quality despite its small model size. Could someone help to convert it to llama.cpp CPU ggml.q4?",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Zuckerberg says they are training LLaMa 3 on 600,000 H100s.. mind blown!",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
What's the smallest but stil useful model you encountered,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
What's your 2024 AI Predictions?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
"Yes I am an expert at training, how could you tell?",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Best LLM under 3 billion parameters currently?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
New Model: Nomic Embed - A Truly Open Embedding Model,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
TinyLlama-1.1B: Compact Language Model Pretrained for Super Long,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Any other smaller LLM (smaLLM? :D) users here?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Orca (built on llama13b) looks like the new sheriff in town,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
2.7B model that performs like Mistral 7B!,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Train Smarter, Not Harder? - MiniSymposium 7b",Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Beating Bigger Models is All You Need (Where is the LLM industry headed?),Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Why is no-one fine-tuning something like t5?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Why did gpu AIB partners stop making extra vram models ?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
GPT-3.5-Turbo-0125 is so much worse I need a replacement,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
First OS embedding model with 8k context,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"Just got a 4090 24GB, what should I run?",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"We all hate LangChain, so what do we actually want?",Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Can someone explain what is mixtral 8x7B?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Models Megathread #3 - What models are you currently using? What are your thoughts on Mixtral?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
What are the business use of LLM that will generate revenue?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
5 x A100 setup finally complete,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Nucleus 1B - an SLM based on Mistral!,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
How would a 4090 laptop fair in the current state of LocalLLaMA?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Mistral has an even more powerfull model in the prototype-phase,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
What would be the smallest open source llm models that are still of reasonable function? ,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Running full Falcon-180B under budget constraint,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Serving a large number of users with a custom 7b model,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Fine tuning for coding,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Writing a novel generator. Looking for help/testers from authors/prompt masters.,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
GPU Requirements for LLMs,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Local LLM movement feels like early days of PCs vs Mainframes,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
What's the best 1-4B LLM for a phone?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"Microsoft makes new 1.3B coding LLM that outperforms all models on MBPP except GPT-4, reaches third place on HumanEval above GPT-3.5, and shows emergent properties",Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Is it possible to run some kind of LLM on 1 GB of RAM?,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
OpenOrca-Preview1-13B released,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Deducing Mistral Medium size from pricing: Is it a 195b parameter - 8x30b MoE model?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Mac vs Windows,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Has anyone tried using a smaller model with an RAG to train a larger model?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Looking for the best cost/price mini(?)-pc for a local llm,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Optimal and Cost-Effective GPUs and Server Specs for Local AI Model Development,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"MoE locally, is it possible?",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
A Paradigm Shift in Machine Translation: how to outperform GPT-3.5 for translation using 7B and 13B models,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
How are people using open source LLMs in production apps?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Making LLAMA model return only what I ask (JSON).,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Arthur Mensch Clarification about recent events,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
RAG: Flexible Context Retrieval around a matching chunk,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
How do you determine which embedding models will fit into memory available?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"Is anyone inferencing on something like an Intel nuc, barebone or similar formfactor?",Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Best uncensored multimodal / vision model?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Microsoft's Phi 1.5 (1.7 B parameter) is now a multimodal model,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
New LLM from (almost?) scratch.,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Is some tiny (1GB) model available through pip install?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
A Recipe for Textbooks Are All You Need,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Mistral 7b v0.2 is nuts,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
New Paper: Proxy-Tuning: An Efficient Alternative to Finetuning Large Language Models,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
"Google is training with 8 bit ints, next will move to 4 bit ints.",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Beginner friendly Guide to run local model AI on 4 Gb ram windows (GGML/GGUF Guide),Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
JSON parsing as the benchmark for a LLM,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Single GPU (3080 10gb) or M1 Max Fine Tuning Help,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
One-bit quantization is a thing now,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Upgrading GTX 1060 6GB to RTX 3070 Ti 8GB is good enough for 13B models?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Evaluating mistral-medium,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Looking for a lightweight Model ideally fine-tuned for generating compliments or similar,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
I still feel a large gap in math vs gpt4,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Is Gemma the old Bard?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
"As of 2024, what is the state of non-english models (embedding/LLM) for RAG?",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Best Model to locally run in a low end GPU with 4 GB RAM right now,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Trying LLM Locally with Tesla P40,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
This is so Deep (Mistral),Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
How much does VRAM matter?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
RTX 3090 vs RTX 3060: inference comparison,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
🚀We trained a new 1.6B parameters code model that reaches 32% HumanEval and is SOTA for the size,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"Using a NUC, SBC, or SFF for LLMs?",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
What tasks are 7b and 13b models really good at?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Most straight-forward repo/library for full-fine-tuning,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Open-source LoRa training guide & Code?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Quip# quantization of Tess-M,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Mixtral-8X7B - Local vs API performance,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Simple Questions Megathread,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Toxicity classification model,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Can I train Llama on my own document st?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Could you guys give me tips for my potential school project?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"That ""No Moat"" Google memo from 7 months ago has aged very well",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
"Excited to share my ambitious free and open-source library for connecting AI, human, and computing systems.",Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Which model works best for role-playing?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Best developing country LLM?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Mixtral can play Akinator. Kinda.,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Give me some advice about mixtral,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
"Would it be good way to learn transformers and LLMs' basic structure by trying to build something by myself? Not any way coherent, just technically working and at least outputting some gibberish? Goal is understand the basics and try same things people do in llama.cpp and new approaches like Mamba",Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Experiences with smaller models with RAG?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
How bad is finetuning on p40s vs p100s ?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
How to add new languages to existing models?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
QuIP: 2-Bit Quantization of Large Language Models With Guarantees,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
So is RAG no longer needed if you have a ton of RAM?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
TIL Some tokens can be multiple words,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Most capable function calling open source models?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Dealing with big datasets,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
I am just learning about LLMs and for educational purposes made a simple language model,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"LLM Chat/RP Comparison/Test (Euryale, FashionGPT, MXLewd, Synthia, Xwin)",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Looking for a Handy LLM for Survival & DIY – Any Suggestions?,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
any ideas on how sites like Grok and Perplexity return the most recent relevant result?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Can we participate in the Subredit Blackout?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
What is the smallest ggml model available?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Anyone is using llamacpp for real?,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
Datasets for local model grammar fixing and writing improvement,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Can you pair RTX 4090 with RTX 6000 Ada?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
What's the latest on 1B models and Mac Mini M2 efficiency?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Google Dialogue Flow replacement,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
This is the new king of LLM hardware with 576 GB of RAM.,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
What is the status of autonomous agents manipulating our browser?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
What can I run on this Dell PowerEdge R710 Server 32G Ram/12 Core?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
"In my opinion open-source projects should focus an a very narrow thing, instead of focusing on being a ""GPT"", that focuses on being able to do everything.",Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
Tiny models for contextually coherent conversations?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Petals vs vLLM vs ? for serving LLMs to many users,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
tlm - using Ollama to create a GitHub Copilot CLI alternative for command line interface intelligence.,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Relationship between GPU memory and context size,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Would a merge between Neural Chat 7B v3.1 and OpenHermes-2.5 work?,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Use llm as a development team,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
GGUF vs AWQ vs GGML,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Proof of Concept: Local LLM to execute terminal comands (Here GPT-2),Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
How do you discover tools/ideas that might help improve your LLM-based apps which are not RAG?,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
DPO training strategy,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
What are the experts in Mixtral?,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
"Quantizing 70b models to 4-bit, how much does performance degrade?",Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Apple's tiny 34M paramters transformer,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Mistral API: Initial test results,Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Current best options for local LLM hosting?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
New merges: Aurora-Nights-70B and Aurora-Nights-103B,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Mistral 7B + MiniGPT-v2 + Go1 robot-dog + custom gripper = CognitiveDog,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
OrangePi 32GB worth it?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
NeuralHermes-2.5: Boosting SFT models' performance with DPO,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Introducing LIVA: Your Local Intelligent Voice Assistant,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Don't underestimate the importance of the system prompt,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
"MiniCPM: An end-side LLM achieves equivalent performance to Mistral-7B, and outperforms Llama2-13B",Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
New Model RP Comparison/Test (7 models tested),Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Cheaper cloud alternatives to train LLM for educational purpose,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Budget machine for tinkering with LLMs,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"Why is CodeLlama on huggingface so opinionated, biased and arrogant ?",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Best tools for automated generation of questions and answers from unstructured text,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
New quantization method AWQ outperforms GPTQ in 4-bit and 3-bit with 1.45x speedup and works with multimodal LLMs,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
Budget rig for LLM,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
Meta Releases Llama Guard - the Hugging Edition,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
How is Solar so good for it's size,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Deepseeker Code language/IDE knowledge Questiom,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
Deploying LM/LMM to edge device,Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
What are the problems faced when an AI customer support chatbot is used by a SaaS business.,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Any help on using knowledge distillation on LLMs like Llama2 or Qwen?,Topic 3,"llm, using, guide, running, hardware, bit, model, locally, gb, run"
"I only said ""Hello..."" :( (Finetune going off the rails)",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Single board computers with 64 GB RAM,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Anyone tested speculative sampling in llama.cpp?,Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
New dataset for fine-tuning: spicyfiction,Topic 0,"model, fine, small, tuning, best, embeddings, finetuning, good, mistral, retrieval"
"Refresh of ""GPUs4AI"" (database of AI-capable GPUs)",Topic 5,"model, small, v, language, llama, large, gpus, question, better, gb"
Local LLM projects,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
What is the smallest possible model (file size)? Or how to create a basic chat model?,Topic 2,"model, open, source, llm, run, local, intel, function, project, chat"
Can i fine tune any model with m1 16gb ram,Topic 9,"model, llm, gb, gpu, run, training, embeddings, llama, ram, help"
Llama 2 Scaling Laws,Topic 4,"llm, model, embedding, local, hardware, rag, context, server, inference, code"
"Just getting started, low cost machine",Topic 7,"llama, use, model, embeddings, hardware, llm, token, local, text, way"
Train LLM From Scratch,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Inference Speed Benchmark,Topic 6,"llm, run, hardware, rtx, model, generation, inference, best, project, speed"
Good local models for data cleaning and extraction?,Topic 8,"model, llm, local, rag, hardware, small, inference, idea, parameter, fine"
"EXLlama test on 2x4090, Windows 11 and Ryzen 7 7800X3D",Topic 1,"ai, model, best, running, hardware, local, llama, device, android, generative"
